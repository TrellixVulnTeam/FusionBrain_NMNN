{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!git clone https://github.com/sberbank-ai/fusion_brain_aij2021.git\n",
    "# !pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install tpu_star==0.0.1rc10\n",
    "# !pip install albumentations==0.5.2\n",
    "# !pip install einops==0.3.2\n",
    "# !pip install pytorch_lightning\n",
    "# !pip install comet_ml\n",
    "# !pip install transformers==4.10.0 \n",
    "# !pip install colorednoise==1.1.1\n",
    "# !pip install catalyst==21.8 \n",
    "# !pip install opencv-python==4.5.3\n",
    "# !pip install gdown==4.0.2\n",
    "# !pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Доступные ресурсы\n",
    "import multiprocessing\n",
    "import torch\n",
    "from psutil import virtual_memory\n",
    "\n",
    "ram_gb = round(virtual_memory().total / 1024**3, 1)\n",
    "\n",
    "print('CPU:', multiprocessing.cpu_count())\n",
    "print('RAM GB:', ram_gb)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device.type)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage import io\n",
    "from sklearn.metrics import accuracy_score\n",
    "import albumentations as A\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "from torch.utils.data import SequentialSampler\n",
    "\n",
    "from fb_baseline.fb_utils.metrics import cer, wer, string_accuracy, acc, vqa_evaluate, detection_evaluate\n",
    "from fb_baseline.fb_utils.c2c_eval import Beam, eval_bleu\n",
    "from fb_baseline.fb_utils.detection_eval import inverse_detection_evaluation\n",
    "from fb_baseline.fb_utils.vqa_eval import inverse_vqa_evaluation\n",
    "from fb_baseline.model.utils.utils import CTCLabeling\n",
    "from fb_baseline.model.dataset.dataset import (\n",
    "    HTRDataset, VQADataset, C2CDataset, DetectionDataset, FusionDataset,\n",
    "    htr_collate_fn, c2c_collate_fn, vqa_collate_fn, detection_collate_fn\n",
    ")\n",
    "from fb_baseline.model.model import InverseAttentionGPT2FusionBrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# Handwritten\n",
    "# #\n",
    "with open('/home/jovyan/vladimir/fusion_brain/data/test/true_fb/true_HTR.json', 'rb') as f:\n",
    "    json_marking = json.load(f)\n",
    "marking = []\n",
    "for image_name, text in json_marking.items():\n",
    "    marking.append({\n",
    "        'task_ids': 'handwritten',\n",
    "        'images': os.path.join('/home/jovyan/vladimir/fusion_brain/data/private_fb/HTR/images/', image_name),\n",
    "        'gt_texts': text,\n",
    "    })\n",
    "df_handwritten = pd.DataFrame(marking)\n",
    "df_handwritten['stage'] = 'test'\n",
    "# #\n",
    "# C2C\n",
    "# #\n",
    "with open('/home/jovyan/vladimir/fusion_brain/data/test/private_fb/C2C/requests.json', 'rb') as f:\n",
    "    java_json = json.load(f)\n",
    "with open('/home/jovyan/vladimir/fusion_brain/data/test/true_fb/true_C2C.json', 'rb') as f:\n",
    "    python_json = json.load(f)\n",
    "marking = []\n",
    "for key in java_json:\n",
    "    marking.append({\n",
    "        'task_ids': 'c2c',\n",
    "        'java': java_json[key],\n",
    "        'python': python_json[key],\n",
    "    })\n",
    "df_c2c = pd.DataFrame(marking)\n",
    "df_c2c['stage'] = 'test'\n",
    "# #\n",
    "# VQA\n",
    "# #\n",
    "with open('/home/jovyan/vladimir/fusion_brain/data/test/private_fb/VQA/questions.json', 'rb') as f:\n",
    "    json_questions = json.load(f)\n",
    "with open('/home/jovyan/vladimir/fusion_brain/data/test/true_fb/true_VQA.json', 'rb') as f:\n",
    "    json_answers = json.load(f)\n",
    "marking = []\n",
    "for key in json_questions:\n",
    "    marking.append({\n",
    "        'task_ids': 'vqa',\n",
    "        'images': os.path.join(\n",
    "            \"/home/jovyan/vladimir/fusion_brain/data/test/private_fb/VQA/images/\", json_questions[key]['file_name']\n",
    "        ),\n",
    "        'questions': json_questions[key]['question'],\n",
    "        'answers': json_answers[key]['answer'],\n",
    "    })\n",
    "df_vqa = pd.DataFrame(marking)\n",
    "df_vqa['stage'] = 'test'\n",
    "# #\n",
    "# Detection\n",
    "# #\n",
    "with open('/home/jovyan/vladimir/fusion_brain/data/test/true_fb/true_zsOD.json', 'rb') as f:\n",
    "    json_true_zsod_test = json.load(f)\n",
    "marking = []\n",
    "for image_name in json_true_zsod_test:\n",
    "    marking.append({\n",
    "        'task_ids': 'detection',\n",
    "        'images': os.path.join(\"/home/jovyan/vladimir/fusion_brain/data/test/private_fb/zsOD/images\", image_name),\n",
    "        'requests': [request for request in json_true_zsod_test[image_name].keys()],\n",
    "        'boxes': [boxes for boxes in json_true_zsod_test[image_name].values()],\n",
    "    })\n",
    "df_detection = pd.DataFrame(marking)\n",
    "df_detection['stage'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_augs = {\n",
    "    'handwritten': A.Compose([\n",
    "        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.25, always_apply=False),\n",
    "        A.Rotate(limit=3, interpolation=1, border_mode=0, p=0.5),\n",
    "        A.JpegCompression(quality_lower=75, p=0.5),\n",
    "    ], p=1.0),\n",
    "    'vqa': A.Compose([\n",
    "        A.Resize(224, 224, always_apply=True),\n",
    "        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ], p=1.0),\n",
    "    'detection': A.Compose([\n",
    "        A.Resize(224, 224, always_apply=True),\n",
    "        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ], p=1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jovyan/vladimir/fusion_brain/FusionBrain/fb_baseline/configs/ctc_chars.txt') as f:\n",
    "    ctc_labeling = CTCLabeling(f.read())\n",
    "model_name = 'gpt2-medium'\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(model_name, pad_token='<|pad|>')\n",
    "\n",
    "gpt_model = GPT2Model.from_pretrained(model_name)\n",
    "gpt_model.resize_token_embeddings(len(gpt_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datasets = {\n",
    "    \"handwritten\": HTRDataset(df_handwritten[:100], ctc_labeling, 512, 128, task_augs),\n",
    "    \"c2c\": C2CDataset(df_c2c[:10], gpt_tokenizer, 300, 250, 'test'),\n",
    "    \"vqa\": VQADataset(df_vqa[:100], gpt_tokenizer, 21, 8, 'test', task_augs),\n",
    "    \"detection\": DetectionDataset(df_detection[:100], gpt_tokenizer, 21, 'test', task_augs)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Как выглядят семплы для каждой задачи\n",
    "def demo_sample(sample):\n",
    "    if sample['task_id'] == 'handwritten':\n",
    "        print('[gt_text]:', sample['gt_text'])\n",
    "        return io.imshow(sample['image'].permute(1,2,0).numpy())\n",
    "    elif sample['task_id'] == 'c2c':\n",
    "        print('[source_text]:', gpt_tokenizer.decode(sample['input_ids'].numpy(), skip_special_tokens=True))\n",
    "        print('[target_text]:', sample['target'])\n",
    "        return\n",
    "    elif sample['task_id'] == 'detection':\n",
    "        print('[input_ids]:', [\n",
    "            gpt_tokenizer.decode(input_ids.numpy(), skip_special_tokens=True)\n",
    "            for input_ids in sample['input_ids']\n",
    "        ])\n",
    "        print('[boxes]:', sample['boxes'])\n",
    "        return\n",
    "    elif sample['task_id'] == 'vqa':\n",
    "        print('[question]:', gpt_tokenizer.decode(sample['input_ids'].numpy(), skip_special_tokens=True))\n",
    "        print('[answers]:', sample['target'])\n",
    "        return\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_sample(test_datasets['handwritten'][np.random.randint(len(test_datasets['handwritten']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_sample(test_datasets['c2c'][np.random.randint(len(test_datasets['c2c']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_sample(test_datasets['vqa'][np.random.randint(len(test_datasets['vqa']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_sample(test_datasets['detection'][np.random.randint(len(test_datasets['detection']))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handwritten_config = {\n",
    "    'patch_w': 8,\n",
    "    'patch_h': 128,\n",
    "    'in_layer_sizes': [8*128*3],\n",
    "    'out_layer_sizes': [64],\n",
    "    'orth_gain': 1.41,\n",
    "    'dropout': 0.1,\n",
    "    'lstm_num_layers': 3,\n",
    "    'output_dim': len(ctc_labeling),\n",
    "}\n",
    "\n",
    "vqa_config = {\n",
    "    'tokens_num': len(gpt_tokenizer),\n",
    "}\n",
    "\n",
    "detection_config = {\n",
    "    'num_mlp_layers': 3,\n",
    "    'num_queries': 12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InverseAttentionGPT2FusionBrain(\n",
    "    gpt_model,\n",
    "    handwritten_config=handwritten_config,\n",
    "    vqa_config=vqa_config,\n",
    "    detection_config=detection_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(task, loader, model, threshold=None, tokenizer=None, device=torch.device('cuda:0')):\n",
    "    result = []\n",
    "    true_json_detection = {}\n",
    "    pred_json_detection = {}\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            if task == 'handwritten':\n",
    "                htr_images, encoded, encoded_length, gt_texts = batch\n",
    "                images = htr_images.to(device)\n",
    "                handwritten_outputs = model('handwritten', images=images)\n",
    "                for encoded, gt_text in zip(handwritten_outputs.argmax(2).data.cpu().numpy(), gt_texts):\n",
    "                    pred_text = ctc_labeling.decode(encoded)\n",
    "                    result.append({\n",
    "                        'task_id': 'handwritten',\n",
    "                        'gt_output': gt_text,\n",
    "                        'pred_output': pred_text,\n",
    "                    })\n",
    "\n",
    "            if task == 'c2c':\n",
    "                code_input_ids, code_attention_masks, code_targets = batch\n",
    "                input_ids = code_input_ids.to(device)\n",
    "                attention_masks = code_attention_masks.to(device)\n",
    "                _, hidden_states = model('c2c', input_ids=input_ids)\n",
    "                bleu_score, _ = eval_bleu(model, hidden_states, input_ids=input_ids, beam_size=5, tokenizer=tokenizer, targets=code_targets)\n",
    "                result.append({\n",
    "                        'task_id': 'trans',\n",
    "                        'true_text': code_targets,\n",
    "                        'bleu_score': bleu_score,\n",
    "                })\n",
    "\n",
    "            if task == 'vqa':\n",
    "                vqa_images, vqa_input_ids, _, targets = batch\n",
    "                images = vqa_images.to(device)\n",
    "                input_ids = vqa_input_ids.to(device)\n",
    "                vqa_outputs = inverse_vqa_evaluation(model, images, input_ids, 10)\n",
    "                for target, pred_labels in zip(targets, vqa_outputs.argmax(-1).cpu().numpy()):\n",
    "                    result.append({\n",
    "                        'task_id': 'vqa',\n",
    "                        'gt_output': target,\n",
    "                        'pred_output': gpt_tokenizer.decode(pred_labels).split('.')[0],\n",
    "                    })\n",
    "\n",
    "            if task == 'detection':\n",
    "                detection_names, detection_images, detection_input_ids, _, boxes, size = batch\n",
    "                images = detection_images.to(device)\n",
    "                input_ids = [input_id.unsqueeze(0).to(device) for input_id in detection_input_ids[0]]\n",
    "                detection_outputs = inverse_detection_evaluation(model, images, input_ids, threshold)\n",
    "                img_h, img_w = size[0]\n",
    "                for i in range(len(detection_outputs)):\n",
    "                    if detection_outputs[i].numel() != 0:\n",
    "                        detection_outputs[i][:,[0, 2]] *= img_w\n",
    "                        detection_outputs[i][:, [1, 3]] *= img_h\n",
    "                    detection_outputs[i] = detection_outputs[i].type(torch.int32).cpu().tolist()\n",
    "                image_name = detection_names[0]\n",
    "                true_json_detection[image_name] = {}\n",
    "                pred_json_detection[image_name] = {}\n",
    "                for requeste, pred_boxes, real_boxes in zip(detection_input_ids[0], detection_outputs, boxes[0]):\n",
    "                    true_json_detection[image_name][gpt_tokenizer.decode(requeste.numpy())[9:-1]] = real_boxes\n",
    "                    pred_json_detection[image_name][gpt_tokenizer.decode(requeste.numpy())[9:-1]] = pred_boxes\n",
    "                result.append({\n",
    "                        'task_id': 'detection',\n",
    "                    })\n",
    "\n",
    "    result = pd.DataFrame(result)\n",
    "\n",
    "    handwritten_result = result[result['task_id'] == 'handwritten']\n",
    "    if handwritten_result.shape[0]:\n",
    "        print('= Handwritten =')\n",
    "        print('CER:', round(cer(handwritten_result['pred_output'], handwritten_result['gt_output']), 3))\n",
    "        print('WER:', round(wer(handwritten_result['pred_output'], handwritten_result['gt_output']), 3))\n",
    "        print('ACC:', round(string_accuracy(handwritten_result['pred_output'], handwritten_result['gt_output']), 3))\n",
    "        print('=== === === ===')\n",
    "\n",
    "    trans_result = result[result['task_id'] == 'trans']\n",
    "    if trans_result.shape[0]:\n",
    "        print('== C2C ==')\n",
    "        print('meanBLEU:', np.mean(trans_result['bleu_score']))\n",
    "        print('=== === === ===')\n",
    "\n",
    "    vqa_result = result[result['task_id'] == 'vqa']\n",
    "    if vqa_result.shape[0]:\n",
    "        print('== VQA ==')\n",
    "        print('ACC:', round(vqa_evaluate(vqa_result), 3))\n",
    "        print('=== === === ===')\n",
    "\n",
    "\n",
    "    if len(true_json_detection):\n",
    "        print('== Detection ==')\n",
    "        print('ACC:', round(detection_evaluate(true_json_detection, pred_json_detection), 3))\n",
    "        print('=== === === ===')\n",
    "\n",
    "    return round(detection_evaluate(true_json_detection, pred_json_detection), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model.eval()\n",
    "state_dict = torch.load(\n",
    "    '/home/jovyan/vladimir/fusion_brain/experiments/fusion/checkpoints/weight-epoch=02-v1.ckpt'\n",
    ")['state_dict']\n",
    "state_dict = OrderedDict({key[6:]: value for key, value in state_dict.items()})\n",
    "model.load_state_dict(state_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Handwritten"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_datasets['handwritten'],\n",
    "    batch_size=1,\n",
    "    sampler=SequentialSampler(test_datasets['handwritten']),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=htr_collate_fn,\n",
    ")\n",
    "\n",
    "evaluation_result = run_evaluation('handwritten', test_loader, model, tokenizer=gpt_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# C2C"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_datasets['c2c'],\n",
    "    batch_size=1,\n",
    "    sampler=SequentialSampler(test_datasets['c2c']),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=c2c_collate_fn,\n",
    ")\n",
    "\n",
    "evaluation_result = run_evaluation('c2c', test_loader, model, tokenizer=gpt_tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VQA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_datasets['vqa'],\n",
    "    batch_size=1,\n",
    "    sampler=SequentialSampler(test_datasets['vqa']),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=vqa_collate_fn,\n",
    ")\n",
    "\n",
    "evaluation_result = run_evaluation('vqa', test_loader, model, tokenizer=gpt_tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Detection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_datasets['detection'],\n",
    "    batch_size=1,\n",
    "    sampler=SequentialSampler(test_datasets['detection']),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=detection_collate_fn,\n",
    ")\n",
    "\n",
    "evaluation_result = run_evaluation('detection', test_loader, model, 0.0, tokenizer=gpt_tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}