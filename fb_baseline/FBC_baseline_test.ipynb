{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!git clone https://github.com/sberbank-ai/fusion_brain_aij2021.git\n",
    "# !pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install tpu_star==0.0.1rc10\n",
    "# !pip install albumentations==0.5.2\n",
    "# !pip install einops==0.3.2\n",
    "# !pip install pytorch_lightning\n",
    "# !pip install comet_ml\n",
    "# !pip install transformers==4.10.0 \n",
    "# !pip install colorednoise==1.1.1\n",
    "# !pip install catalyst==21.8 \n",
    "# !pip install opencv-python==4.5.3\n",
    "# !pip install gdown==4.0.2\n",
    "# !pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Доступные ресурсы\n",
    "import multiprocessing\n",
    "import torch\n",
    "from psutil import virtual_memory\n",
    "\n",
    "ram_gb = round(virtual_memory().total / 1024**3, 1)\n",
    "\n",
    "print('CPU:', multiprocessing.cpu_count())\n",
    "print('RAM GB:', ram_gb)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device.type)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import albumentations as A\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "from fb_utils.utils import simple_detect_lang\n",
    "from fb_utils.metrics import cer, wer, string_accuracy, acc, vqa_evaluate, detection_evaluate\n",
    "from fb_utils.c2c_eval import Beam, eval_bleu\n",
    "from fb_utils.detection_eval import cross_detection_evaluation, inverse_detection_evaluation\n",
    "from fb_utils.vqa_eval import cross_vqa_evaluation, inverse_vqa_evaluation\n",
    "from model.utils.utils import CTCLabeling\n",
    "from model.dataset.dataset import DatasetRetriever, fb_collate_fn\n",
    "from model.model import CrossAttentionGPT2FusionBrain, InverseAttentionGPT2FusionBrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных и сбор в единый DataFrame\n",
    "# #\n",
    "# Handwritten\n",
    "# #\n",
    "json_marking = json.load(open('true_fb/true_HTR.json', 'rb'))\n",
    "marking = []\n",
    "for image_name, text in json_marking.items():\n",
    "    if '%' not in text:\n",
    "        marking.append({\n",
    "            'path': image_name,\n",
    "            'text': text,\n",
    "            'lang': simple_detect_lang(text),\n",
    "        })\n",
    "df_handwritten = pd.DataFrame(marking)\n",
    "df_handwritten['stage'] = 'test'\n",
    "# Detection\n",
    "marking = []\n",
    "json_true_zsod_test = json.load(open('true_fb/true_zsOD.json', 'rb'))\n",
    "for image_name in json_true_zsod_test:\n",
    "    marking.append({\n",
    "        'task_id': 'detection',\n",
    "        'path': image_name,\n",
    "        'req': ';'.join([request for request in json_true_zsod_test[image_name].keys()]),\n",
    "        'boxes': [boxes for boxes in json_true_zsod_test[image_name].values()],\n",
    "    })\n",
    "df_detection = pd.DataFrame(marking)\n",
    "df_detection['stage'] = 'test'\n",
    "# #\n",
    "# VQA\n",
    "# #\n",
    "json_questions = json.load(open('private_fb/VQA/questions.json', 'rb'))\n",
    "json_answers = json.load(open('true_fb/true_VQA.json', 'rb'))\n",
    "marking = []\n",
    "for key in json_questions:\n",
    "    marking.append({\n",
    "        'path': json_questions[key]['file_name'],\n",
    "        'question': json_questions[key]['question'],\n",
    "        'answer': json_answers[key]['answer'][0],\n",
    "        'lang': simple_detect_lang(json_answers[key]['answer'][0])\n",
    "    })\n",
    "df_vqa = pd.DataFrame(marking)\n",
    "df_vqa['stage'] = 'test'\n",
    "# #\n",
    "# C2C\n",
    "# #\n",
    "java_json = json.load(open('private_fb/C2C/requests.json', 'rb'))\n",
    "python_json = json.load(open('true_fb/true_C2C.json', 'rb'))\n",
    "marking = []\n",
    "for key in java_json:\n",
    "    marking.append({\n",
    "        'java': java_json[key],\n",
    "        'python': python_json[key],\n",
    "    })\n",
    "df_c2c = pd.DataFrame(marking)\n",
    "df_c2c['stage'] = 'test'\n",
    "\n",
    "\n",
    "# #\n",
    "# Merge in common set\n",
    "# #\n",
    "dataset = []\n",
    "for image_name, text, stage in zip(df_handwritten['path'], df_handwritten['text'], df_handwritten['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'handwritten',   \n",
    "        'modality': 'image', \n",
    "        'input_image': image_name,\n",
    "        'output_text': text,\n",
    "        'stage': stage,\n",
    "    })\n",
    "    \n",
    "for java, python, stage in zip(df_c2c['java'], df_c2c['python'], df_c2c['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'trans',\n",
    "        'modality': 'code',    \n",
    "        'input_text': java,\n",
    "        'output_text': python,\n",
    "        'stage': stage,\n",
    "    })\n",
    "    \n",
    "for image_name, text_input, text_output, stage in zip(df_vqa['path'], df_vqa['question'], df_vqa['answer'], df_vqa['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'vqa', \n",
    "        'modality': 'image+text', \n",
    "        'input_image': image_name,\n",
    "        'input_text': text_input,\n",
    "        'output_text': text_output,\n",
    "        'stage': stage,\n",
    "    })\n",
    "for image_name, text_input, boxes, stage in zip(df_detection['path'], df_detection['req'], df_detection['boxes'], df_detection['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'detection', \n",
    "        'modality': 'image+text', \n",
    "        'input_image': image_name,\n",
    "        'input_text': text_input,\n",
    "        'output_boxes': boxes,\n",
    "        'stage': stage,\n",
    "    })\n",
    "\n",
    "random.shuffle(dataset)\n",
    "df = pd.DataFrame(dataset)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_augs = {\n",
    "    'handwritten': A.Compose([\n",
    "        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.25, always_apply=False),\n",
    "        A.Rotate(limit=3, interpolation=1, border_mode=0, p=0.5),\n",
    "        A.JpegCompression(quality_lower=75, p=0.5),\n",
    "    ], p=1.0),\n",
    "    'vqa': A.Compose([\n",
    "        A.Resize(224, 224, always_apply=True),\n",
    "        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ], p=1.0),\n",
    "    'detection': A.Compose([\n",
    "        A.Resize(224, 224, always_apply=True),\n",
    "        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ], p=1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = df[df['stage'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка предобученной модели и токенизатора, а также CTC Labeling для задачи распознавания рукописного текста\n",
    "CHARS = ' !\"#&\\'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ' + \\\n",
    "        '[]_abcdefghijklmnopqrstuvwxyz|}ЁАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЩЫЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№'\n",
    "ctc_labeling = CTCLabeling(CHARS)\n",
    "model_name = '/home/jovyan/vladimir/fb_baseline/gpt3_medium_py'\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(model_name, bos_token='<s>',\n",
    "                                              eos_token='</s>', pad_token='<pad>', unk_token='<|UNKNOWN|>',\n",
    "                                              sep_token='<|SEP|>')\n",
    "\n",
    "gpt_model = GPT2Model.from_pretrained(model_name)\n",
    "gpt_model.resize_token_embeddings(len(gpt_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = DatasetRetriever(\n",
    "    task_ids=df_eval['task_id'].values,\n",
    "    input_images=df_eval['input_image'].values,\n",
    "    input_texts=df_eval['input_text'].values,\n",
    "    output_texts=df_eval['output_text'].values,\n",
    "    output_boxes=df_eval['output_boxes'].values,\n",
    "    stage='test',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Как выглядят семплы для каждой задачи\n",
    "def demo_sample(sample):\n",
    "    if sample['task_id'] == 'handwritten':\n",
    "        print('[gt_text]:',sample['gt_text'])\n",
    "        return io.imshow(sample['image'].permute(1,2,0).numpy())\n",
    "    elif sample['task_id'] == 'trans':\n",
    "        print('[source_text]:', gpt_tokenizer.decode(sample['input_ids'].numpy(), skip_special_tokens=True))\n",
    "        print('[target_text]:', sample['target'])\n",
    "        return\n",
    "    elif sample['task_id'] == 'detection':\n",
    "        print('[boxes]:', sample['boxes'])\n",
    "        return\n",
    "    elif sample['task_id'] == 'vqa':\n",
    "        print('[question and answer]:', gpt_tokenizer.decode(sample['input_ids'].numpy(), skip_special_tokens=True))\n",
    "        return\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_sample(eval_dataset[(eval_dataset.task_ids == 'detection').argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_sample(eval_dataset[(eval_dataset.task_ids == 'trans').argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_sample(eval_dataset[(eval_dataset.task_ids == 'vqa').argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_sample(eval_dataset[(eval_dataset.task_ids == 'handwritten').argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handwritten_config = {\n",
    "    'patch_w': 8,\n",
    "    'patch_h': 128,\n",
    "    'in_layer_sizes': [8*128*3],\n",
    "    'out_layer_sizes': [64],\n",
    "    'orth_gain': 1.41,\n",
    "    'dropout': 0.1,\n",
    "    'lstm_num_layers': 3,\n",
    "    'output_dim': len(ctc_labeling), # 152\n",
    "}\n",
    "\n",
    "attention_config = {\n",
    "    'num_attention_layers': 2,\n",
    "    'num_heads': 8,\n",
    "    'pf_dim': 2048,\n",
    "}\n",
    "\n",
    "vqa_config = {\n",
    "    'tokens_num': len(gpt_tokenizer),\n",
    "}\n",
    "\n",
    "detection_config = {\n",
    "    'num_mlp_layers': 1,\n",
    "    'num_queries': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CrossAttentionGPT2FusionBrain(\n",
    "    gpt_model,\n",
    "    attention_config=attention_config,\n",
    "    handwritten_config=handwritten_config,\n",
    "    vqa_config=vqa_config,\n",
    "    detection_config=detection_config\n",
    ") # InverseAttentionGPT2FusionBrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(loader, model, tokenizer=None, device=torch.device('cuda:0')):\n",
    "    result = []\n",
    "    true_json_detection = {}\n",
    "    pred_json_detection = {}\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            (htr_images, encoded, encoded_length, gt_texts), (code_input_ids, code_input_labels, code_targets), (vqa_images, vqa_input_ids, labels, targets), (detection_names, detection_images, detection_input_ids, detection_attention_masks, boxes, size) = batch\n",
    "            if len(htr_images) > 0:\n",
    "                images = htr_images.to(device, dtype=torch.float32)\n",
    "                handwritten_outputs = model('handwritten', images=images)\n",
    "                for encoded, gt_text in zip(handwritten_outputs.argmax(2).data.cpu().numpy(), gt_texts):\n",
    "                    pred_text = ctc_labeling.decode(encoded)\n",
    "                    result.append({\n",
    "                        'task_id': 'handwritten',\n",
    "                        'gt_output': gt_text,\n",
    "                        'pred_output': pred_text,\n",
    "                    })\n",
    "\n",
    "            if len(code_input_ids) > 0:\n",
    "                code_input_ids = code_input_ids.to(device, dtype=torch.long)\n",
    "                code_input_labels = code_input_labels.to(device, dtype=torch.long)\n",
    "                loss_mask = torch.tensor(code_input_labels.clone().detach() == 2, dtype=torch.uint8)\n",
    "                loss_mask = loss_mask.to(device)\n",
    "                hidden_states = model('trans', input_ids=code_input_ids, input_labels=code_input_labels, eval_bleu=True)\n",
    "                bleu_score, _ = eval_bleu(model, hidden_states, input_ids=code_input_ids, beam_size=5, tokenizer=tokenizer, targets=code_targets)\n",
    "                result.append({\n",
    "                        'task_id': 'trans',\n",
    "                        'true_text': code_targets,\n",
    "                        'bleu_score': bleu_score,\n",
    "                })\n",
    "                \n",
    "            if len(labels) > 0:\n",
    "                images = vqa_images.to(device, dtype=torch.float32)\n",
    "                input_ids = vqa_input_ids.to(device, dtype=torch.long)\n",
    "                labels = labels.to(device, dtype=torch.float) \n",
    "                attention_mask = torch.tensor(labels.clone().detach() != 0, dtype=torch.uint8)\n",
    "                attention_mask = attention_mask.to(labels.device)\n",
    "                vqa_outputs = cross_vqa_evaluation(model, images, input_ids, attention_mask, 10)\n",
    "                for target, pred_labels in zip(targets, vqa_outputs.argmax(-1).cpu().numpy()):\n",
    "                    result.append({\n",
    "                        'task_id': 'vqa',\n",
    "                        'gt_output': target,\n",
    "                        'pred_output': gpt_tokenizer.decode(pred_labels),\n",
    "                    })\n",
    "                    \n",
    "            if len(boxes) > 0:\n",
    "                images = detection_images.to(device, dtype=torch.float32)\n",
    "                input_ids = [input_id.to(device, dtype=torch.long) for input_id in detection_input_ids]\n",
    "                attention_masks = [attention_mask.to(device, dtype=torch.long) for attention_mask in detection_attention_masks]\n",
    "                detection_outputs = cross_detection_evaluation(model, images, input_ids, attention_masks, 0.2)\n",
    "                img_h, img_w = size[0]\n",
    "                for i in range(len(detection_outputs)):\n",
    "                    if detection_outputs[i].numel() != 0:\n",
    "                        detection_outputs[i][:, 0] = detection_outputs[i][:, 0] * img_w\n",
    "                        detection_outputs[i][:, 2] = detection_outputs[i][:, 2] * img_w\n",
    "                        detection_outputs[i][:, 1] = detection_outputs[i][:, 1] * img_h\n",
    "                        detection_outputs[i][:, 3] = detection_outputs[i][:, 3] * img_h\n",
    "                image_name = detection_names[0]\n",
    "                for boxes_for_img in boxes:\n",
    "                    true_json_detection[image_name] = boxes_for_img\n",
    "                    pred_json_detection[image_name] = {\n",
    "                        input_text: output.type(torch.int32).cpu().tolist()\n",
    "                        for input_text, output in zip(boxes_for_img.keys(), detection_outputs)\n",
    "                    }\n",
    "                result.append({\n",
    "                        'task_id': 'detection',\n",
    "                    })\n",
    "                \n",
    "    result = pd.DataFrame(result)\n",
    "\n",
    "    handwritten_result = result[result['task_id'] == 'handwritten']\n",
    "    if handwritten_result.shape[0]:\n",
    "        print('= Handwritten =')\n",
    "        print('CER:', round(cer(handwritten_result['pred_output'], handwritten_result['gt_output']), 3))\n",
    "        print('WER:', round(wer(handwritten_result['pred_output'], handwritten_result['gt_output']), 3))\n",
    "        print('ACC:', round(string_accuracy(handwritten_result['pred_output'], handwritten_result['gt_output']), 3))\n",
    "        print('=== === === ===')\n",
    "        \n",
    "    trans_result = result[result['task_id'] == 'trans']   \n",
    "    if trans_result.shape[0]:\n",
    "        print('== C2C ==')\n",
    "        print('meanBLEU:', np.mean(trans_result['bleu_score']))\n",
    "        print('=== === === ===')\n",
    "        \n",
    "    vqa_result = result[result['task_id'] == 'vqa']\n",
    "    if vqa_result.shape[0]:\n",
    "        print('== VQA ==')\n",
    "        print('ACC:', round(vqa_evaluate(vqa_result), 3))\n",
    "        print('=== === === ===')\n",
    "        \n",
    "    \n",
    "    if len(true_json_detection):\n",
    "        print('== Detection ==')\n",
    "        print('ACC:', round(detection_evaluate(true_json_detection, pred_json_detection), 3))\n",
    "        print('=== === === ===')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_valid = df[(df['task_id'] == 'detection') & (df['stage'] == 'test')]\n",
    "\n",
    "vqa_eval_dataset = DatasetRetriever(\n",
    "    task_ids=vqa_valid['task_id'].values,\n",
    "    input_images=vqa_valid['input_image'].values,\n",
    "    input_texts=vqa_valid['input_text'].values,\n",
    "    output_texts=vqa_valid['output_text'].values,\n",
    "    output_boxes=vqa_valid['output_boxes'].values,\n",
    "    stage='test',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "state_dict = torch.load('LightningExperimentsNew/main_concat_small/checkpoints/weight-epoch=118.ckpt')['state_dict']\n",
    "state_dict = OrderedDict({key[6:]: value for key, value in state_dict.items()})\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    vqa_eval_dataset,\n",
    "    batch_size=1,\n",
    "    sampler=SequentialSampler(vqa_eval_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "evaluation_result = run_evaluation(valid_loader, model, tokenizer=gpt_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
