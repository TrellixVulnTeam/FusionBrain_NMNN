{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e84814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.8.0 torchvision==0.8.2 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install tpu_star==0.0.1rc10\n",
    "# !pip install albumentations==0.5.2\n",
    "# !pip install einops==0.3.2 \n",
    "# !pip install transformers==4.10.0\n",
    "# !pip install adapter-transformers\n",
    "# !pip install colorednoise==1.1.1\n",
    "# !pip install catalyst==21.8 \n",
    "# !pip install opencv-python==4.5.3\n",
    "# !pip install gdown==4.0.2\n",
    "# !pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "642ed6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 96\n",
      "RAM GB: 1510.6\n",
      "PyTorch version: 1.7.1+cu101\n",
      "CUDA version: 10.1\n",
      "cuDNN version: 7603\n",
      "device: cuda\n",
      "Tue Nov  2 00:07:39 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.129      Driver Version: 410.129      CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM3...  On   | 00000000:57:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    48W / 350W |     13MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Доступные ресурсы\n",
    "import multiprocessing\n",
    "import torch\n",
    "from psutil import virtual_memory\n",
    "\n",
    "ram_gb = round(virtual_memory().total / 1024**3, 1)\n",
    "\n",
    "print('CPU:', multiprocessing.cpu_count())\n",
    "print('RAM GB:', ram_gb)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device.type)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2f7529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 00:07:44.354191: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "fusion_brain_aij2021/fb_baseline/fb_utils/metrics.py:73: DeprecationWarning: invalid escape sequence \\d\n",
      "  self.periodStrip = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\n",
      "fusion_brain_aij2021/fb_baseline/fb_utils/metrics.py:74: DeprecationWarning: invalid escape sequence \\d\n",
      "  self.commaStrip = re.compile(\"(\\d)(\\,)(\\d)\")\n",
      "/home/user/conda/lib/python3.7/site-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from skimage import io\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from catalyst.data import BalanceClassSampler, DistributedSamplerWrapper\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tpu_star.experiment import TorchGPUExperiment\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'fusion_brain_aij2021/fb_baseline')\n",
    "from fb_utils.download import download_and_extract\n",
    "from fb_utils.loss import LabelSmoothing, onehot\n",
    "from fb_utils.metrics import cer, wer, string_accuracy, acc, vqa_evaluate, detection_evaluate\n",
    "from fb_utils.handwritten import simple_detect_lang, CTCLabeling, resize_if_need, make_img_padding\n",
    "from fb_utils.BLEU import _bleu\n",
    "from fb_utils.c2c_eval import Beam, eval_bleu\n",
    "from fb_utils.detection_vqa import (vqa_evaluation, detection_evaluation, CrossAttentionLayer, MLP,\n",
    "                                    FeedForwardComponent, DetectionCriterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55a0a891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>modality</th>\n",
       "      <th>lang</th>\n",
       "      <th>input_image</th>\n",
       "      <th>input_text</th>\n",
       "      <th>output_boxes</th>\n",
       "      <th>stage</th>\n",
       "      <th>output_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>detection</td>\n",
       "      <td>image+text</td>\n",
       "      <td>ru</td>\n",
       "      <td>2390398.jpg</td>\n",
       "      <td>бейсболист с согнутой ногой</td>\n",
       "      <td>[[163.0, 138.0, 63.0, 77.0]]</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>detection</td>\n",
       "      <td>image+text</td>\n",
       "      <td>en</td>\n",
       "      <td>2385519.jpg</td>\n",
       "      <td>wheels of an aeroplane</td>\n",
       "      <td>[[228.0, 288.0, 62.0, 41.0]]</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vqa</td>\n",
       "      <td>image+text</td>\n",
       "      <td>en</td>\n",
       "      <td>2376590.jpg</td>\n",
       "      <td>What color is the surfboard?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>detection</td>\n",
       "      <td>image+text</td>\n",
       "      <td>en</td>\n",
       "      <td>2365646.jpg</td>\n",
       "      <td>top of a rock wall;shadows on green grass;thre...</td>\n",
       "      <td>[[[32.0, 131.0, 80.0, 57.0]], [[122.0, 138.0, ...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>detection</td>\n",
       "      <td>image+text</td>\n",
       "      <td>ru</td>\n",
       "      <td>2337502.jpg</td>\n",
       "      <td>нос на лице ребенка</td>\n",
       "      <td>[[198.0, 198.0, 32.0, 28.0]]</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>detection</td>\n",
       "      <td>image+text</td>\n",
       "      <td>ru</td>\n",
       "      <td>2404461.jpg</td>\n",
       "      <td>сине-белая повязка на голове</td>\n",
       "      <td>[[300.0, 32.0, 35.0, 25.0]]</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vqa</td>\n",
       "      <td>image+text</td>\n",
       "      <td>en</td>\n",
       "      <td>2361912.jpg</td>\n",
       "      <td>Where are the gummy worms?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>on the black tray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>detection</td>\n",
       "      <td>image+text</td>\n",
       "      <td>ru</td>\n",
       "      <td>2405628.jpg</td>\n",
       "      <td>секция белой кухонной плитки</td>\n",
       "      <td>[[371.0, 219.0, 110.0, 112.0]]</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>handwritten</td>\n",
       "      <td>image</td>\n",
       "      <td>ru</td>\n",
       "      <td>8597.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>на</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>detection</td>\n",
       "      <td>image+text</td>\n",
       "      <td>ru</td>\n",
       "      <td>2363671.jpg</td>\n",
       "      <td>большой ствол дерева</td>\n",
       "      <td>[[260.0, 14.0, 87.0, 207.0]]</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       task_id    modality lang  input_image  \\\n",
       "0    detection  image+text   ru  2390398.jpg   \n",
       "1    detection  image+text   en  2385519.jpg   \n",
       "2          vqa  image+text   en  2376590.jpg   \n",
       "3    detection  image+text   en  2365646.jpg   \n",
       "4    detection  image+text   ru  2337502.jpg   \n",
       "5    detection  image+text   ru  2404461.jpg   \n",
       "6          vqa  image+text   en  2361912.jpg   \n",
       "7    detection  image+text   ru  2405628.jpg   \n",
       "8  handwritten       image   ru     8597.png   \n",
       "9    detection  image+text   ru  2363671.jpg   \n",
       "\n",
       "                                          input_text  \\\n",
       "0                        бейсболист с согнутой ногой   \n",
       "1                             wheels of an aeroplane   \n",
       "2                       What color is the surfboard?   \n",
       "3  top of a rock wall;shadows on green grass;thre...   \n",
       "4                                нос на лице ребенка   \n",
       "5                       сине-белая повязка на голове   \n",
       "6                         Where are the gummy worms?   \n",
       "7                       секция белой кухонной плитки   \n",
       "8                                                NaN   \n",
       "9                               большой ствол дерева   \n",
       "\n",
       "                                        output_boxes  stage        output_text  \n",
       "0                       [[163.0, 138.0, 63.0, 77.0]]  train                NaN  \n",
       "1                       [[228.0, 288.0, 62.0, 41.0]]  train                NaN  \n",
       "2                                                NaN  train             yellow  \n",
       "3  [[[32.0, 131.0, 80.0, 57.0]], [[122.0, 138.0, ...   test                NaN  \n",
       "4                       [[198.0, 198.0, 32.0, 28.0]]  train                NaN  \n",
       "5                        [[300.0, 32.0, 35.0, 25.0]]  train                NaN  \n",
       "6                                                NaN  train  on the black tray  \n",
       "7                     [[371.0, 219.0, 110.0, 112.0]]  train                NaN  \n",
       "8                                                NaN  train                 на  \n",
       "9                       [[260.0, 14.0, 87.0, 207.0]]  train                NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Подготовка данных и сбор в единый DataFrame\n",
    "# #\n",
    "# Handwritten\n",
    "# #\n",
    "json_marking = json.load(open('handwritten/train_labels.json', 'rb'))\n",
    "marking = []\n",
    "for image_name, text in json_marking.items():\n",
    "    marking.append({\n",
    "        'path': image_name,\n",
    "        'text': text,\n",
    "        'lang': simple_detect_lang(text),\n",
    "    })\n",
    "df_handwritten = pd.DataFrame(marking)\n",
    "df_handwritten['stage'] = 'train'\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "train_index, valid_index = next(skf.split(df_handwritten.index, df_handwritten['lang']))\n",
    "df_handwritten.loc[valid_index, 'stage'] = 'valid'\n",
    "# #\n",
    "# Detection\n",
    "# #\n",
    "json_true_zsod = json.load(open('russian_detection_vqa/vg_intersection_eng.json', 'rb'))\n",
    "json_true_ru_zsod = json.load(open('russian_detection_vqa/vg_intersection_rus.json', 'rb'))\n",
    "json_true_zsod.update(json_true_ru_zsod)\n",
    "ru_images = set(json_true_ru_zsod.keys())\n",
    "marking = []\n",
    "for image_name in json_true_zsod:\n",
    "    marking.extend([{\n",
    "        'task_id': 'detection',\n",
    "        'path': image_name,\n",
    "        'req': request,\n",
    "        'boxes': boxes,\n",
    "        'lang': 'ru' if image_name in ru_images else 'en'\n",
    "    } for request, boxes in json_true_zsod[image_name].items() if boxes])\n",
    "df_detection = pd.DataFrame(marking)\n",
    "df_detection['stage'] = 'train'\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "train_index, valid_index = next(skf.split(df_detection.index, df_detection['lang']))\n",
    "df_detection.loc[valid_index, 'stage'] = 'valid'\n",
    "# #\n",
    "# Eval Detection #\n",
    "# #\n",
    "valid_detection_images = set(df_detection[df_detection['stage'] == 'valid']['path'])\n",
    "marking = []\n",
    "for image_name in json_true_zsod:\n",
    "    if image_name in valid_detection_images:\n",
    "        marking.append({\n",
    "            'task_id': 'detection',\n",
    "            'path': image_name,\n",
    "            'req': ';'.join([request for request in json_true_zsod[image_name].keys()]),\n",
    "            'boxes': [boxes for boxes in json_true_zsod[image_name].values()],\n",
    "            'lang': 'ru' if image_name in ru_images else 'en'\n",
    "        })\n",
    "df_eval_detection = pd.DataFrame(marking)\n",
    "df_eval_detection['stage'] = 'test'\n",
    "# #\n",
    "# VQA\n",
    "# #\n",
    "json_questions = json.load(open('train_detection_vqa/vqa_questions.json', 'rb'))\n",
    "json_true_vqa = json.load(open('train_detection_vqa/vqa_answers.json', 'rb'))\n",
    "marking = []\n",
    "for key in json_questions:\n",
    "    if json_true_vqa[key]['lang'] == 'en':\n",
    "        marking.append({\n",
    "            'path': json_questions[key]['file_name'],\n",
    "            'question': json_questions[key]['question'],\n",
    "            'answer': json_true_vqa[key]['answer'],\n",
    "            'lang': json_true_vqa[key]['lang']\n",
    "        })\n",
    "df_vqa = pd.DataFrame(marking)\n",
    "df_vqa['stage'] = 'train'\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "train_index, valid_index = next(skf.split(df_vqa.index, df_vqa['lang']))\n",
    "df_vqa.loc[valid_index, 'stage'] = 'valid'\n",
    "# #\n",
    "# C2C\n",
    "# #\n",
    "df_c2c = pd.read_json(path_or_buf='c2c/java-python.jsonl', lines=True)\n",
    "train, test = train_test_split(df_c2c, test_size=0.2)\n",
    "valid, test = train_test_split(test, test_size=0.05)\n",
    "\n",
    "df_c2c.loc[train.index.to_list(), 'stage'] = 'train'\n",
    "df_c2c.loc[valid.index.to_list(), 'stage'] = 'valid'\n",
    "df_c2c.loc[test.index.to_list(), 'stage'] = 'test'\n",
    "\n",
    "\n",
    "# #\n",
    "# Merge in common set\n",
    "# #\n",
    "dataset = []\n",
    "for lang, image_name, text, stage in zip(df_handwritten['lang'], df_handwritten['path'], df_handwritten['text'], df_handwritten['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'handwritten',   \n",
    "        'modality': 'image',\n",
    "        'lang': lang,\n",
    "        'input_image': image_name,\n",
    "        'output_text': text,\n",
    "        'stage': stage,\n",
    "    })\n",
    "    \n",
    "for java, python, stage in zip(df_c2c['java'], df_c2c['python'], df_c2c['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'trans',\n",
    "        'modality': 'code',\n",
    "        'input_text': java,\n",
    "        'output_text': python,\n",
    "        'stage': stage,\n",
    "    })\n",
    "    \n",
    "for lang, image_name, text_input, text_output, stage in zip(df_vqa['lang'], df_vqa['path'], df_vqa['question'], df_vqa['answer'], df_vqa['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'vqa', \n",
    "        'modality': 'image+text',\n",
    "        'lang': lang,\n",
    "        'input_image': image_name,\n",
    "        'input_text': text_input,\n",
    "        'output_text': text_output,\n",
    "        'stage': stage,\n",
    "    })\n",
    "for lang, image_name, text_input, boxes, stage in zip(df_detection['lang'], df_detection['path'], df_detection['req'], df_detection['boxes'], df_detection['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'detection', \n",
    "        'modality': 'image+text',\n",
    "        'lang': lang,\n",
    "        'input_image': image_name,\n",
    "        'input_text': text_input,\n",
    "        'output_boxes': boxes,\n",
    "        'stage': stage,\n",
    "    })\n",
    "for lang, image_name, text_input, boxes, stage in zip(df_eval_detection['lang'], df_eval_detection['path'], df_eval_detection['req'], df_eval_detection['boxes'], df_eval_detection['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'detection', \n",
    "        'modality': 'image+text',\n",
    "        'lang': lang,\n",
    "        'input_image': image_name,\n",
    "        'input_text': text_input,\n",
    "        'output_boxes': boxes,\n",
    "        'stage': stage,\n",
    "    })\n",
    "\n",
    "random.shuffle(dataset)\n",
    "df = pd.DataFrame(dataset)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b5f321c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['modality'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f596275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, \n",
    "                 task_ids, \n",
    "                 input_images,\n",
    "                 input_texts,\n",
    "                 output_texts,\n",
    "                 output_boxes,\n",
    "                 tokenizer, \n",
    "                 stage,\n",
    "                 handwritten_max_tokens_length,\n",
    "                 max_request_tokens_length,\n",
    "                 vqa_max_tokens_length, \n",
    "                 task_augs=None):\n",
    "        super().__init__()\n",
    "        self.task_ids = task_ids\n",
    "        \n",
    "        self.input_images = input_images\n",
    "        self.input_texts = input_texts\n",
    "        self.output_texts = output_texts\n",
    "        \n",
    "        self.task_augs = task_augs or {}\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stage = stage\n",
    "\n",
    "        # handwritten[image]:\n",
    "        self.handwritten_max_tokens_length = handwritten_max_tokens_length\n",
    "        self.handwritten_image_w = 512\n",
    "        self.handwritten_image_h = 128\n",
    "\n",
    "        # code2code\n",
    "        self.code_max_length = 512\n",
    "        \n",
    "        # detection[image, text]:\n",
    "        self.max_request_tokens_length = max_request_tokens_length\n",
    "        self.output_boxes = output_boxes\n",
    "        \n",
    "        # vqa[image, text]:\n",
    "        self.vqa_max_tokens_length = vqa_max_tokens_length\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        task_id = self.task_ids[idx]\n",
    "        if task_id == 'handwritten':\n",
    "            return self.get_handwritten_sample(idx)\n",
    "        elif task_id == 'trans':\n",
    "            return self.get_trans_sample(idx)\n",
    "        elif task_id == 'detection':\n",
    "            return self.get_detection_sample(idx)\n",
    "        elif task_id == 'vqa':\n",
    "            return self.get_vqa_sample(idx)\n",
    "        return {'task_id': task_id}\n",
    "\n",
    "    def get_trans_sample(self, idx):\n",
    "            \n",
    "        source = self.input_texts[idx]\n",
    "        encoded_source = self.tokenizer.encode(str(source))\n",
    "        target = self.output_texts[idx]\n",
    "        encoded_target = self.tokenizer.encode(str(target))\n",
    "        \n",
    "        input_ids, input_labels = self.pad_and_get_mask(encoded_target, encoded_source, self.tokenizer)\n",
    "        input_ids, input_labels = torch.tensor(input_ids), torch.tensor(input_labels)\n",
    "        \n",
    "        return {\n",
    "            'task_id': self.task_ids[idx],\n",
    "            'input_ids': input_ids,\n",
    "            'input_labels': input_labels,\n",
    "            'target': target\n",
    "        }\n",
    "\n",
    "    def get_handwritten_sample(self, idx):\n",
    "        path = 'handwritten/images/' + self.input_images[idx]\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image, _ = self.resize_image(image)\n",
    "\n",
    "        gt_text = self.output_texts[idx]\n",
    "        tokens = self.tokenizer.encode(gt_text)[:6]\n",
    "        encoded = [self.tokenizer.bos_token_id] + tokens + [self.tokenizer.eos_token_id]\n",
    "        labels = [2] * (len(tokens) + 1) + [0]\n",
    "        \n",
    "        pad_len = self.handwritten_max_tokens_length - len(encoded)\n",
    "        encoded += [self.tokenizer.pad_token_id] * pad_len\n",
    "        labels += [0] * pad_len\n",
    "\n",
    "        ## Augs ##\n",
    "        transforms = self.task_augs.get('handwritten')\n",
    "        if transforms:\n",
    "            image = transforms(image=image)['image']\n",
    "        ##########\n",
    "\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "\n",
    "        return {\n",
    "            'task_id': self.task_ids[idx],\n",
    "            'image': image,\n",
    "            'gt_text': gt_text,\n",
    "            'labels': torch.tensor(labels),\n",
    "            'encoded': torch.tensor(encoded),\n",
    "        }\n",
    "    \n",
    "    def get_detection_sample(self, idx):\n",
    "        path = 'russian_detection_vqa/images/' + self.input_images[idx]\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image_h, image_w, _ = image.shape\n",
    "        \n",
    "        ## Augs ##\n",
    "        transforms = self.task_augs.get('detection')\n",
    "        if transforms:\n",
    "            image = transforms(image=image)['image']\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        image_name = self.input_images[idx]\n",
    "        ##########\n",
    "        \n",
    "        ## Input tokens ##\n",
    "        if self.stage == 'train' or self.stage == 'valid':\n",
    "            input_text = self.input_texts[idx]\n",
    "            input_tokens = self.tokenizer.encode_plus(input_text)\n",
    "            input_tokens['input_ids'] = input_tokens['input_ids'][:21]\n",
    "            input_tokens['attention_mask'] = input_tokens['attention_mask'][:21]\n",
    "            pad_len = self.max_request_tokens_length - len(input_tokens['input_ids'])\n",
    "            input_tokens['input_ids'] += [self.tokenizer.pad_token_id] * pad_len\n",
    "            input_tokens['attention_mask'] += [0] * pad_len\n",
    "            input_ids = torch.tensor(input_tokens['input_ids'])\n",
    "            attention_mask = torch.tensor(input_tokens['attention_mask'])\n",
    "        else:\n",
    "            input_texts = self.input_texts[idx].split(';')\n",
    "            input_ids = list(map(self.tokenizer.encode, input_texts))\n",
    "            attention_mask = [[1 for _ in input_token] for input_token in input_ids]\n",
    "            input_ids = [torch.tensor(input_id) for input_id in input_ids]\n",
    "            attention_mask = [torch.tensor(mask) for mask in attention_mask]\n",
    "        ###########\n",
    "        \n",
    "        ## Boxes ##\n",
    "        output_boxes = self.output_boxes[idx]\n",
    "        if self.stage == 'train' or self.stage == 'valid':\n",
    "            output_boxes = torch.tensor(output_boxes, dtype=torch.float32)\n",
    "            output_boxes[:, 0] /= image_w\n",
    "            output_boxes[:, 1] /= image_h\n",
    "            output_boxes[:, 2] /= image_w\n",
    "            output_boxes[:, 3] /= image_h\n",
    "        else:\n",
    "            output_boxes = {\n",
    "                input_text: boxes for input_text, boxes in zip(input_texts, output_boxes)\n",
    "            }\n",
    "        ##########\n",
    "        \n",
    "        return {\n",
    "            'task_id': self.task_ids[idx],\n",
    "            'image_name': image_name,\n",
    "            'image': image,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'boxes': output_boxes,\n",
    "            'size': (image_h, image_w)\n",
    "        }\n",
    "    \n",
    "    def get_vqa_sample(self, idx): \n",
    "        path = 'train_detection_vqa/images/' + self.input_images[idx]\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        ## Augs ##\n",
    "        transforms = self.task_augs.get('vqa')\n",
    "        if transforms:\n",
    "            image = transforms(image=image)['image']\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        image_name = self.input_images[idx]\n",
    "        ##########\n",
    "        \n",
    "        ## Question and Answer ##\n",
    "        input_text = self.input_texts[idx]\n",
    "        input_tokens = self.tokenizer.encode(input_text)\n",
    "        output_text = self.output_texts[idx]\n",
    "        output_tokens = self.tokenizer.encode(output_text)\n",
    "        \n",
    "        if self.stage == 'train' or self.stage == 'valid':\n",
    "            input_tokens, output_tokens = input_tokens[:12], output_tokens[:7]\n",
    "            input_ids = input_tokens + [self.tokenizer.bos_token_id] + output_tokens + [self.tokenizer.eos_token_id]\n",
    "            labels = [1] * len(input_tokens) + [2] * (len(output_tokens) + 1) + [0]\n",
    "            \n",
    "            pad_len = self.vqa_max_tokens_length - len(input_ids)\n",
    "            input_ids += [self.tokenizer.pad_token_id] * pad_len\n",
    "            labels += [0] * pad_len\n",
    "        else:\n",
    "            input_ids = input_tokens + [self.tokenizer.bos_token_id]\n",
    "            labels = [1] * len(input_tokens) + [2]\n",
    "        ##########\n",
    "        \n",
    "        return {\n",
    "            'task_id': self.task_ids[idx],\n",
    "            'image_name': image_name,\n",
    "            'image': image,\n",
    "            'input_ids': torch.tensor(input_ids),\n",
    "            'labels': torch.tensor(labels),\n",
    "            'target': output_text\n",
    "        }\n",
    "\n",
    "\n",
    "    def resize_image(self, image):\n",
    "        image, coef = resize_if_need(image, self.handwritten_image_h, self.handwritten_image_w)\n",
    "        image = make_img_padding(image, self.handwritten_image_h, self.handwritten_image_w)\n",
    "        return image, coef\n",
    "    \n",
    "    def pad_and_get_mask(self, target, source, tokenizer):\n",
    "        if self.stage == 'test':\n",
    "            target = []\n",
    "        while len(target) + len(source) + 2 > self.code_max_length:\n",
    "            if len(target) > len(source):\n",
    "                target = target[:-1]\n",
    "            else:\n",
    "                source = source[:-1]\n",
    "        if self.stage == 'train' or self.stage == 'valid':\n",
    "            inputs = source + [tokenizer.bos_token_id] + target + [tokenizer.eos_token_id]\n",
    "            labels = [1] * len(source) + [2] * (len(target) + 1) + [0]\n",
    "\n",
    "        else:\n",
    "            inputs = source + [tokenizer.bos_token_id]\n",
    "            labels = [1] * len(source) + [2]\n",
    "            \n",
    "            return inputs, labels\n",
    "        \n",
    "        assert len(inputs) <= self.code_max_length\n",
    "        pad_len = self.code_max_length - len(inputs)\n",
    "        inputs += [tokenizer.pad_token_id] * pad_len\n",
    "        labels += [0] * pad_len\n",
    "        assert len(inputs) == len(labels)\n",
    "        \n",
    "        return inputs, labels\n",
    "\n",
    " \n",
    "    def __len__(self) -> int:\n",
    "        return self.task_ids.shape[0]\n",
    "\n",
    "    def get_task_labels(self):\n",
    "        return list(self.task_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b09d2e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:1748: DeprecationWarning: This class has been deprecated. Please use ImageCompression\n",
      "  warnings.warn(\"This class has been deprecated. Please use ImageCompression\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "task_augs = {\n",
    "    'handwritten': A.Compose([\n",
    "        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.25, always_apply=False),\n",
    "        A.Rotate(limit=3, interpolation=1, border_mode=0, p=0.5),\n",
    "        A.JpegCompression(quality_lower=75, p=0.5),\n",
    "    ], p=1.0),\n",
    "    'vqa': A.Compose([\n",
    "        A.Resize(224, 224, always_apply=True),\n",
    "        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ], p=1.0),\n",
    "    'detection': A.Compose([\n",
    "        A.Resize(224, 224, always_apply=True),\n",
    "        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ], p=1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11f107d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['stage'] == 'train']\n",
    "df_valid = df[df['stage'] == 'valid']\n",
    "df_eval = df[df['stage'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6553b20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d45c6c2fd244a6b9b4cfd13f59d9730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.63M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d091c818e548708f68576ba1d7ea50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.21M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at sberbank-ai/rugpt3small_based_on_gpt2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(model_name, bos_token='<s>',\n",
    "            eos_token='</s>', pad_token='<pad>', unk_token='<|UNKNOWN|>', sep_token='<|SEP|>')\n",
    "\n",
    "gpt_model = GPT2Model.from_pretrained(model_name)\n",
    "gpt_model.resize_token_embeddings(len(gpt_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "075b9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetRetriever(\n",
    "    task_ids=df_train['task_id'].values,\n",
    "    input_images=df_train['input_image'].values,\n",
    "    input_texts=df_train['input_text'].values,\n",
    "    output_texts=df_train['output_text'].values,\n",
    "    output_boxes=df_train['output_boxes'].values,\n",
    "    stage='train',\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    handwritten_max_tokens_length=8,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "valid_dataset = DatasetRetriever(\n",
    "    task_ids=df_valid['task_id'].values,\n",
    "    input_images=df_valid['input_image'].values,\n",
    "    input_texts=df_valid['input_text'].values,\n",
    "    output_texts=df_valid['output_text'].values,\n",
    "    output_boxes=df_valid['output_boxes'].values,\n",
    "    stage='valid',\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    handwritten_max_tokens_length=8,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "eval_dataset = DatasetRetriever(\n",
    "    task_ids=df_eval['task_id'].values,\n",
    "    input_images=df_eval['input_image'].values,\n",
    "    input_texts=df_eval['input_text'].values,\n",
    "    output_texts=df_eval['output_text'].values,\n",
    "    output_boxes=df_eval['output_boxes'].values,\n",
    "    stage='test',\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    handwritten_max_tokens_length=8,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3502b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Как выглядят семплы для каждой задачи\n",
    "def demo_sample(sample):\n",
    "    if sample['task_id'] == 'handwritten':\n",
    "        print('[gt_text]:',sample['gt_text'])\n",
    "        return io.imshow(sample['image'].permute(1,2,0).numpy())\n",
    "    elif sample['task_id'] == 'trans':\n",
    "        print('[source_text]:', gpt_tokenizer.decode(sample['input_ids'].numpy(), skip_special_tokens=True))\n",
    "        print('[target_text]:', sample['target'])\n",
    "        return\n",
    "    elif sample['task_id'] == 'detection':\n",
    "        print('[input_text]:', gpt_tokenizer.decode(sample['input_ids'].numpy(), skip_special_tokens=True))\n",
    "        print('[boxes]:', sample['boxes'].numpy())\n",
    "        return\n",
    "    elif sample['task_id'] == 'vqa':\n",
    "        print('[question and answer]:', gpt_tokenizer.decode(sample['input_ids'].numpy(), skip_special_tokens=True))\n",
    "        return\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e1f4044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[input_text]: бейсболист с согнутой ногой\n",
      "[boxes]: [[0.326      0.41317365 0.126      0.23053892]]\n"
     ]
    }
   ],
   "source": [
    "demo_sample(train_dataset[(train_dataset.task_ids == 'detection').argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b44402f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[source_text]: import java. util. ArrayList ; import java. util. List ; import java. util. Scanner ; public class Main { public static void main ( String [ ] args ) { Scanner scanner = new Scanner ( System. in ) ; int N = scanner. nextInt ( ) ; Node [ ] nodes = new Node [ N ] ; for ( int i = 0 ; i < N ; i ++ ) nodes [ i ] = new Node ( i ) ; for ( int i = 0 ; i < N - 1 ; i ++ ) { int a = scanner. nextInt ( ) - 1 ; int b = scanner. nextInt ( ) - 1 ; int c = scanner. nextInt ( ) ; nodes [ a ]. edges. add ( new Edge ( nodes [ b ], c ) ) ; nodes [ b ]. edges. add ( new Edge ( nodes [ a ], c ) ) ; } int Q = scanner. nextInt ( ) ; int K = scdef solve ( N, ABCs, Q, K, XYs ) : NEW_LINE INDENT path = [ [ ] for _ in range ( N + 1 ) ] NEW_LINE for ( a, b, c ) in ABCs : NEW_LINE INDENT path [ a ]. append ( ( b, c ) ) NEW_LINE path [ b ]. append ( ( a, c ) ) NEW_LINE DEDENT dst = [ 0 for _ in range ( N + 1 ) ] NEW_LINE q = [ K ] NEW_LINE while len ( q ) : NEW_LINE INDENT index = q. pop ( ) NEW_LINE for n, w in path [ index ] : NEW_LINE INDENT if dst [ n ] == 0 : NEW_LINE INDENT dst [ n ] = dst [ index ] + w NEW_LINE q. append\n",
      "[target_text]: def solve ( N , ABCs , Q , K , XYs ) : NEW_LINE INDENT path = [ [ ] for _ in range ( N + 1 ) ] NEW_LINE for ( a , b , c ) in ABCs : NEW_LINE INDENT path [ a ] . append ( ( b , c ) ) NEW_LINE path [ b ] . append ( ( a , c ) ) NEW_LINE DEDENT dst = [ 0 for _ in range ( N + 1 ) ] NEW_LINE q = [ K ] NEW_LINE while len ( q ) : NEW_LINE INDENT index = q . pop ( ) NEW_LINE for n , w in path [ index ] : NEW_LINE INDENT if dst [ n ] == 0 : NEW_LINE INDENT dst [ n ] = dst [ index ] + w NEW_LINE q . append ( n ) NEW_LINE DEDENT DEDENT DEDENT ans = [ ] NEW_LINE for x , y in XYs : NEW_LINE INDENT ans . append ( str ( dst [ x ] + dst [ y ] ) ) NEW_LINE DEDENT return \" \\n \" . join ( ans ) NEW_LINE DEDENT if __name__ == \" _ _ main _ _ \" : NEW_LINE INDENT N = int ( input ( ) ) NEW_LINE ABCs = [ tuple ( map ( int , input ( ) . split ( \" ▁ \" ) ) ) for _ in range ( N - 1 ) ] NEW_LINE Q , K = tuple ( map ( int , input ( ) . split ( \" ▁ \" ) ) ) NEW_LINE XYs = [ tuple ( map ( int , input ( ) . split ( \" ▁ \" ) ) ) for _ in range ( Q ) ] NEW_LINE print ( solve ( N , ABCs , Q , K , XYs ) ) NEW_LINE DEDENT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_sample(train_dataset[(train_dataset.task_ids == 'trans').argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd0603b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[question and answer]: What color is the surfboard?yellow\n"
     ]
    }
   ],
   "source": [
    "demo_sample(train_dataset[(train_dataset.task_ids == 'vqa').argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdf5a195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gt_text]: на\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff6c0ebc790>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo_sample(train_dataset[(train_dataset.task_ids == 'handwritten').argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93ae67ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fb_collate_fn(batch):\n",
    "    \"\"\" fusion brain collate fn \"\"\"\n",
    "    encoded, htr_labels, htr_images, gt_texts = [], [], [], [] # handwritten[image]\n",
    "    code_input_ids, code_input_labels, code_targets = [], [], [] #code\n",
    "    vqa_images, vqa_input_ids, labels, targets = [], [], [], []  # vqa[image, text]\n",
    "    detection_names, detection_images, detection_input_ids, detection_attention_masks, boxes, size = [], [], [], [], [], [] # detection[image, text]\n",
    "    \n",
    "    for i, sample in enumerate(batch):\n",
    "        if sample['task_id'] == 'handwritten':\n",
    "            encoded.append(sample['encoded'])\n",
    "            htr_labels.append(sample['labels'])\n",
    "            htr_images.append(sample['image'])\n",
    "            gt_texts.append(sample['gt_text'])\n",
    "        elif sample['task_id'] == 'trans':\n",
    "            code_input_ids.append(sample['input_ids'])\n",
    "            code_input_labels.append(sample['input_labels'])\n",
    "            code_targets.append(sample['target'])\n",
    "        elif sample['task_id'] == 'detection':\n",
    "            detection_images.append(sample['image'])\n",
    "            detection_input_ids.append(sample['input_ids'])\n",
    "            detection_attention_masks.append(sample['attention_mask'])\n",
    "            boxes.append(sample['boxes'])\n",
    "            size.append(sample['size'])\n",
    "            detection_names.append(sample['image_name'])\n",
    "        elif sample['task_id'] == 'vqa':\n",
    "            vqa_images.append(sample['image'])\n",
    "            vqa_input_ids.append(sample['input_ids'])\n",
    "            labels.append(sample['labels'])\n",
    "            targets.append(sample['target'])\n",
    "        \n",
    "    if htr_images:\n",
    "        htr_images = pad_sequence(htr_images, batch_first=True)\n",
    "        encoded = torch.stack(encoded)\n",
    "        htr_labels = torch.stack(htr_labels)\n",
    "    if detection_images:\n",
    "        detection_images = torch.stack(detection_images)   \n",
    "    if vqa_images:\n",
    "        vqa_images = torch.stack(vqa_images) \n",
    "    if detection_attention_masks and torch.is_tensor(detection_attention_masks[0]):\n",
    "        detection_input_ids = pad_sequence(detection_input_ids, batch_first=True)\n",
    "        detection_attention_masks = torch.stack(detection_attention_masks)\n",
    "    elif detection_attention_masks:\n",
    "        detection_input_ids = [input_id.unsqueeze(0) for input_id in detection_input_ids[0]]\n",
    "        detection_attention_masks = [attention_mask.unsqueeze(0) for attention_mask in detection_attention_masks[0]]\n",
    "    if labels:\n",
    "        vqa_input_ids = pad_sequence(vqa_input_ids, batch_first=True)\n",
    "        labels = pad_sequence(labels, batch_first=True)    \n",
    "    if code_input_ids:\n",
    "        code_input_ids = pad_sequence(code_input_ids, batch_first=True)\n",
    "        code_input_labels = pad_sequence(code_input_labels, batch_first=True)\n",
    "    return (htr_images, encoded, htr_labels, gt_texts), (code_input_ids, code_input_labels, code_targets), (vqa_images, vqa_input_ids, labels, targets), (detection_names, detection_images, detection_input_ids, detection_attention_masks, boxes, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23256f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2FusionBrain(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 gpt_model, \n",
    "                 attention_config, \n",
    "                 handwritten_config, \n",
    "                 vqa_config, \n",
    "                 detection_config, \n",
    "                 **freeze_gpt_kwargs):\n",
    "        super().__init__()\n",
    "        self.gpt_model = gpt_model\n",
    "        self.embedding_size = self.gpt_model.config.n_embd\n",
    "        self.freeze_gpt(**freeze_gpt_kwargs)\n",
    "\n",
    "        # handwritten[image] input/output layers:\n",
    "        self.handwritten_config = handwritten_config\n",
    "        self.handwritten_input_layer = self._build_input_net(\n",
    "            input_dim=handwritten_config['patch_w']*handwritten_config['patch_h']*3,\n",
    "            in_layer_sizes=handwritten_config['in_layer_sizes'],\n",
    "            orth_gain=handwritten_config['orth_gain'],\n",
    "            dropout=handwritten_config['dropout'],\n",
    "        )\n",
    "        self.htr_tokens_embed = nn.Linear(self.embedding_size, self.gpt_model.config.vocab_size, bias=False)\n",
    "        print('=== HANDWRITTEN TASK ===')\n",
    "        self._calculate_trainable_params([\n",
    "            self.handwritten_input_layer,\n",
    "            self.gpt_model, \n",
    "        ], without_emb=True)\n",
    "        print('=== === === === ===')\n",
    "        #####\n",
    "\n",
    "        # code2code\n",
    "        self.beam_size=3\n",
    "        self.sos_id=self.gpt_model.config.bos_token_id\n",
    "        self.eos_id=self.gpt_model.config.eos_token_id\n",
    "        self.lm_head = nn.Linear(self.gpt_model.config.n_embd, self.gpt_model.config.vocab_size, bias=False)\n",
    "\n",
    "        print('=== C2C TASK ===')\n",
    "        self._calculate_trainable_params([self.gpt_model, self.lm_head])\n",
    "        print('=== === === === ===')\n",
    "        \n",
    "        ## zhOD[image, text] and VQA[image, text] layers:\n",
    "        self.attention_config = attention_config\n",
    "        self.cross_attention = nn.ModuleList([\n",
    "            CrossAttentionLayer(self.embedding_size, attention_config['num_heads'], attention_config['pf_dim'])\n",
    "            for _ in range(attention_config['num_attention_layers'])\n",
    "        ])\n",
    "        #####\n",
    "        \n",
    "        # detection[image, text] input/output layers:\n",
    "        self.detection_config = detection_config\n",
    "        self.detection_input_image_layer = self._build_input_net(\n",
    "            input_dim=detection_config['patch_w']*detection_config['patch_h']*3,\n",
    "            in_layer_sizes=detection_config['in_layer_sizes'],\n",
    "            orth_gain=detection_config['orth_gain'],\n",
    "            dropout=detection_config['dropout'],\n",
    "        )\n",
    "        self.detection_pool = nn.AdaptiveMaxPool2d((detection_config[\"num_queries\"], None))\n",
    "        self.bbox_embed = MLP(self.embedding_size, self.embedding_size, 5, detection_config['num_mlp_layers'])\n",
    "        print('=== DETECTION TASK ===')\n",
    "        self._calculate_trainable_params([\n",
    "            self.detection_input_image_layer,\n",
    "            self.gpt_model,\n",
    "            self.cross_attention,\n",
    "            self.bbox_embed\n",
    "        ], without_emb=True)\n",
    "        print('=== === === === ===')\n",
    "        #####\n",
    "        \n",
    "         # vqa[image, text] input/output layers:\n",
    "        self.vqa_config = vqa_config\n",
    "        self.vqa_input_image_layer = self._build_input_net(\n",
    "            input_dim=vqa_config['patch_w']*vqa_config['patch_h']*3,\n",
    "            in_layer_sizes=vqa_config['in_layer_sizes'],\n",
    "            orth_gain=vqa_config['orth_gain'],\n",
    "            dropout=vqa_config['dropout'],\n",
    "        )\n",
    "        self.tokens_embed = nn.Linear(self.embedding_size, self.gpt_model.config.vocab_size, bias=False)\n",
    "        print('=== VQA TASK ===')\n",
    "        self._calculate_trainable_params([\n",
    "            self.vqa_input_image_layer,\n",
    "            self.gpt_model, \n",
    "            self.cross_attention,\n",
    "            self.tokens_embed\n",
    "        ], without_emb=True)\n",
    "        print('=== === === === ===')\n",
    "        #####\n",
    "        \n",
    "        print('=== COMMON PARAMS ===')\n",
    "        self._calculate_common_params()\n",
    "        print('=== === === === ===')\n",
    "\n",
    "\n",
    "    def forward(self, task_id, **kwargs):\n",
    "        if task_id == 'handwritten':\n",
    "            return self.forward_handwritten(**kwargs)\n",
    "        elif task_id == 'trans':\n",
    "            return self.forward_trans(**kwargs)\n",
    "        elif task_id == 'vqa':\n",
    "            return self.forward_vqa(**kwargs)\n",
    "        elif task_id == 'detection':\n",
    "            return self.forward_detection(**kwargs)\n",
    "\n",
    "    def forward_trans(self, input_ids, input_labels=None, eval_bleu=False, past=None):\n",
    "        if not eval_bleu:\n",
    "            attn_mask = torch.tensor(input_labels.clone().detach() != 0, dtype=torch.uint8)\n",
    "            attn_mask = attn_mask.to(input_labels.device)\n",
    "            outputs = self.gpt_model(input_ids, attention_mask=attn_mask)\n",
    "            x = self.lm_head(outputs[0])\n",
    "            return x\n",
    "        else:\n",
    "            if past != None:\n",
    "                outputs = self.gpt_model(input_ids, past_key_values=past)\n",
    "                logits = self.lm_head(outputs[0])\n",
    "                return logits, outputs[1]\n",
    "            else:\n",
    "                outputs = self.gpt_model(input_ids)[1]\n",
    "                return outputs\n",
    "\n",
    "    def forward_handwritten(self, images, target, labels):\n",
    "        patchs = rearrange(images, 'b c (h p1) (w p2) -> b (w) (h) (p1 p2 c)',\n",
    "                      p1=self.handwritten_config['patch_h'], p2=self.handwritten_config['patch_w'])\n",
    "        patchs = patchs.squeeze(2)\n",
    "        patchs = self.handwritten_input_layer(patchs)\n",
    "        \n",
    "        tokens_emb = self.gpt_model.wte(target)\n",
    "        tokens_emb = self.gpt_model.drop(tokens_emb)\n",
    "        \n",
    "        input_emb = torch.cat((patchs, tokens_emb), dim=1)\n",
    "        attention_mask = torch.tensor(labels.clone().detach() != 0, dtype=torch.uint8)\n",
    "        attention_mask = attention_mask.to(labels.device)\n",
    "        # Fusion Brain\n",
    "        gpt_output = self.gpt_model(inputs_embeds=input_emb, attention_mask=attention_mask).last_hidden_state\n",
    "        #####\n",
    "        output_logits = self.htr_tokens_embed(gpt_output)\n",
    "\n",
    "        return output_logits\n",
    "\n",
    "    def forward_vqa(self, images, tokens, labels):\n",
    "        patchs = rearrange(images, 'b c (h p1) (w p2) -> b (w) (h) (p1 p2 c)',\n",
    "                      p1=self.vqa_config['patch_h'], p2=self.vqa_config['patch_w'])\n",
    "        patchs = patchs.squeeze(2)\n",
    "        patchs = self.vqa_input_image_layer(patchs)\n",
    "        attention_mask = torch.tensor(labels.clone().detach() != 0, dtype=torch.uint8)\n",
    "        attention_mask = attention_mask.to(labels.device)\n",
    "        # Fusion Brain\n",
    "        img_emb = self.gpt_model(inputs_embeds=patchs).last_hidden_state\n",
    "        tokens_emb = self.gpt_model(input_ids=tokens, attention_mask=attention_mask).last_hidden_state\n",
    "        #####\n",
    "        for layer in self.cross_attention:\n",
    "            tokens_emb, _ = layer(tokens_emb, img_emb)\n",
    "            \n",
    "        output_logits = self.tokens_embed(tokens_emb)\n",
    "        \n",
    "        return output_logits\n",
    "    \n",
    "    def forward_detection(self, images, tokens, attention_masks):\n",
    "        patchs = rearrange(images, 'b c (h p1) (w p2) -> b (w) (h) (p1 p2 c)',\n",
    "                      p1=self.detection_config['patch_h'], p2=self.detection_config['patch_w'])\n",
    "        patchs = patchs.squeeze(2)\n",
    "        patchs = self.detection_input_image_layer(patchs)\n",
    "        # Fusion Brain\n",
    "        img_embs = self.gpt_model(inputs_embeds=patchs).last_hidden_state\n",
    "        tokens_embs = self.gpt_model(input_ids=tokens, attention_mask=attention_masks).last_hidden_state\n",
    "        #####\n",
    "        norm_img_embs = F.normalize(img_embs, p=2, dim=-1)\n",
    "        norm_tokens_embs = F.normalize(tokens_embs, p=2, dim=-1)\n",
    "        \n",
    "        text_masks = attention_masks.type(torch.bool)\n",
    "        for layer in self.cross_attention:\n",
    "            img_embs, _ = layer(img_embs, tokens_embs, ~text_masks)\n",
    "        img_embs = self.detection_pool(img_embs)\n",
    "        \n",
    "        output_logits = self.bbox_embed(img_embs).sigmoid()\n",
    "        out = {\n",
    "            'pred_logits': output_logits,\n",
    "            'proj_queries': norm_img_embs,\n",
    "            'proj_tokens':norm_tokens_embs,\n",
    "        }\n",
    "\n",
    "        return out\n",
    "\n",
    "    def freeze_gpt(self, freeze_pos=True, freeze_ln=True, freeze_attn=True, freeze_ff=True, freeze_other=True):\n",
    "        for name, p in self.gpt_model.named_parameters():\n",
    "            name = name.lower()\n",
    "            if 'ln' in name or 'norm' in name:\n",
    "                p.requires_grad = not freeze_ln\n",
    "            elif 'wpe' in name or 'position_embeddings' in name or 'pos_drop' in name:\n",
    "                p.requires_grad = not freeze_pos\n",
    "            elif 'mlp' in name:\n",
    "                p.requires_grad = not freeze_ff\n",
    "            elif 'attn' in name:\n",
    "                p.requires_grad = not freeze_attn\n",
    "            else:\n",
    "                p.requires_grad = not freeze_other\n",
    "\n",
    "    def _build_input_net(self, input_dim, in_layer_sizes=None, orth_gain=1.41, dropout=0.1):\n",
    "        \"\"\" вспомогательный метод для сборки input слоя, который приводит размер входящих данный к эмбеддингу gpt \"\"\"\n",
    "        in_layer_sizes = [] if not in_layer_sizes else in_layer_sizes\n",
    "        in_layers = []\n",
    "        last_output_size = input_dim\n",
    "        for size in in_layer_sizes:\n",
    "            layer = nn.Linear(last_output_size, size)\n",
    "            if orth_gain is not None:\n",
    "                torch.nn.init.orthogonal_(layer.weight, gain=orth_gain)\n",
    "            layer.bias.data.zero_()\n",
    "\n",
    "            in_layers.append(layer)\n",
    "            in_layers.append(nn.ReLU())\n",
    "            in_layers.append(nn.Dropout(dropout))\n",
    "            last_output_size = size\n",
    "\n",
    "        final_linear = nn.Linear(last_output_size, self.embedding_size)\n",
    "        if orth_gain is not None:\n",
    "            torch.nn.init.orthogonal_(final_linear.weight, gain=orth_gain)\n",
    "        final_linear.bias.data.zero_()\n",
    "\n",
    "        in_layers.append(final_linear)\n",
    "        in_layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        return nn.Sequential(*in_layers)\n",
    "\n",
    "    def _calculate_trainable_params(self, layers, without_emb=False):\n",
    "        trainable_params, all_used_params = 0, 0\n",
    "        for layer in layers:\n",
    "            trainable_params += sum(p.numel() for p in layer.parameters() if p.requires_grad)\n",
    "            all_used_params += sum(p.numel() for p in layer.parameters())        \n",
    "        print('trainable_params:', trainable_params)\n",
    "        print(' all_used_params:', all_used_params)\n",
    "        print('               %:', round(trainable_params/all_used_params*100, 2))\n",
    "\n",
    "    def _calculate_common_params(self):\n",
    "        all_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        common_params = sum(p.numel() for p in list(self.gpt_model.parameters()))\n",
    "        print('common_params:', common_params)\n",
    "        print('   all_params:', all_params)\n",
    "        print('            %:', round(common_params/all_params*100, 2))     \n",
    "        print('trainable_params:', trainable_params)\n",
    "        print('               %:', round(trainable_params/all_params*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30dc04e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "handwritten_config = {\n",
    "    'patch_w': 8,\n",
    "    'patch_h': 128,\n",
    "    'in_layer_sizes': [8*128*3],\n",
    "    'orth_gain': 1.41,\n",
    "    'dropout': 0.1,\n",
    "}\n",
    "\n",
    "attention_config = {\n",
    "    'num_attention_layers': 3,\n",
    "    'num_heads': 8,\n",
    "    'pf_dim': 2048,\n",
    "}\n",
    "\n",
    "vqa_config = {\n",
    "    'patch_w': 4,\n",
    "    'patch_h': 224,\n",
    "    'in_layer_sizes': [4*224*3],\n",
    "    'out_layer_sizes': [64],\n",
    "    'orth_gain': 1.41,\n",
    "    'dropout': 0.1,\n",
    "    'tokens_num': len(gpt_tokenizer),\n",
    "}\n",
    "\n",
    "detection_config = {\n",
    "    'patch_w': 4,\n",
    "    'patch_h': 224,\n",
    "    'in_layer_sizes': [4*224*3],\n",
    "    'out_layer_sizes': [64],\n",
    "    'orth_gain': 1.41,\n",
    "    'dropout': 0.1,\n",
    "    'num_mlp_layers': 3,\n",
    "    'num_queries': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2f43485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HANDWRITTEN TASK ===\n",
      "trainable_params: 11800320\n",
      " all_used_params: 137027328\n",
      "               %: 8.61\n",
      "=== === === === ===\n",
      "=== C2C TASK ===\n",
      "trainable_params: 38598144\n",
      " all_used_params: 163825152\n",
      "               %: 23.56\n",
      "=== === === === ===\n",
      "=== DETECTION TASK ===\n",
      "trainable_params: 27020165\n",
      " all_used_params: 152247173\n",
      "               %: 17.75\n",
      "=== === === === ===\n",
      "=== VQA TASK ===\n",
      "trainable_params: 64433280\n",
      " all_used_params: 189660288\n",
      "               %: 33.97\n",
      "=== === === === ===\n",
      "=== COMMON PARAMS ===\n",
      "common_params: 125227008\n",
      "   all_params: 289135109\n",
      "            %: 43.31\n",
      "trainable_params: 163908101\n",
      "               %: 56.69\n",
      "=== === === === ===\n"
     ]
    }
   ],
   "source": [
    "model = GPT2FusionBrain(\n",
    "    gpt_model,\n",
    "    attention_config=attention_config,\n",
    "    handwritten_config=handwritten_config,\n",
    "    vqa_config=vqa_config,\n",
    "    detection_config=detection_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6136d54a",
   "metadata": {},
   "source": [
    "# Lehas Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a0ae62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Скрипты для обучения и оценки качества модели\n",
    "\n",
    "\n",
    "class FusionBrainExperiment(TorchGPUExperiment):\n",
    "\n",
    "    handwritten_criterion = nn.CrossEntropyLoss()\n",
    "    detection_criterion = DetectionCriterion(['boxes', 'classification', 'contrastive'], 0.07)\n",
    "    detection_loss_weights = [1.0, 1.0, 1.0, 1.0]\n",
    "    vqa_criterion = nn.CrossEntropyLoss()\n",
    "    c2c_criterion = nn.CrossEntropyLoss()\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<s>',\n",
    "            eos_token='</s>', pad_token='<pad>', unk_token='<|UNKNOWN|>', sep_token='<|SEP|>')\n",
    "    \n",
    "#     def custom_action_before_train_one_epoch(self, test_loader):\n",
    "#         run_evaluation(test_loader, self.model, tokenizer=self.tokenizer)\n",
    "        \n",
    "#     def _custom_action_before_train_one_epoch(self):\n",
    "#         self._wipe_memory()\n",
    "#         self.custom_action_before_train_one_epoch(test_loader)\n",
    "\n",
    "    def calculate_handwritten_metrics(self, gt_texts, outputs):\n",
    "        pred_texts = []\n",
    "        for encoded in outputs.argmax(2).data.cpu().numpy():\n",
    "            pred_texts.append(self.ctc_labeling.decode(encoded))\n",
    "        texts = [self.ctc_labeling.preprocess(text) for text in gt_texts]\n",
    "        return {\n",
    "            'h_cer': cer(pred_texts, texts),\n",
    "            'h_wer': wer(pred_texts, texts),\n",
    "            'h_acc': string_accuracy(pred_texts, texts),\n",
    "        }\n",
    "\n",
    "    def handle_one_batch(self, batch):\n",
    "        (htr_images, encoded, htr_labels, gt_texts), (code_input_ids, code_input_labels, code_targets), (vqa_images, vqa_input_ids, labels, targets), (detection_names, detection_images, detection_input_ids, detection_attention_masks, boxes, size) = batch\n",
    "        losses = []\n",
    "        metrics = {}\n",
    "\n",
    "        if len(htr_images) > 0:\n",
    "            bs = htr_images.shape[0]\n",
    "            images = htr_images.to(self.device, dtype=torch.float32)\n",
    "            encoded = encoded.to(self.device, dtype=torch.long)\n",
    "            htr_labels = htr_labels.to(self.device, dtype=torch.long)\n",
    "            image_labels = torch.ones(bs, 64).to(self.device, dtype=torch.long)\n",
    "            all_labels = torch.cat((image_labels, htr_labels), dim=-1)\n",
    "            loss_mask = torch.tensor(htr_labels == 2, dtype=torch.uint8)\n",
    "            loss_mask = loss_mask.to(self.device)\n",
    "            logits = self.model('handwritten', images=images, target=encoded, labels=all_labels)\n",
    "            shift_logits = logits[..., 64:-1, :].contiguous()\n",
    "            shift_labels = encoded[..., 1:].contiguous()\n",
    "            \n",
    "            flatten_shift_loss_mask = loss_mask[..., :-1].contiguous().view(-1)\n",
    "            ids = torch.nonzero(flatten_shift_loss_mask).view(-1)\n",
    "            handwritten_loss = self.handwritten_criterion(shift_logits.view(-1, shift_logits.size(-1))[ids], shift_labels.view(-1)[ids])\n",
    "            metrics['h_loss'] = handwritten_loss.detach().cpu().item()\n",
    "            losses.append(handwritten_loss)\n",
    "\n",
    "        if len(code_input_ids) > 0:\n",
    "            bs = code_input_ids.shape[0]\n",
    "            code_input_ids = code_input_ids.to(self.device, dtype=torch.long) \n",
    "            code_input_labels = code_input_labels.to(self.device, dtype=torch.long) \n",
    "            loss_mask = torch.tensor(code_input_labels.clone().detach() == 2, dtype=torch.uint8)\n",
    "            loss_mask = loss_mask.to(self.device)\n",
    "            lm_logits = self.model('trans', input_ids=code_input_ids, input_labels=code_input_labels)\n",
    "            c_labels = code_input_ids\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = c_labels[..., 1:].contiguous()\n",
    "            \n",
    "            flatten_shift_loss_mask = loss_mask[..., :-1].contiguous().view(-1)\n",
    "            ids = torch.nonzero(flatten_shift_loss_mask).view(-1)\n",
    "            c2c_loss = self.c2c_criterion(shift_logits.view(-1, shift_logits.size(-1))[ids], shift_labels.view(-1)[ids])\n",
    "            metrics['c2c_loss'] = c2c_loss.detach().cpu().item()\n",
    "            losses.append(c2c_loss)\n",
    "            \n",
    "        if len(labels) > 0:\n",
    "            images = vqa_images.to(self.device, dtype=torch.float32)\n",
    "            input_ids = vqa_input_ids.to(self.device, dtype=torch.long)\n",
    "            labels = labels.to(self.device, dtype=torch.float32)\n",
    "            loss_mask = torch.tensor(labels.clone().detach() == 2, dtype=torch.uint8)\n",
    "            loss_mask = loss_mask.to(self.device)\n",
    "            lm_logits = self.model('vqa', images=images, tokens=input_ids, labels=labels)\n",
    "            labels = input_ids\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "            flatten_shift_loss_mask = loss_mask[..., :-1].contiguous().view(-1)\n",
    "            ids = torch.nonzero(flatten_shift_loss_mask).view(-1)\n",
    "            vqa_loss = self.vqa_criterion(shift_logits.view(-1, shift_logits.size(-1))[ids], shift_labels.view(-1)[ids])\n",
    "            metrics['vqa_loss'] = vqa_loss.detach().cpu().item()\n",
    "            losses.append(vqa_loss)\n",
    "            \n",
    "        if len(boxes) > 0:\n",
    "            images = detection_images.to(self.device, dtype=torch.float32)\n",
    "            input_ids = detection_input_ids.to(self.device, dtype=torch.long) \n",
    "            attention_masks = detection_attention_masks.to(self.device, dtype=torch.long) \n",
    "            boxes = [boxes_per_label.to(self.device, dtype=torch.float) for boxes_per_label in boxes]\n",
    "            detection_outputs = self.model('detection', images=images, tokens=input_ids, attention_masks=attention_masks)\n",
    "            detection_loss = self.detection_criterion(detection_outputs, boxes)\n",
    "            #detection_loss = sum([\n",
    "            #    loss * weight for loss, weight in zip(detection_loss.values(), self.detection_loss_weights)\n",
    "            #])\n",
    "            #metrics['detection_acc'] = acc(targets.argmax(axis=1), sentiment_outputs)\n",
    "            metrics['loss_giou'] = detection_loss['loss_giou'].detach().cpu().item()\n",
    "            metrics['loss_bbox'] = detection_loss['loss_bbox'].detach().cpu().item()\n",
    "            metrics['loss_contrastive'] = detection_loss['loss_contrastive'].detach().cpu().item()\n",
    "            metrics['loss_classification'] = detection_loss['loss_classification'].detach().cpu().item()\n",
    "            losses.append(detection_loss['loss_giou'])\n",
    "            losses.append(detection_loss['loss_bbox'])\n",
    "            losses.append(detection_loss['loss_contrastive'])\n",
    "            losses.append(detection_loss['loss_classification'])\n",
    "\n",
    "        #loss = sum(losses)\n",
    "        loss = sum([loss * weight for loss, weight in zip(losses, self.detection_loss_weights)])\n",
    "        \n",
    "        #print(loss)\n",
    "        self.metrics.update(loss=loss.detach().cpu().item(), **metrics)\n",
    "        \n",
    "        #self.metrics.update(loss=loss.item(), **metrics)\n",
    "\n",
    "        if self.is_train:\n",
    "            loss.backward()\n",
    "            self.optimizer_step()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scheduler.step()\n",
    "\n",
    "\n",
    "def run_evaluation(loader, model, tokenizer=None, device=torch.device('cuda:0')):\n",
    "    result = []\n",
    "    true_json_detection = {}\n",
    "    pred_json_detection = {}\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            (htr_images, encoded, encoded_length, gt_texts), (code_input_ids, code_input_labels, code_targets), (vqa_images, vqa_input_ids, labels, targets), (detection_names, detection_images, detection_input_ids, detection_attention_masks, boxes, size) = batch\n",
    "            if len(htr_images) > 0:\n",
    "                images = htr_images.to(device, dtype=torch.float32)\n",
    "                handwritten_outputs = model('handwritten', images=images)\n",
    "                for encoded, gt_text in zip(handwritten_outputs.argmax(2).data.cpu().numpy(), gt_texts):\n",
    "                    pred_text = ctc_labeling.decode(encoded)\n",
    "                    result.append({\n",
    "                        'task_id': 'handwritten',\n",
    "                        'gt_output': gt_text,\n",
    "                        'pred_output': pred_text,\n",
    "                    })\n",
    "\n",
    "            if len(code_input_ids) > 0:\n",
    "                code_input_ids = code_input_ids.to(device, dtype=torch.long)\n",
    "                code_input_labels = code_input_labels.to(device, dtype=torch.long)\n",
    "                loss_mask = torch.tensor(code_input_labels.clone().detach() == 2, dtype=torch.uint8)\n",
    "                loss_mask = loss_mask.to(device)\n",
    "                hidden_states = model('trans', input_ids=code_input_ids, input_labels=code_input_labels, eval_bleu=True)\n",
    "                bleu_score, _ = eval_bleu(model, hidden_states, input_ids=code_input_ids, beam_size=5, tokenizer=tokenizer, targets=code_targets)\n",
    "                result.append({\n",
    "                        'task_id': 'trans',\n",
    "                        'true_text': code_targets,\n",
    "                        'bleu_score': bleu_score,\n",
    "                })\n",
    "                \n",
    "            if len(labels) > 0:\n",
    "                images = vqa_images.to(device, dtype=torch.float32)\n",
    "                input_ids = vqa_input_ids.to(device, dtype=torch.long)\n",
    "                labels = labels.to(device, dtype=torch.float) \n",
    "                attention_mask = torch.tensor(labels.clone().detach() != 0, dtype=torch.uint8)\n",
    "                attention_mask = attention_mask.to(labels.device)\n",
    "                vqa_outputs = vqa_evaluation(model, images, input_ids, attention_mask, 10)\n",
    "                for target, pred_labels in zip(targets, vqa_outputs.argmax(-1).cpu().numpy()):\n",
    "                    result.append({\n",
    "                        'task_id': 'vqa',\n",
    "                        'gt_output': target,\n",
    "                        'pred_output': gpt_tokenizer.decode(pred_labels).split(gpt_tokenizer.eos_token)[0],\n",
    "                    })\n",
    "                    \n",
    "            if len(boxes) > 0:\n",
    "                images = detection_images.to(device, dtype=torch.float32)\n",
    "                input_ids = [input_id.to(device, dtype=torch.long) for input_id in detection_input_ids]\n",
    "                attention_masks = [attention_mask.to(device, dtype=torch.long) for attention_mask in detection_attention_masks]\n",
    "                detection_outputs = detection_evaluation(model, images, input_ids, attention_masks, 0.12, 0.5)\n",
    "                img_h, img_w = size[0]\n",
    "                for i in range(len(detection_outputs)):\n",
    "                    if detection_outputs[i].numel() != 0:\n",
    "                        detection_outputs[i][:, 0] = detection_outputs[i][:, 0] * img_w\n",
    "                        detection_outputs[i][:, 2] = detection_outputs[i][:, 2] * img_w\n",
    "                        detection_outputs[i][:, 1] = detection_outputs[i][:, 1] * img_h\n",
    "                        detection_outputs[i][:, 3] = detection_outputs[i][:, 3] * img_h\n",
    "                image_name = detection_names[0]\n",
    "                for boxes_for_img in boxes:\n",
    "                    true_json_detection[image_name] = boxes_for_img\n",
    "                    pred_json_detection[image_name] = {\n",
    "                        input_text: output.type(torch.int32).cpu().tolist()\n",
    "                        for input_text, output in zip(boxes_for_img.keys(), detection_outputs)\n",
    "                    }\n",
    "                result.append({\n",
    "                        'task_id': 'detection',\n",
    "                    })\n",
    "                \n",
    "    result = pd.DataFrame(result)\n",
    "\n",
    "    handwritten_result = result[result['task_id'] == 'handwritten']\n",
    "    if handwritten_result.shape[0]:\n",
    "        print('= Handwritten =')\n",
    "        print('CER:', round(cer(handwritten_result['pred_output'], handwritten_result['gt_output']), 3))\n",
    "        print('WER:', round(wer(handwritten_result['pred_output'], handwritten_result['gt_output']), 3))\n",
    "        print('ACC:', round(string_accuracy(handwritten_result['pred_output'], handwritten_result['gt_output']), 3))\n",
    "        print('=== === === ===')\n",
    "        \n",
    "    trans_result = result[result['task_id'] == 'trans']   \n",
    "    if trans_result.shape[0]:\n",
    "        print('== C2C ==')\n",
    "        print('meanBLEU:', np.mean(trans_result['bleu_score']))\n",
    "        print('=== === === ===')\n",
    "        \n",
    "    vqa_result = result[result['task_id'] == 'vqa']\n",
    "    if vqa_result.shape[0]:\n",
    "        print('== VQA ==')\n",
    "        print('ACC:', round(vqa_evaluate(vqa_result), 3))\n",
    "        print('=== === === ===')\n",
    "        \n",
    "    \n",
    "    if len(true_json_detection):\n",
    "        print('== Detection ==')\n",
    "        #print(true_json_detection)\n",
    "        #print(\"PRED\")\n",
    "        #print(pred_json_detection)\n",
    "        print('ACC:', round(detection_evaluate(true_json_detection, pred_json_detection), 3))\n",
    "        print('=== === === ===')\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_model(model, experiment_name):\n",
    "    paths = sorted(glob(f'./saved_models/{experiment_name}*'))\n",
    "    if len(paths) == 0:\n",
    "        print('Warning! Model not found')\n",
    "        return model\n",
    "    checkpoint_path = paths[-1] + '/last.pt'\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "#     metrics = defaultdict(list)\n",
    "#     for epoch in range(len(checkpoint['metrics_state_dict']['train_metrics'])):\n",
    "#         train_metrics = checkpoint['metrics_state_dict']['train_metrics'][epoch]['avg']\n",
    "#         valid_metrics = checkpoint['metrics_state_dict']['valid_metrics'][epoch]['avg']\n",
    "#         for key in train_metrics.keys():\n",
    "#             metrics[f'train_{key}'].append(train_metrics[key])\n",
    "#             metrics[f'valid_{key}'].append(valid_metrics[key])\n",
    "#         metrics['epoch'].append(epoch)        \n",
    "#     metrics = pd.DataFrame(metrics)\n",
    "    return model\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a5fbca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:29<00:00, 11.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Detection ==\n",
      "ACC: 0.122\n",
      "=== === === ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#@title c2c evaluation\n",
    "vqa_valid = df[(df['task_id'] == 'detection') & (df['stage'] == 'test')][:1000]\n",
    "\n",
    "vqa_eval_dataset = DatasetRetriever(\n",
    "    task_ids=vqa_valid['task_id'].values,\n",
    "    input_images=vqa_valid['input_image'].values,\n",
    "    input_texts=vqa_valid['input_text'].values,\n",
    "    output_texts=vqa_valid['output_text'].values,\n",
    "    output_boxes=vqa_valid['output_boxes'].values,\n",
    "    stage='test',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "#model = load_model(model, 'fusion-brain-vqa')\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('MainWeights/fusion.pt')['model_state_dict'])\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    vqa_eval_dataset,\n",
    "    batch_size=1,\n",
    "    sampler=SequentialSampler(vqa_eval_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "evaluation_result = run_evaluation(valid_loader, model, tokenizer=gpt_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c0341",
   "metadata": {},
   "source": [
    "## Code To Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0e0d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-10-21T09:02:08.207967\n",
      "lr: 3.9999999999999996e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/ipykernel_launcher.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/user/conda/lib/python3.7/site-packages/ipykernel_launcher.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2041 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2041 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0, time: 1301.8s, loss=1.95123, c2c_loss=1.95123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 0, time: 116.4s, loss=1.02776, c2c_loss=1.02776\n",
      "\n",
      "2021-10-21T09:25:53.931896\n",
      "lr: 0.00037170022953611124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3870 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 1, time: 1301.6s, loss=1.01975, c2c_loss=1.01975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 1, time: 116.5s, loss=0.91148, c2c_loss=0.91148\n",
      "\n",
      "2021-10-21T09:49:39.614044\n",
      "lr: 0.0009083632420014913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 2, time: 1301.8s, loss=0.92662, c2c_loss=0.92662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 2, time: 116.4s, loss=0.84657, c2c_loss=0.84657\n",
      "\n",
      "2021-10-21T10:13:25.565278\n",
      "lr: 0.000998781427515107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2041 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 3, time: 1301.8s, loss=0.85747, c2c_loss=0.85747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 3, time: 116.3s, loss=0.79679, c2c_loss=0.79679\n",
      "\n",
      "2021-10-21T10:37:11.607805\n",
      "lr: 0.0009890720484871023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2041 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4192 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 4, time: 1301.4s, loss=0.81109, c2c_loss=0.81109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 4, time: 116.2s, loss=0.78488, c2c_loss=0.78488\n",
      "\n",
      "2021-10-21T11:00:57.402542\n",
      "lr: 0.0009698434772742306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3870 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2041 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 5, time: 1301.0s, loss=0.77845, c2c_loss=0.77845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 5, time: 116.4s, loss=0.74551, c2c_loss=0.74551\n",
      "\n",
      "2021-10-21T11:24:42.745747\n",
      "lr: 0.0009414699761429753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2041 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 6, time: 1300.5s, loss=0.75354, c2c_loss=0.75354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 6, time: 116.5s, loss=0.73339, c2c_loss=0.73339\n",
      "\n",
      "2021-10-21T11:48:27.851531\n",
      "lr: 0.000904503803018477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4192 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 7, time: 1300.8s, loss=0.72978, c2c_loss=0.72978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 7, time: 116.4s, loss=0.71579, c2c_loss=0.71579\n",
      "\n",
      "2021-10-21T12:12:13.560431\n",
      "lr: 0.0008596644624122053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2041 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 8, time: 1301.6s, loss=0.70968, c2c_loss=0.70968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 8, time: 116.4s, loss=0.70949, c2c_loss=0.70949\n",
      "\n",
      "2021-10-21T12:35:59.546089\n",
      "lr: 0.0008078247010850673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2041 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 9, time: 1301.4s, loss=0.69156, c2c_loss=0.69156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 9, time: 116.2s, loss=0.68824, c2c_loss=0.68824\n",
      "\n",
      "2021-10-21T12:59:45.076979\n",
      "lr: 0.0007499935210244414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 10, time: 1300.8s, loss=0.67406, c2c_loss=0.67406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 10, time: 116.4s, loss=0.67466, c2c_loss=0.67466\n",
      "\n",
      "2021-10-21T13:23:30.187945\n",
      "lr: 0.0006872965403682151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2041 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 11, time: 1300.9s, loss=0.65681, c2c_loss=0.65681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 11, time: 116.6s, loss=0.66940, c2c_loss=0.66940\n",
      "\n",
      "2021-10-21T13:47:15.606882\n",
      "lr: 0.0006209540845281034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4192 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 12, time: 1301.1s, loss=0.64181, c2c_loss=0.64181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 12, time: 116.3s, loss=0.65397, c2c_loss=0.65397\n",
      "\n",
      "2021-10-21T14:11:01.718004\n",
      "lr: 0.0005522574339436092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3870 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 13, time: 1300.9s, loss=0.62582, c2c_loss=0.62582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 13, time: 116.6s, loss=0.64749, c2c_loss=0.64749\n",
      "\n",
      "2021-10-21T14:34:47.518252\n",
      "lr: 0.000482543690777078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2041 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3870 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 14, time: 1303.6s, loss=0.61034, c2c_loss=0.61034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 14, time: 116.6s, loss=0.63532, c2c_loss=0.63532\n",
      "\n",
      "2021-10-21T14:58:36.583684\n",
      "lr: 0.0004131697537410383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4192 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 15, time: 1302.1s, loss=0.59542, c2c_loss=0.59542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 15, time: 116.5s, loss=0.62522, c2c_loss=0.62522\n",
      "\n",
      "2021-10-21T15:22:23.530502\n",
      "lr: 0.0003454859076082133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4192 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 16, time: 1301.2s, loss=0.58135, c2c_loss=0.58135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 16, time: 116.3s, loss=0.62069, c2c_loss=0.62069\n",
      "\n",
      "2021-10-21T15:46:09.427350\n",
      "lr: 0.00028080954145434406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4192 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2041 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 17, time: 1302.2s, loss=0.56886, c2c_loss=0.56886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 17, time: 116.5s, loss=0.61373, c2c_loss=0.61373\n",
      "\n",
      "2021-10-21T16:09:57.009753\n",
      "lr: 0.00022039950717834018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3870 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4192 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 18, time: 1302.0s, loss=0.55819, c2c_loss=0.55819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 18, time: 116.4s, loss=0.60874, c2c_loss=0.60874\n",
      "\n",
      "2021-10-21T16:33:43.651335\n",
      "lr: 0.00016543161738198804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4192 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 19, time: 1303.9s, loss=0.54771, c2c_loss=0.54771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 19, time: 116.5s, loss=0.60225, c2c_loss=0.60225\n",
      "\n",
      "2021-10-21T16:57:32.327720\n",
      "lr: 0.00011697575951512611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 20, time: 1303.1s, loss=0.53889, c2c_loss=0.53889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 20, time: 116.6s, loss=0.60062, c2c_loss=0.60062\n",
      "\n",
      "2021-10-21T17:21:20.055666\n",
      "lr: 7.597507173341122e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4192 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2041 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 21, time: 1302.8s, loss=0.53196, c2c_loss=0.53196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 21, time: 116.5s, loss=0.59888, c2c_loss=0.59888\n",
      "\n",
      "2021-10-21T17:45:07.609198\n",
      "lr: 4.322758578692332e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2587 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 22, time: 1302.7s, loss=0.52782, c2c_loss=0.52782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 22, time: 116.5s, loss=0.59730, c2c_loss=0.59730\n",
      "\n",
      "2021-10-21T18:08:54.240406\n",
      "lr: 1.9370694239897702e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 23, time: 1301.9s, loss=0.52427, c2c_loss=0.52427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid epoch 23, time: 116.5s, loss=0.59730, c2c_loss=0.59730\n",
      "\n",
      "2021-10-21T18:32:40.929178\n",
      "lr: 4.868744349485383e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1303 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Запуск обучения c2c[code]\n",
    "\n",
    "DEMO_LOGS = False \n",
    "\n",
    "trans_train = df[(df['task_id'] == 'trans') & (df['stage'] == 'train')]\n",
    "trans_valid = df[(df['task_id'] == 'trans') & (df['stage'] == 'valid')]\n",
    "trans_test = df[(df['task_id'] == 'trans') & (df['stage'] == 'test')]\n",
    "\n",
    "\n",
    "trans_train_dataset = DatasetRetriever(\n",
    "    task_ids=trans_train['task_id'].values,\n",
    "    input_images=trans_train['input_image'].values,\n",
    "    input_texts=trans_train['input_text'].values,\n",
    "    output_texts=trans_train['output_text'].values,\n",
    "    output_boxes=trans_train['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='train',\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "trans_valid_dataset = DatasetRetriever(\n",
    "    task_ids=trans_valid['task_id'].values,\n",
    "    input_images=trans_valid['input_image'].values,\n",
    "    input_texts=trans_valid['input_text'].values,\n",
    "    output_texts=trans_valid['output_text'].values,\n",
    "    output_boxes=trans_valid['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='valid',\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "trans_test_dataset = DatasetRetriever(\n",
    "    task_ids=trans_test['task_id'].values,\n",
    "    input_images=trans_test['input_image'].values,\n",
    "    input_texts=trans_test['input_text'].values,\n",
    "    output_texts=trans_test['output_text'].values,\n",
    "    output_boxes=trans_test['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='test',\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "\n",
    "#model.freeze_gpt()\n",
    "model.freeze_gpt(freeze_pos=False, freeze_ln=False, freeze_attn=True, freeze_ff=True, freeze_other=False)\n",
    "\n",
    "CONFIG = {\n",
    "    'description': 'Обучение c2c task',\n",
    "    'experiment_name': f'fusion-brain-c2c-{round(datetime.utcnow().timestamp())}',\n",
    "    'lr': 0.000005,\n",
    "    'bs': 4,\n",
    "    'num_epochs': 25,\n",
    "    'max_lr': 0.001,\n",
    "    'pct_start': 0.1,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trans_train_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=RandomSampler(trans_train_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    trans_valid_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=SequentialSampler(trans_valid_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    trans_test_dataset,\n",
    "    batch_size=1,\n",
    "    sampler=SequentialSampler(trans_test_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['max_lr'],\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    pct_start=CONFIG['pct_start'],\n",
    "    epochs=CONFIG['num_epochs'],\n",
    ")\n",
    "\n",
    "if DEMO_LOGS:\n",
    "    print(open('./saved_models/fusion-brain-detection-1631794561/log.txt').read())\n",
    "else:\n",
    "    experiment = FusionBrainExperiment(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=None,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        base_dir=f'./saved_models',\n",
    "        experiment_name=CONFIG['experiment_name'],\n",
    "        verbose_step=10**5,\n",
    "        seed=42,\n",
    "        best_saving=False,\n",
    "        last_saving=True,\n",
    "        #test_loader=test_loader #before each epoch BLEU is calculated (custom_action_before_train_one_epoch in Experiment)\n",
    "    )\n",
    "    experiment.fit(train_loader, valid_loader, CONFIG['num_epochs'])\n",
    "    experiment.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd817ae7",
   "metadata": {},
   "source": [
    "## Handwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22794a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-11-02T00:35:48.402965\n",
      "lr: 3.9999999999999996e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0, time: 277.5s, loss=4.59753, h_loss=4.59753\n",
      "Valid epoch 0, time: 15.8s, loss=3.82527, h_loss=3.82527\n",
      "\n",
      "2021-11-02T00:40:53.517105\n",
      "lr: 0.0001317097328982558\n",
      "Train epoch 1, time: 274.7s, loss=3.41587, h_loss=3.41587\n",
      "Valid epoch 1, time: 16.0s, loss=3.22051, h_loss=3.22051\n",
      "\n",
      "2021-11-02T00:45:56.925938\n",
      "lr: 0.000371794451975234\n",
      "Train epoch 2, time: 274.9s, loss=3.01585, h_loss=3.01585\n",
      "Valid epoch 2, time: 15.8s, loss=3.09134, h_loss=3.09134\n",
      "\n",
      "2021-11-02T00:51:01.309028\n",
      "lr: 0.0006685120511496742\n",
      "Train epoch 3, time: 276.3s, loss=2.85973, h_loss=2.85973\n",
      "Valid epoch 3, time: 15.8s, loss=3.04393, h_loss=3.04393\n",
      "\n",
      "2021-11-02T00:56:06.947527\n",
      "lr: 0.0009084796480611615\n",
      "Train epoch 4, time: 275.3s, loss=2.74945, h_loss=2.74945\n",
      "Valid epoch 4, time: 17.7s, loss=2.98452, h_loss=2.98452\n",
      "\n",
      "2021-11-02T01:01:15.037704\n",
      "lr: 0.0009999999986092108\n",
      "Train epoch 5, time: 275.0s, loss=2.62503, h_loss=2.62503\n",
      "Valid epoch 5, time: 16.0s, loss=2.94470, h_loss=2.94470\n",
      "\n",
      "2021-11-02T01:06:19.167286\n",
      "lr: 0.000998779427171525\n",
      "Train epoch 6, time: 275.4s, loss=2.51336, h_loss=2.51336\n",
      "Valid epoch 6, time: 15.8s, loss=2.91889, h_loss=2.91889\n",
      "\n",
      "2021-11-02T01:11:23.416567\n",
      "lr: 0.0009951288622455912\n",
      "Train epoch 7, time: 274.9s, loss=2.42012, h_loss=2.42012\n",
      "Valid epoch 7, time: 15.8s, loss=2.90126, h_loss=2.90126\n",
      "\n",
      "2021-11-02T01:16:28.350969\n",
      "lr: 0.0009890660890167747\n",
      "Train epoch 8, time: 274.7s, loss=2.34000, h_loss=2.34000\n",
      "Valid epoch 8, time: 15.6s, loss=2.94757, h_loss=2.94757\n",
      "\n",
      "2021-11-02T01:21:27.366005\n",
      "lr: 0.0009806206447068192\n",
      "Train epoch 9, time: 274.7s, loss=2.26532, h_loss=2.26532\n",
      "Valid epoch 9, time: 16.2s, loss=2.94790, h_loss=2.94790\n",
      "\n",
      "2021-11-02T01:26:26.829299\n",
      "lr: 0.0009698336746714689\n",
      "Train epoch 10, time: 274.9s, loss=2.19969, h_loss=2.19969\n",
      "Valid epoch 10, time: 15.9s, loss=2.93884, h_loss=2.93884\n",
      "\n",
      "2021-11-02T01:31:26.130890\n",
      "lr: 0.0009567577319444342\n",
      "Train epoch 11, time: 274.9s, loss=2.14318, h_loss=2.14318\n",
      "Valid epoch 11, time: 15.8s, loss=2.91803, h_loss=2.91803\n",
      "\n",
      "2021-11-02T01:36:25.659836\n",
      "lr: 0.0009414565212042919\n",
      "Train epoch 12, time: 274.8s, loss=2.08671, h_loss=2.08671\n",
      "Valid epoch 12, time: 15.8s, loss=2.94326, h_loss=2.94326\n",
      "\n",
      "2021-11-02T01:41:24.903786\n",
      "lr: 0.0009240045884116955\n",
      "Train epoch 13, time: 274.7s, loss=2.03337, h_loss=2.03337\n",
      "Valid epoch 13, time: 16.3s, loss=2.92458, h_loss=2.92458\n",
      "\n",
      "2021-11-02T01:46:24.472770\n",
      "lr: 0.0009044869576289485\n",
      "Train epoch 14, time: 275.0s, loss=1.98030, h_loss=1.98030\n",
      "Valid epoch 14, time: 16.0s, loss=2.95418, h_loss=2.95418\n",
      "\n",
      "2021-11-02T01:51:24.132011\n",
      "lr: 0.000882998716791319\n",
      "Train epoch 15, time: 276.1s, loss=1.93684, h_loss=1.93684\n",
      "Valid epoch 15, time: 15.8s, loss=2.99642, h_loss=2.99642\n",
      "\n",
      "2021-11-02T01:56:24.733631\n",
      "lr: 0.0008596445544481776\n",
      "Train epoch 16, time: 275.0s, loss=1.88833, h_loss=1.88833\n",
      "Valid epoch 16, time: 17.5s, loss=3.01535, h_loss=3.01535\n",
      "\n",
      "2021-11-02T02:01:27.448640\n",
      "lr: 0.0008345382497309078\n",
      "Train epoch 17, time: 275.2s, loss=1.84637, h_loss=1.84637\n",
      "Valid epoch 17, time: 15.8s, loss=3.00537, h_loss=3.00537\n",
      "\n",
      "2021-11-02T02:06:26.550250\n",
      "lr: 0.0008078021180324153\n",
      "Train epoch 18, time: 274.8s, loss=1.80255, h_loss=1.80255\n",
      "Valid epoch 18, time: 16.0s, loss=3.03011, h_loss=3.03011\n",
      "\n",
      "2021-11-02T02:11:26.400211\n",
      "lr: 0.0007795664150988283\n",
      "Train epoch 19, time: 275.0s, loss=1.76467, h_loss=1.76467\n",
      "Valid epoch 19, time: 15.9s, loss=3.04833, h_loss=3.04833\n",
      "\n",
      "2021-11-02T02:16:26.040722\n",
      "lr: 0.0007499687024365969\n",
      "Train epoch 20, time: 274.9s, loss=1.72913, h_loss=1.72913\n",
      "Valid epoch 20, time: 16.0s, loss=3.04010, h_loss=3.04010\n",
      "\n",
      "2021-11-02T02:21:25.822359\n",
      "lr: 0.0007191531771266593\n",
      "Train epoch 21, time: 275.7s, loss=1.69257, h_loss=1.69257\n",
      "Valid epoch 21, time: 16.2s, loss=3.04660, h_loss=3.04660\n",
      "\n",
      "2021-11-02T02:26:26.320103\n",
      "lr: 0.0006872699693107599\n",
      "Train epoch 22, time: 275.3s, loss=1.65750, h_loss=1.65750\n",
      "Valid epoch 22, time: 15.9s, loss=3.09030, h_loss=3.09030\n",
      "\n",
      "2021-11-02T02:31:26.707518\n",
      "lr: 0.000654474410772489\n",
      "Train epoch 23, time: 275.1s, loss=1.61886, h_loss=1.61886\n",
      "Valid epoch 23, time: 16.1s, loss=3.10499, h_loss=3.10499\n",
      "\n",
      "2021-11-02T02:36:26.805252\n",
      "lr: 0.0006209262781764469\n",
      "Train epoch 24, time: 274.8s, loss=1.59107, h_loss=1.59107\n",
      "Valid epoch 24, time: 15.8s, loss=3.12620, h_loss=3.12620\n",
      "\n",
      "2021-11-02T02:41:25.863684\n",
      "lr: 0.0005867890146523952\n",
      "Train epoch 25, time: 274.8s, loss=1.55624, h_loss=1.55624\n",
      "Valid epoch 25, time: 15.9s, loss=3.11459, h_loss=3.11459\n",
      "\n",
      "2021-11-02T02:46:25.224739\n",
      "lr: 0.000552228933516757\n",
      "Train epoch 26, time: 274.8s, loss=1.52305, h_loss=1.52305\n",
      "Valid epoch 26, time: 15.8s, loss=3.11958, h_loss=3.11958\n",
      "\n",
      "2021-11-02T02:51:24.407118\n",
      "lr: 0.0005174144080108583\n",
      "Train epoch 27, time: 274.8s, loss=1.49566, h_loss=1.49566\n",
      "Valid epoch 27, time: 15.9s, loss=3.16868, h_loss=3.16868\n",
      "\n",
      "2021-11-02T02:56:23.870646\n",
      "lr: 0.00048251505100341984\n",
      "Train epoch 28, time: 275.0s, loss=1.46715, h_loss=1.46715\n",
      "Valid epoch 28, time: 17.2s, loss=3.16255, h_loss=3.16255\n",
      "\n",
      "2021-11-02T03:01:25.368161\n",
      "lr: 0.00044770088865371066\n",
      "Train epoch 29, time: 275.2s, loss=1.43853, h_loss=1.43853\n",
      "Valid epoch 29, time: 16.0s, loss=3.17522, h_loss=3.17522\n",
      "\n",
      "2021-11-02T03:06:25.327406\n",
      "lr: 0.0004131415320611915\n",
      "Train epoch 30, time: 275.0s, loss=1.40986, h_loss=1.40986\n",
      "Valid epoch 30, time: 15.9s, loss=3.19402, h_loss=3.19402\n",
      "\n",
      "2021-11-02T03:11:25.105380\n",
      "lr: 0.0003790053509372863\n",
      "Train epoch 31, time: 274.9s, loss=1.38374, h_loss=1.38374\n",
      "Valid epoch 31, time: 16.6s, loss=3.17734, h_loss=3.17734\n",
      "\n",
      "2021-11-02T03:16:25.400690\n",
      "lr: 0.00034545865332507404\n",
      "Train epoch 32, time: 274.7s, loss=1.36024, h_loss=1.36024\n",
      "Valid epoch 32, time: 15.8s, loss=3.18556, h_loss=3.18556\n",
      "\n",
      "2021-11-02T03:21:24.602381\n",
      "lr: 0.00031266487536321923\n",
      "Train epoch 33, time: 274.7s, loss=1.33755, h_loss=1.33755\n",
      "Valid epoch 33, time: 16.1s, loss=3.21905, h_loss=3.21905\n",
      "\n",
      "2021-11-02T03:26:23.878436\n",
      "lr: 0.00028078378504153286\n",
      "Train epoch 34, time: 275.0s, loss=1.31224, h_loss=1.31224\n",
      "Valid epoch 34, time: 16.2s, loss=3.22201, h_loss=3.22201\n",
      "\n",
      "2021-11-02T03:31:23.541371\n",
      "lr: 0.0002499707038273862\n",
      "Train epoch 35, time: 275.0s, loss=1.29153, h_loss=1.29153\n",
      "Valid epoch 35, time: 15.8s, loss=3.22464, h_loss=3.22464\n",
      "\n",
      "2021-11-02T03:36:22.926342\n",
      "lr: 0.0002203757499551346\n",
      "Train epoch 36, time: 275.4s, loss=1.27142, h_loss=1.27142\n",
      "Valid epoch 36, time: 16.0s, loss=3.22241, h_loss=3.22241\n",
      "\n",
      "2021-11-02T03:41:22.781256\n",
      "lr: 0.00019214310706516894\n",
      "Train epoch 37, time: 275.0s, loss=1.25563, h_loss=1.25563\n",
      "Valid epoch 37, time: 15.8s, loss=3.23570, h_loss=3.23570\n",
      "\n",
      "2021-11-02T03:46:22.272104\n",
      "lr: 0.0001654103217557142\n",
      "Train epoch 38, time: 274.8s, loss=1.24150, h_loss=1.24150\n",
      "Valid epoch 38, time: 15.8s, loss=3.24170, h_loss=3.24170\n",
      "\n",
      "2021-11-02T03:51:21.473927\n",
      "lr: 0.0001403076334696278\n",
      "Train epoch 39, time: 275.7s, loss=1.22112, h_loss=1.22112\n",
      "Valid epoch 39, time: 15.9s, loss=3.25513, h_loss=3.25513\n",
      "\n",
      "2021-11-02T03:56:21.414866\n",
      "lr: 0.00011695733998092613\n",
      "Train epoch 40, time: 274.9s, loss=1.20885, h_loss=1.20885\n",
      "Valid epoch 40, time: 16.7s, loss=3.25232, h_loss=3.25232\n",
      "\n",
      "2021-11-02T04:01:22.341294\n",
      "lr: 9.547320157232273e-05\n",
      "Train epoch 41, time: 275.0s, loss=1.19842, h_loss=1.19842\n",
      "Valid epoch 41, time: 15.9s, loss=3.27484, h_loss=3.27484\n",
      "\n",
      "2021-11-02T04:06:21.546001\n",
      "lr: 7.595988680656638e-05\n",
      "Train epoch 42, time: 274.8s, loss=1.18693, h_loss=1.18693\n",
      "Valid epoch 42, time: 15.9s, loss=3.27118, h_loss=3.27118\n",
      "\n",
      "2021-11-02T04:11:20.534219\n",
      "lr: 5.8512462591724364e-05\n",
      "Train epoch 43, time: 275.1s, loss=1.17760, h_loss=1.17760\n",
      "Valid epoch 43, time: 16.0s, loss=3.28042, h_loss=3.28042\n",
      "\n",
      "2021-11-02T04:16:20.363600\n",
      "lr: 4.321593102476221e-05\n",
      "Train epoch 44, time: 275.0s, loss=1.16819, h_loss=1.16819\n",
      "Valid epoch 44, time: 16.0s, loss=3.27478, h_loss=3.27478\n",
      "\n",
      "2021-11-02T04:21:20.072453\n",
      "lr: 3.0144815269872318e-05\n",
      "Train epoch 45, time: 275.8s, loss=1.16267, h_loss=1.16267\n",
      "Valid epoch 45, time: 15.9s, loss=3.27423, h_loss=3.27423\n",
      "\n",
      "2021-11-02T04:26:20.369552\n",
      "lr: 1.9362796489108634e-05\n",
      "Train epoch 46, time: 274.7s, loss=1.15463, h_loss=1.15463\n",
      "Valid epoch 46, time: 16.1s, loss=3.28458, h_loss=3.28458\n",
      "\n",
      "2021-11-02T04:31:19.327777\n",
      "lr: 1.0922403594166195e-05\n",
      "Train epoch 47, time: 274.9s, loss=1.15389, h_loss=1.15389\n",
      "Valid epoch 47, time: 16.1s, loss=3.28089, h_loss=3.28089\n",
      "\n",
      "2021-11-02T04:36:18.827847\n",
      "lr: 4.864757330803851e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 48, time: 275.8s, loss=1.15050, h_loss=1.15050\n",
      "Valid epoch 48, time: 15.8s, loss=3.27899, h_loss=3.27899\n",
      "\n",
      "2021-11-02T04:41:19.197604\n",
      "lr: 1.2193699427042562e-06\n",
      "Train epoch 49, time: 274.8s, loss=1.15116, h_loss=1.15116\n",
      "Valid epoch 49, time: 16.0s, loss=3.27945, h_loss=3.27945\n"
     ]
    }
   ],
   "source": [
    "# @title Запуск обучения handwritten[image]\n",
    "\n",
    "DEMO_LOGS = False #@param {type:\"boolean\"}\n",
    "\n",
    "handwritten_train = df[(df['task_id'] == 'handwritten') & (df['stage'] == 'train')]\n",
    "handwritten_valid = df[(df['task_id'] == 'handwritten') & (df['stage'] == 'valid')]\n",
    "\n",
    "handwritten_train_dataset = DatasetRetriever(\n",
    "    task_ids=handwritten_train['task_id'].values,\n",
    "    input_images=handwritten_train['input_image'].values,\n",
    "    input_texts=handwritten_train['input_text'].values,\n",
    "    output_texts=handwritten_train['output_text'].values,\n",
    "    output_boxes=handwritten_train['output_boxes'].values,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='train',\n",
    "    handwritten_max_tokens_length=8,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "handwritten_valid_dataset = DatasetRetriever(\n",
    "    task_ids=handwritten_valid['task_id'].values,\n",
    "    input_images=handwritten_valid['input_image'].values,\n",
    "    input_texts=handwritten_valid['input_text'].values,\n",
    "    output_texts=handwritten_valid['output_text'].values,\n",
    "    output_boxes=handwritten_valid['output_boxes'].values,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='valid',\n",
    "    handwritten_max_tokens_length=8,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "CONFIG = {\n",
    "    'description': 'Обучение handwritten task',\n",
    "    'experiment_name': f'fusion-brain-handwritten-{round(datetime.utcnow().timestamp())}',\n",
    "    'lr': 0.00008,\n",
    "    'bs': 64,\n",
    "    'num_epochs': 50,\n",
    "    'max_lr': 0.001,\n",
    "    'pct_start': 0.1,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    handwritten_train_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=RandomSampler(handwritten_train_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    handwritten_valid_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=SequentialSampler(handwritten_valid_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "model.freeze_gpt()\n",
    "model = model.to(device)\n",
    "#model.load_state_dict(torch.load('MainWeights/c2c_handwritten.pt')['model_state_dict'])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['max_lr'],\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    pct_start=CONFIG['pct_start'],\n",
    "    epochs=CONFIG['num_epochs'],\n",
    ")\n",
    "\n",
    "if DEMO_LOGS:\n",
    "    print(open('./saved_models/fusion-brain-handwritten-1631797524/log.txt').read())\n",
    "else:\n",
    "    experiment = FusionBrainExperiment(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=None,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        base_dir=f'./saved_models',\n",
    "        experiment_name=CONFIG['experiment_name'],\n",
    "        verbose_step=10**5,\n",
    "        seed=42,\n",
    "        best_saving=True,\n",
    "        last_saving=True,\n",
    "    )\n",
    "    experiment.fit(train_loader, valid_loader, CONFIG['num_epochs'])\n",
    "    experiment.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655ab291",
   "metadata": {},
   "source": [
    "## VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d23b08d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-10-30T14:50:33.652155\n",
      "lr: 3.9999999999999996e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/ipykernel_launcher.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1319/759358235.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mlast_saving\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/tpu_star/experiment/torch_gpu.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, valid_loader, n_epochs)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rebuild_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;31m# #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mstage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/tpu_star/experiment/torch_gpu.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1319/4045563101.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_detection_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'vqa'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vqa_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'task_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtask_id\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1319/4045563101.py\u001b[0m in \u001b[0;36mget_vqa_sample\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m## Question and Answer ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0minput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0moutput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2173\u001b[0m             \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2174\u001b[0m             \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2175\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2176\u001b[0m         )\n\u001b[1;32m   2177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2509\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2511\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2512\u001b[0m         )\n\u001b[1;32m   2513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m             )\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;31m# Simple mapping string => AddedToken for special tokens with specific tokenization behaviors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         all_special_tokens_extended = dict(\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens_extended\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAddedToken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mall_special_tokens_extended\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \"\"\"\n\u001b[1;32m   1244\u001b[0m         \u001b[0mall_toks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1245\u001b[0;31m         \u001b[0mset_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_tokens_map_extended\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1246\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattr_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0mall_toks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_toks\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mattr_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mspecial_tokens_map_extended\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1217\u001b[0m         \"\"\"\n\u001b[1;32m   1218\u001b[0m         \u001b[0mset_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSPECIAL_TOKENS_ATTRIBUTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0mattr_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Запуск обучения vqa[image+text]\n",
    "\n",
    "DEMO_LOGS = False\n",
    "\n",
    "vqa_train = df[(df['task_id'] == 'vqa') & (df['stage'] == 'train')]\n",
    "vqa_valid = df[(df['task_id'] == 'vqa') & (df['stage'] == 'valid')]\n",
    "\n",
    "vqa_train_dataset = DatasetRetriever(\n",
    "    task_ids=vqa_train['task_id'].values,\n",
    "    input_images=vqa_train['input_image'].values,\n",
    "    input_texts=vqa_train['input_text'].values,\n",
    "    output_texts=vqa_train['output_text'].values,\n",
    "    output_boxes=vqa_train['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='train',\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "vqa_valid_dataset = DatasetRetriever(\n",
    "    task_ids=vqa_valid['task_id'].values,\n",
    "    input_images=vqa_valid['input_image'].values,\n",
    "    input_texts=vqa_valid['input_text'].values,\n",
    "    output_texts=vqa_valid['output_text'].values,\n",
    "    output_boxes=vqa_valid['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='valid',\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "model.freeze_gpt(freeze_pos=False, freeze_ln=False, freeze_attn=True, freeze_ff=True, freeze_other=False)\n",
    "\n",
    "CONFIG = {\n",
    "    'description': 'Обучение vqa task',\n",
    "    'experiment_name': f'fusion-brain-vqa-{round(datetime.utcnow().timestamp())}',\n",
    "    'lr': 0.000004,\n",
    "    'bs': 64,\n",
    "    'num_epochs': 20,\n",
    "    'max_lr': 0.001,\n",
    "    'pct_start': 0.1,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    vqa_train_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=RandomSampler(vqa_train_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    vqa_valid_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=SequentialSampler(vqa_valid_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "#model.load_state_dict(torch.load('MainWeights/c2c_handwritten.pt'), strict=False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['max_lr'],\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    pct_start=CONFIG['pct_start'],\n",
    "    epochs=CONFIG['num_epochs'],\n",
    ")\n",
    "\n",
    "if DEMO_LOGS:\n",
    "    print(open('./saved_models/fusion-brain-vqa-1631794561/log.txt').read())\n",
    "else:\n",
    "    experiment = FusionBrainExperiment(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=None,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        base_dir=f'./saved_models',\n",
    "        experiment_name=CONFIG['experiment_name'],\n",
    "        verbose_step=10**5,\n",
    "        seed=42,\n",
    "        best_saving=True,\n",
    "        last_saving=True,\n",
    "    )\n",
    "    experiment.fit(train_loader, valid_loader, CONFIG['num_epochs'])\n",
    "    experiment.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62061271",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dabe8e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-10-30T14:50:58.898481\n",
      "lr: 4.000000000000002e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1319/1166003257.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mlast_saving\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/tpu_star/experiment/torch_gpu.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, valid_loader, n_epochs)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rebuild_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;31m# #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mstage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/tpu_star/experiment/torch_gpu.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1319/4045563101.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trans_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'detection'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_detection_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'vqa'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vqa_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1319/4045563101.py\u001b[0m in \u001b[0;36mget_detection_sample\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mimage_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Запуск обучения detection[image+text]\n",
    "\n",
    "DEMO_LOGS = False\n",
    "\n",
    "detection_train = df[(df['task_id'] == 'detection') & (df['stage'] == 'train')]\n",
    "detection_valid = df[(df['task_id'] == 'detection') & (df['stage'] == 'valid')]\n",
    "\n",
    "detection_train_dataset = DatasetRetriever(\n",
    "    task_ids=detection_train['task_id'].values,\n",
    "    input_images=detection_train['input_image'].values,\n",
    "    input_texts=detection_train['input_text'].values,\n",
    "    output_texts=detection_train['output_text'].values,\n",
    "    output_boxes=detection_train['output_boxes'].values,\n",
    "    stage='train',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "detection_valid_dataset = DatasetRetriever(\n",
    "    task_ids=detection_valid['task_id'].values,\n",
    "    input_images=detection_valid['input_image'].values,\n",
    "    input_texts=detection_valid['input_text'].values,\n",
    "    output_texts=detection_valid['output_text'].values,\n",
    "    output_boxes=detection_valid['output_boxes'].values,\n",
    "    stage='valid',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "model.freeze_gpt(freeze_pos=True, freeze_ln=False, freeze_attn=True, freeze_ff=True, freeze_other=False)\n",
    "\n",
    "CONFIG = {\n",
    "    'description': 'Обучение detection task',\n",
    "    'experiment_name': f'fusion-brain-detection-{round(datetime.utcnow().timestamp())}',\n",
    "    'lr': 0.000004,\n",
    "    'bs': 32,\n",
    "    'num_epochs': 60,\n",
    "    'max_lr': 0.0001,\n",
    "    'pct_start': 0.1,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    detection_train_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=RandomSampler(detection_train_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    detection_valid_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=SequentialSampler(detection_valid_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "#model.load_state_dict(torch.load('MainWeights/c2c_handwritten_vqa.pt'), strict=False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['max_lr'],\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    pct_start=CONFIG['pct_start'],\n",
    "    epochs=CONFIG['num_epochs'],\n",
    ")\n",
    "\n",
    "if DEMO_LOGS:\n",
    "    print(open('./saved_models/fusion-brain-detection-1631794561/log.txt').read())\n",
    "else:\n",
    "    experiment = FusionBrainExperiment(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=None,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        base_dir=f'./saved_models',\n",
    "        experiment_name=CONFIG['experiment_name'],\n",
    "        verbose_step=10**5,\n",
    "        seed=42,\n",
    "        best_saving=True,\n",
    "        last_saving=True,\n",
    "    )\n",
    "    experiment.fit(train_loader, valid_loader, CONFIG['num_epochs'])\n",
    "    experiment.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45ff95",
   "metadata": {},
   "source": [
    "## Fusion Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Запуск обучения fusion brain [text, image]\n",
    "\n",
    "DEMO_LOGS = False #@param {type:\"boolean\"}\n",
    "\n",
    "CONFIG = {\n",
    "    'description': 'Fusion Brain',\n",
    "    'experiment_name': f'fusion-brain-main-{round(datetime.utcnow().timestamp())}',\n",
    "    'lr': 0.00008,\n",
    "    'bs': 64,\n",
    "    'num_epochs': 20,\n",
    "    'max_lr': 0.00004,\n",
    "    'pct_start': 0.1,\n",
    "    'final_div_factor': 100\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=BalanceClassSampler(labels=train_dataset.get_task_labels()),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=BalanceClassSampler(labels=valid_dataset.get_task_labels()),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "model.freeze_gpt(False, False, False, False, False)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('MainWeights/c2c_handwritten.pt'), strict=False)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['max_lr'],\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    pct_start=CONFIG['pct_start'],\n",
    "    epochs=CONFIG['num_epochs'],\n",
    "    final_div_factor=CONFIG['final_div_factor'],\n",
    ")\n",
    "\n",
    "if DEMO_LOGS:\n",
    "    print(open('./saved_models/fusion-brain-main-1631924391/log.txt').read())\n",
    "else:\n",
    "    experiment = FusionBrainExperiment(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=None,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        base_dir=f'./saved_models',\n",
    "        experiment_name=CONFIG['experiment_name'],\n",
    "        verbose_step=10**5,\n",
    "        seed=42,\n",
    "        best_saving=True,\n",
    "        last_saving=True,\n",
    "    )\n",
    "    experiment.fit(train_loader, valid_loader, CONFIG['num_epochs'])\n",
    "    experiment.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fa8018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eafc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
