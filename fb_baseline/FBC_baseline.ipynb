{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fb52255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.8.0 torchvision==0.8.2 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install tpu_star==0.0.1rc10\n",
    "# !pip install albumentations==0.5.2\n",
    "# !pip install einops==0.3.2 \n",
    "# !pip install transformers==4.10.0\n",
    "# !pip install adapter-transformers\n",
    "# !pip install colorednoise==1.1.1\n",
    "# !pip install catalyst==21.8 \n",
    "# !pip install opencv-python==4.5.3\n",
    "# !pip install gdown==4.0.2\n",
    "# !pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eaeace3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 96\n",
      "RAM GB: 1510.6\n",
      "PyTorch version: 1.7.1+cu101\n",
      "CUDA version: 10.1\n",
      "cuDNN version: 7603\n",
      "device: cuda\n",
      "Tue Nov  2 12:38:13 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.129      Driver Version: 410.129      CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM3...  On   | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    49W / 350W |     13MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Доступные ресурсы\n",
    "import multiprocessing\n",
    "import torch\n",
    "from psutil import virtual_memory\n",
    "\n",
    "ram_gb = round(virtual_memory().total / 1024**3, 1)\n",
    "\n",
    "print('CPU:', multiprocessing.cpu_count())\n",
    "print('RAM GB:', ram_gb)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device.type)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b6d2b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 12:38:14.590958: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "/home/user/conda/lib/python3.7/site-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from skimage import io\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from catalyst.data import BalanceClassSampler, DistributedSamplerWrapper\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tpu_star.experiment import TorchGPUExperiment\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'fusion_brain_aij2021/fb_baseline')\n",
    "from fb_utils.download import download_and_extract\n",
    "from fb_utils.loss import LabelSmoothing, onehot\n",
    "from fb_utils.metrics import cer, wer, string_accuracy, acc, vqa_evaluate, detection_evaluate\n",
    "from fb_utils.handwritten import simple_detect_lang, CTCLabeling, resize_if_need, make_img_padding\n",
    "from fb_utils.BLEU import _bleu\n",
    "from fb_utils.c2c_eval import Beam, eval_bleu\n",
    "\n",
    "from fb_utils.detection_vqa_htr import (vqa_evaluation, detection_evaluation, htr_evaluation,\n",
    "                                    CrossAttentionLayer, MLP, FeedForwardComponent, DetectionCriterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee3690b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>modality</th>\n",
       "      <th>lang</th>\n",
       "      <th>input_image</th>\n",
       "      <th>input_text</th>\n",
       "      <th>output_text</th>\n",
       "      <th>stage</th>\n",
       "      <th>output_boxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vqa</td>\n",
       "      <td>image+text</td>\n",
       "      <td>en</td>\n",
       "      <td>2334634.jpg</td>\n",
       "      <td>How many people are in this picture?</td>\n",
       "      <td>one</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>detection</td>\n",
       "      <td>image+text</td>\n",
       "      <td>en</td>\n",
       "      <td>2327362.jpg</td>\n",
       "      <td>green and wite signpost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>[[89.0, 407.0, 127.0, 92.0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>detection</td>\n",
       "      <td>image+text</td>\n",
       "      <td>en</td>\n",
       "      <td>2375174.jpg</td>\n",
       "      <td>yellow bag under cat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>[[32.0, 228.0, 297.0, 113.0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>detection</td>\n",
       "      <td>image+text</td>\n",
       "      <td>en</td>\n",
       "      <td>2333039.jpg</td>\n",
       "      <td>pointed white ear with black</td>\n",
       "      <td>NaN</td>\n",
       "      <td>valid</td>\n",
       "      <td>[[140.0, 140.0, 35.0, 26.0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>detection</td>\n",
       "      <td>image+text</td>\n",
       "      <td>en</td>\n",
       "      <td>2342071.jpg</td>\n",
       "      <td>black roman numerals on white clock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>[[160.0, 163.0, 33.0, 37.0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vqa</td>\n",
       "      <td>image+text</td>\n",
       "      <td>en</td>\n",
       "      <td>2375875.jpg</td>\n",
       "      <td>Who is looking at the camera?</td>\n",
       "      <td>the cat</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>detection</td>\n",
       "      <td>image+text</td>\n",
       "      <td>en</td>\n",
       "      <td>2356404.jpg</td>\n",
       "      <td>barber wears a blue striped shirt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>[[273.0, 63.0, 225.0, 307.0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vqa</td>\n",
       "      <td>image+text</td>\n",
       "      <td>en</td>\n",
       "      <td>2333096.jpg</td>\n",
       "      <td>What is the object on the tracks?</td>\n",
       "      <td>a train</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>detection</td>\n",
       "      <td>image+text</td>\n",
       "      <td>ru</td>\n",
       "      <td>2402391.jpg</td>\n",
       "      <td>полосатый самолет в небе</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>[[170.0, 91.0, 207.0, 97.0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>detection</td>\n",
       "      <td>image+text</td>\n",
       "      <td>en</td>\n",
       "      <td>2345347.jpg</td>\n",
       "      <td>person has skis on feet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>[[359.0, 297.0, 62.0, 51.0]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     task_id    modality lang  input_image  \\\n",
       "0        vqa  image+text   en  2334634.jpg   \n",
       "1  detection  image+text   en  2327362.jpg   \n",
       "2  detection  image+text   en  2375174.jpg   \n",
       "3  detection  image+text   en  2333039.jpg   \n",
       "4  detection  image+text   en  2342071.jpg   \n",
       "5        vqa  image+text   en  2375875.jpg   \n",
       "6  detection  image+text   en  2356404.jpg   \n",
       "7        vqa  image+text   en  2333096.jpg   \n",
       "8  detection  image+text   ru  2402391.jpg   \n",
       "9  detection  image+text   en  2345347.jpg   \n",
       "\n",
       "                             input_text output_text  stage  \\\n",
       "0  How many people are in this picture?         one  train   \n",
       "1               green and wite signpost         NaN  train   \n",
       "2                  yellow bag under cat         NaN  train   \n",
       "3          pointed white ear with black         NaN  valid   \n",
       "4   black roman numerals on white clock         NaN  train   \n",
       "5         Who is looking at the camera?     the cat  train   \n",
       "6     barber wears a blue striped shirt         NaN  train   \n",
       "7     What is the object on the tracks?     a train  train   \n",
       "8              полосатый самолет в небе         NaN  train   \n",
       "9               person has skis on feet         NaN  train   \n",
       "\n",
       "                    output_boxes  \n",
       "0                            NaN  \n",
       "1   [[89.0, 407.0, 127.0, 92.0]]  \n",
       "2  [[32.0, 228.0, 297.0, 113.0]]  \n",
       "3   [[140.0, 140.0, 35.0, 26.0]]  \n",
       "4   [[160.0, 163.0, 33.0, 37.0]]  \n",
       "5                            NaN  \n",
       "6  [[273.0, 63.0, 225.0, 307.0]]  \n",
       "7                            NaN  \n",
       "8   [[170.0, 91.0, 207.0, 97.0]]  \n",
       "9   [[359.0, 297.0, 62.0, 51.0]]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Подготовка данных и сбор в единый DataFrame\n",
    "# #\n",
    "# Handwritten\n",
    "# #\n",
    "json_marking = json.load(open('handwritten/train_labels.json', 'rb'))\n",
    "marking = []\n",
    "for image_name, text in json_marking.items():\n",
    "    marking.append({\n",
    "        'path': image_name,\n",
    "        'text': text,\n",
    "        'lang': simple_detect_lang(text),\n",
    "    })\n",
    "df_handwritten = pd.DataFrame(marking)\n",
    "df_handwritten['stage'] = 'train'\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "train_index, valid_index = next(skf.split(df_handwritten.index, df_handwritten['lang']))\n",
    "df_handwritten.loc[valid_index, 'stage'] = 'valid'\n",
    "# #\n",
    "# Detection\n",
    "# #\n",
    "json_true_zsod = json.load(open('russian_detection_vqa/vg_intersection_eng.json', 'rb'))\n",
    "json_true_ru_zsod = json.load(open('russian_detection_vqa/vg_intersection_rus.json', 'rb'))\n",
    "json_true_zsod.update(json_true_ru_zsod)\n",
    "ru_images = set(json_true_ru_zsod.keys())\n",
    "marking = []\n",
    "for image_name in json_true_zsod:\n",
    "    marking.extend([{\n",
    "        'task_id': 'detection',\n",
    "        'path': image_name,\n",
    "        'req': request,\n",
    "        'boxes': boxes,\n",
    "        'lang': 'ru' if image_name in ru_images else 'en'\n",
    "    } for request, boxes in json_true_zsod[image_name].items() if boxes])\n",
    "df_detection = pd.DataFrame(marking)\n",
    "df_detection['stage'] = 'train'\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "train_index, valid_index = next(skf.split(df_detection.index, df_detection['lang']))\n",
    "df_detection.loc[valid_index, 'stage'] = 'valid'\n",
    "# #\n",
    "# Eval Detection #\n",
    "# #\n",
    "valid_detection_images = set(df_detection[df_detection['stage'] == 'valid']['path'])\n",
    "marking = []\n",
    "for image_name in json_true_zsod:\n",
    "    if image_name in valid_detection_images:\n",
    "        marking.append({\n",
    "            'task_id': 'detection',\n",
    "            'path': image_name,\n",
    "            'req': ';'.join([request for request in json_true_zsod[image_name].keys()]),\n",
    "            'boxes': [boxes for boxes in json_true_zsod[image_name].values()],\n",
    "            'lang': 'ru' if image_name in ru_images else 'en'\n",
    "        })\n",
    "df_eval_detection = pd.DataFrame(marking)\n",
    "df_eval_detection['stage'] = 'test'\n",
    "# #\n",
    "# VQA\n",
    "# #\n",
    "json_questions = json.load(open('train_detection_vqa/vqa_questions.json', 'rb'))\n",
    "json_true_vqa = json.load(open('train_detection_vqa/vqa_answers.json', 'rb'))\n",
    "marking = []\n",
    "for key in json_questions:\n",
    "    if json_true_vqa[key]['lang'] == 'en':\n",
    "        marking.append({\n",
    "            'path': json_questions[key]['file_name'],\n",
    "            'question': json_questions[key]['question'],\n",
    "            'answer': json_true_vqa[key]['answer'],\n",
    "            'lang': json_true_vqa[key]['lang']\n",
    "        })\n",
    "df_vqa = pd.DataFrame(marking)\n",
    "df_vqa['stage'] = 'train'\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "train_index, valid_index = next(skf.split(df_vqa.index, df_vqa['lang']))\n",
    "df_vqa.loc[valid_index, 'stage'] = 'valid'\n",
    "# #\n",
    "# C2C\n",
    "# #\n",
    "df_c2c = pd.read_json(path_or_buf='c2c/java-python.jsonl', lines=True)\n",
    "train, test = train_test_split(df_c2c, test_size=0.2)\n",
    "valid, test = train_test_split(test, test_size=0.05)\n",
    "\n",
    "df_c2c.loc[train.index.to_list(), 'stage'] = 'train'\n",
    "df_c2c.loc[valid.index.to_list(), 'stage'] = 'valid'\n",
    "df_c2c.loc[test.index.to_list(), 'stage'] = 'test'\n",
    "\n",
    "\n",
    "# #\n",
    "# Merge in common set\n",
    "# #\n",
    "dataset = []\n",
    "for lang, image_name, text, stage in zip(df_handwritten['lang'], df_handwritten['path'], df_handwritten['text'], df_handwritten['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'handwritten',   \n",
    "        'modality': 'image',\n",
    "        'lang': lang,\n",
    "        'input_image': image_name,\n",
    "        'output_text': text,\n",
    "        'stage': stage,\n",
    "    })\n",
    "    \n",
    "for java, python, stage in zip(df_c2c['java'], df_c2c['python'], df_c2c['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'trans',\n",
    "        'modality': 'code',\n",
    "        'input_text': java,\n",
    "        'output_text': python,\n",
    "        'stage': stage,\n",
    "    })\n",
    "    \n",
    "for lang, image_name, text_input, text_output, stage in zip(df_vqa['lang'], df_vqa['path'], df_vqa['question'], df_vqa['answer'], df_vqa['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'vqa', \n",
    "        'modality': 'image+text',\n",
    "        'lang': lang,\n",
    "        'input_image': image_name,\n",
    "        'input_text': text_input,\n",
    "        'output_text': text_output,\n",
    "        'stage': stage,\n",
    "    })\n",
    "for lang, image_name, text_input, boxes, stage in zip(df_detection['lang'], df_detection['path'], df_detection['req'], df_detection['boxes'], df_detection['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'detection', \n",
    "        'modality': 'image+text',\n",
    "        'lang': lang,\n",
    "        'input_image': image_name,\n",
    "        'input_text': text_input,\n",
    "        'output_boxes': boxes,\n",
    "        'stage': stage,\n",
    "    })\n",
    "for lang, image_name, text_input, boxes, stage in zip(df_eval_detection['lang'], df_eval_detection['path'], df_eval_detection['req'], df_eval_detection['boxes'], df_eval_detection['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'detection', \n",
    "        'modality': 'image+text',\n",
    "        'lang': lang,\n",
    "        'input_image': image_name,\n",
    "        'input_text': text_input,\n",
    "        'output_boxes': boxes,\n",
    "        'stage': stage,\n",
    "    })\n",
    "\n",
    "random.shuffle(dataset)\n",
    "df = pd.DataFrame(dataset)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0def9497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['modality'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "128dbb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, \n",
    "                 task_ids, \n",
    "                 input_images,\n",
    "                 input_texts,\n",
    "                 output_texts,\n",
    "                 output_boxes,\n",
    "                 tokenizer, \n",
    "                 stage,\n",
    "                 handwritten_max_tokens_length,\n",
    "                 max_request_tokens_length,\n",
    "                 vqa_max_tokens_length, \n",
    "                 task_augs=None):\n",
    "        super().__init__()\n",
    "        self.task_ids = task_ids\n",
    "        \n",
    "        self.input_images = input_images\n",
    "        self.input_texts = input_texts\n",
    "        self.output_texts = output_texts\n",
    "        \n",
    "        self.task_augs = task_augs or {}\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stage = stage\n",
    "\n",
    "        # handwritten[image]:\n",
    "        self.handwritten_max_tokens_length = handwritten_max_tokens_length\n",
    "        self.handwritten_image_w = 512\n",
    "        self.handwritten_image_h = 128\n",
    "\n",
    "        # code2code\n",
    "        self.code_max_length = 512\n",
    "        \n",
    "        # detection[image, text]:\n",
    "        self.max_request_tokens_length = max_request_tokens_length\n",
    "        self.output_boxes = output_boxes\n",
    "        \n",
    "        # vqa[image, text]:\n",
    "        self.vqa_max_tokens_length = vqa_max_tokens_length\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        task_id = self.task_ids[idx]\n",
    "        if task_id == 'handwritten':\n",
    "            return self.get_handwritten_sample(idx)\n",
    "        elif task_id == 'trans':\n",
    "            return self.get_trans_sample(idx)\n",
    "        elif task_id == 'detection':\n",
    "            return self.get_detection_sample(idx)\n",
    "        elif task_id == 'vqa':\n",
    "            return self.get_vqa_sample(idx)\n",
    "        return {'task_id': task_id}\n",
    "\n",
    "    def get_trans_sample(self, idx):\n",
    "            \n",
    "        source = self.input_texts[idx]\n",
    "        encoded_source = self.tokenizer.encode(str(source))\n",
    "        target = self.output_texts[idx]\n",
    "        encoded_target = self.tokenizer.encode(str(target))\n",
    "        \n",
    "        input_ids, input_labels = self.pad_and_get_mask(encoded_target, encoded_source, self.tokenizer)\n",
    "        input_ids, input_labels = torch.tensor(input_ids), torch.tensor(input_labels)\n",
    "        \n",
    "        return {\n",
    "            'task_id': self.task_ids[idx],\n",
    "            'input_ids': input_ids,\n",
    "            'input_labels': input_labels,\n",
    "            'target': target\n",
    "        }\n",
    "\n",
    "    def get_handwritten_sample(self, idx):\n",
    "        path = 'handwritten/images/' + self.input_images[idx]\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image, _ = self.resize_image(image)\n",
    "\n",
    "        gt_text = self.output_texts[idx]\n",
    "        tokens = self.tokenizer.encode(gt_text)[:6]\n",
    "        encoded = [self.tokenizer.bos_token_id] + tokens + [self.tokenizer.eos_token_id]\n",
    "        labels = [2] * (len(tokens) + 1) + [0]\n",
    "        \n",
    "        pad_len = self.handwritten_max_tokens_length - len(encoded)\n",
    "        encoded += [self.tokenizer.pad_token_id] * pad_len\n",
    "        labels += [0] * pad_len\n",
    "\n",
    "        ## Augs ##\n",
    "        transforms = self.task_augs.get('handwritten')\n",
    "        if transforms:\n",
    "            image = transforms(image=image)['image']\n",
    "        ##########\n",
    "\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "\n",
    "        return {\n",
    "            'task_id': self.task_ids[idx],\n",
    "            'image': image,\n",
    "            'gt_text': gt_text,\n",
    "            'labels': torch.tensor(labels),\n",
    "            'encoded': torch.tensor(encoded),\n",
    "        }\n",
    "    \n",
    "    def get_detection_sample(self, idx):\n",
    "        path = 'russian_detection_vqa/images/' + self.input_images[idx]\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image_h, image_w, _ = image.shape\n",
    "        \n",
    "        ## Augs ##\n",
    "        transforms = self.task_augs.get('detection')\n",
    "        if transforms:\n",
    "            image = transforms(image=image)['image']\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        image_name = self.input_images[idx]\n",
    "        ##########\n",
    "        \n",
    "        ## Input tokens ##\n",
    "        if self.stage == 'train' or self.stage == 'valid':\n",
    "            input_text = self.input_texts[idx]\n",
    "            input_tokens = self.tokenizer.encode_plus(input_text)\n",
    "            input_tokens['input_ids'] = input_tokens['input_ids'][:21]\n",
    "            input_tokens['attention_mask'] = input_tokens['attention_mask'][:21]\n",
    "            pad_len = self.max_request_tokens_length - len(input_tokens['input_ids'])\n",
    "            input_tokens['input_ids'] += [self.tokenizer.pad_token_id] * pad_len\n",
    "            input_tokens['attention_mask'] += [0] * pad_len\n",
    "            input_ids = torch.tensor(input_tokens['input_ids'])\n",
    "            attention_mask = torch.tensor(input_tokens['attention_mask'])\n",
    "        else:\n",
    "            input_texts = self.input_texts[idx].split(';')\n",
    "            input_ids = list(map(self.tokenizer.encode, input_texts))\n",
    "            attention_mask = [[1 for _ in input_token] for input_token in input_ids]\n",
    "            input_ids = [torch.tensor(input_id) for input_id in input_ids]\n",
    "            attention_mask = [torch.tensor(mask) for mask in attention_mask]\n",
    "        ###########\n",
    "        \n",
    "        ## Boxes ##\n",
    "        output_boxes = self.output_boxes[idx]\n",
    "        if self.stage == 'train' or self.stage == 'valid':\n",
    "            output_boxes = torch.tensor(output_boxes, dtype=torch.float32)\n",
    "            output_boxes[:, 0] /= image_w\n",
    "            output_boxes[:, 1] /= image_h\n",
    "            output_boxes[:, 2] /= image_w\n",
    "            output_boxes[:, 3] /= image_h\n",
    "        else:\n",
    "            output_boxes = {\n",
    "                input_text: boxes for input_text, boxes in zip(input_texts, output_boxes)\n",
    "            }\n",
    "        ##########\n",
    "        \n",
    "        return {\n",
    "            'task_id': self.task_ids[idx],\n",
    "            'image_name': image_name,\n",
    "            'image': image,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'boxes': output_boxes,\n",
    "            'size': (image_h, image_w)\n",
    "        }\n",
    "    \n",
    "    def get_vqa_sample(self, idx): \n",
    "        path = 'train_detection_vqa/images/' + self.input_images[idx]\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        ## Augs ##\n",
    "        transforms = self.task_augs.get('vqa')\n",
    "        if transforms:\n",
    "            image = transforms(image=image)['image']\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        image_name = self.input_images[idx]\n",
    "        ##########\n",
    "        \n",
    "        ## Question and Answer ##\n",
    "        input_text = self.input_texts[idx]\n",
    "        input_tokens = self.tokenizer.encode(input_text)\n",
    "        output_text = self.output_texts[idx]\n",
    "        output_tokens = self.tokenizer.encode(output_text)\n",
    "        \n",
    "        if self.stage == 'train' or self.stage == 'valid':\n",
    "            input_tokens, output_tokens = input_tokens[:12], output_tokens[:7]\n",
    "            input_ids = input_tokens + [self.tokenizer.bos_token_id] + output_tokens + [self.tokenizer.eos_token_id]\n",
    "            labels = [1] * len(input_tokens) + [2] * (len(output_tokens) + 1) + [0]\n",
    "            \n",
    "            pad_len = self.vqa_max_tokens_length - len(input_ids)\n",
    "            input_ids += [self.tokenizer.pad_token_id] * pad_len\n",
    "            labels += [0] * pad_len\n",
    "        else:\n",
    "            input_ids = input_tokens + [self.tokenizer.bos_token_id]\n",
    "            labels = [1] * len(input_tokens) + [2]\n",
    "        ##########\n",
    "        \n",
    "        return {\n",
    "            'task_id': self.task_ids[idx],\n",
    "            'image_name': image_name,\n",
    "            'image': image,\n",
    "            'input_ids': torch.tensor(input_ids),\n",
    "            'labels': torch.tensor(labels),\n",
    "            'target': output_text\n",
    "        }\n",
    "\n",
    "\n",
    "    def resize_image(self, image):\n",
    "        image, coef = resize_if_need(image, self.handwritten_image_h, self.handwritten_image_w)\n",
    "        image = make_img_padding(image, self.handwritten_image_h, self.handwritten_image_w)\n",
    "        return image, coef\n",
    "    \n",
    "    def pad_and_get_mask(self, target, source, tokenizer):\n",
    "        if self.stage == 'test':\n",
    "            target = []\n",
    "        while len(target) + len(source) + 2 > self.code_max_length:\n",
    "            if len(target) > len(source):\n",
    "                target = target[:-1]\n",
    "            else:\n",
    "                source = source[:-1]\n",
    "        if self.stage == 'train' or self.stage == 'valid':\n",
    "            inputs = source + [tokenizer.bos_token_id] + target + [tokenizer.eos_token_id]\n",
    "            labels = [1] * len(source) + [2] * (len(target) + 1) + [0]\n",
    "\n",
    "        else:\n",
    "            inputs = source + [tokenizer.bos_token_id]\n",
    "            labels = [1] * len(source) + [2]\n",
    "            \n",
    "            return inputs, labels\n",
    "        \n",
    "        assert len(inputs) <= self.code_max_length\n",
    "        pad_len = self.code_max_length - len(inputs)\n",
    "        inputs += [tokenizer.pad_token_id] * pad_len\n",
    "        labels += [0] * pad_len\n",
    "        assert len(inputs) == len(labels)\n",
    "        \n",
    "        return inputs, labels\n",
    "\n",
    " \n",
    "    def __len__(self) -> int:\n",
    "        return self.task_ids.shape[0]\n",
    "\n",
    "    def get_task_labels(self):\n",
    "        return list(self.task_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f701c820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:1748: DeprecationWarning: This class has been deprecated. Please use ImageCompression\n",
      "  warnings.warn(\"This class has been deprecated. Please use ImageCompression\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "task_augs = {\n",
    "    'handwritten': A.Compose([\n",
    "        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.25, always_apply=False),\n",
    "        A.Rotate(limit=3, interpolation=1, border_mode=0, p=0.5),\n",
    "        A.JpegCompression(quality_lower=75, p=0.5),\n",
    "    ], p=1.0),\n",
    "    'vqa': A.Compose([\n",
    "        A.Resize(224, 224, always_apply=True),\n",
    "        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ], p=1.0),\n",
    "    'detection': A.Compose([\n",
    "        A.Resize(224, 224, always_apply=True),\n",
    "        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ], p=1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0508ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['stage'] == 'train']\n",
    "df_valid = df[df['stage'] == 'valid']\n",
    "df_eval = df[df['stage'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51e35a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at sberbank-ai/rugpt3small_based_on_gpt2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(model_name, bos_token='<s>',\n",
    "            eos_token='</s>', pad_token='<pad>', unk_token='<|UNKNOWN|>', sep_token='<|SEP|>')\n",
    "\n",
    "gpt_model = GPT2Model.from_pretrained(model_name)\n",
    "gpt_model.resize_token_embeddings(len(gpt_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35e689cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetRetriever(\n",
    "    task_ids=df_train['task_id'].values,\n",
    "    input_images=df_train['input_image'].values,\n",
    "    input_texts=df_train['input_text'].values,\n",
    "    output_texts=df_train['output_text'].values,\n",
    "    output_boxes=df_train['output_boxes'].values,\n",
    "    stage='train',\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    handwritten_max_tokens_length=8,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "valid_dataset = DatasetRetriever(\n",
    "    task_ids=df_valid['task_id'].values,\n",
    "    input_images=df_valid['input_image'].values,\n",
    "    input_texts=df_valid['input_text'].values,\n",
    "    output_texts=df_valid['output_text'].values,\n",
    "    output_boxes=df_valid['output_boxes'].values,\n",
    "    stage='valid',\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    handwritten_max_tokens_length=8,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "eval_dataset = DatasetRetriever(\n",
    "    task_ids=df_eval['task_id'].values,\n",
    "    input_images=df_eval['input_image'].values,\n",
    "    input_texts=df_eval['input_text'].values,\n",
    "    output_texts=df_eval['output_text'].values,\n",
    "    output_boxes=df_eval['output_boxes'].values,\n",
    "    stage='test',\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    handwritten_max_tokens_length=8,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd4d4894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Как выглядят семплы для каждой задачи\n",
    "def demo_sample(sample):\n",
    "    if sample['task_id'] == 'handwritten':\n",
    "        print('[gt_text]:',sample['gt_text'])\n",
    "        return io.imshow(sample['image'].permute(1,2,0).numpy())\n",
    "    elif sample['task_id'] == 'trans':\n",
    "        print('[source_text]:', gpt_tokenizer.decode(sample['input_ids'].numpy(), skip_special_tokens=True))\n",
    "        print('[target_text]:', sample['target'])\n",
    "        return\n",
    "    elif sample['task_id'] == 'detection':\n",
    "        print('[input_text]:', gpt_tokenizer.decode(sample['input_ids'].numpy(), skip_special_tokens=True))\n",
    "        print('[boxes]:', sample['boxes'].numpy())\n",
    "        return\n",
    "    elif sample['task_id'] == 'vqa':\n",
    "        print('[question and answer]:', gpt_tokenizer.decode(sample['input_ids'].numpy(), skip_special_tokens=True))\n",
    "        return\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8408c94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[input_text]: green and wite signpost\n",
      "[boxes]: [[0.23733333 0.814      0.33866668 0.184     ]]\n"
     ]
    }
   ],
   "source": [
    "demo_sample(train_dataset[(train_dataset.task_ids == 'detection').argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2e5e3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[source_text]: import java. util. Scanner ; import java. io. PrintStream ; import java. io. OutputStream ; import java. io. IOException ; import java. io. FileOutputStream ; import java. util. Arrays ; import java. io. PrintWriter ; import java. io. FileInputStream ; import java. io. InputStream ; public class Main { public static void main ( String [ ] args ) { InputStream inputStream ; try { inputStream = new FileInputStream ( \" gcj1. in \" ) ; } catch ( IOException e ) { throw new RuntimeException ( e ) ; } OutputStream outputStream ; try { outputStream = new FileOutputStream ( \" gcj1. out \" ) ; } catch ( IOException e ) { throw new RuntimeException (dictionary = {'a':'y ','b':'h ','c':'e ','d':'s ','e':'o ','f':'c ','g':'v ','h':'x ','i':'d ','j':'u ','k':'i ','l':'g ','m':'l ','n':'b ','o':'k ','p':'r ','q':'z ','r':'t ','s':'n ','t':'w ','u':'j ','v':'p ','w':'f ','x':'m ','y':'a ','z':'q'} NEW_LINE def process_file ( file ) : NEW_LINE INDENT fsock = open ( file ) NE\n",
      "[target_text]: dictionary = { ' a ' : ' y ' , ' b ' : ' h ' , ' c ' : ' e ' , ' d ' : ' s ' , ' e ' : ' o ' , ' f ' : ' c ' , ' g ' : ' v ' , ' h ' : ' x ' , ' i ' : ' d ' , ' j ' : ' u ' , ' k ' : ' i ' , ' l ' : ' g ' , ' m ' : ' l ' , ' n ' : ' b ' , ' o ' : ' k ' , ' p ' : ' r ' , ' q ' : ' z ' , ' r ' : ' t ' , ' s ' : ' n ' , ' t ' : ' w ' , ' u ' : ' j ' , ' v ' : ' p ' , ' w ' : ' f ' , ' x ' : ' m ' , ' y ' : ' a ' , ' z ' : ' q ' } NEW_LINE def process_file ( file ) : NEW_LINE INDENT fsock = open ( file ) NEW_LINE text = fsock . read ( ) NEW_LINE fsock . close ( ) NEW_LINE lines = text . split ( ' \\n ' ) NEW_LINE return lines NEW_LINE DEDENT def process_lines ( lines ) : NEW_LINE INDENT ans = [ ] NEW_LINE first = True NEW_LINE for line in lines : NEW_LINE INDENT if first == True : NEW_LINE INDENT first = False NEW_LINE DEDENT else : NEW_LINE INDENT trans = ' ' NEW_LINE for char in line : NEW_LINE INDENT if char in dictionary : NEW_LINE INDENT trans += dictionary [ char ] NEW_LINE DEDENT else : NEW_LINE INDENT trans += char NEW_LINE DEDENT DEDENT if trans != ' ' : NEW_LINE INDENT ans . append ( trans ) NEW_LINE DEDENT DEDENT DEDENT return ans NEW_LINE DEDENT if __name__ == \" _ _ main _ _ \" : NEW_LINE INDENT import sys NEW_LINE filename = sys . argv [ 1 ] NEW_LINE lines = process_file ( filename ) NEW_LINE res = process_lines ( lines ) NEW_LINE c = 0 NEW_LINE for line in res : NEW_LINE INDENT c += 1 NEW_LINE print \" Case ▁ # % d : ▁ % s \" % ( c , line ) NEW_LINE DEDENT DEDENT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_sample(train_dataset[(train_dataset.task_ids == 'trans').argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ff5a4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[question and answer]: How many people are in this picture?one\n"
     ]
    }
   ],
   "source": [
    "demo_sample(train_dataset[(train_dataset.task_ids == 'vqa').argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aab439c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gt_text]: Вариант\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4d5b8553d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo_sample(train_dataset[(train_dataset.task_ids == 'handwritten').argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07eaed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fb_collate_fn(batch):\n",
    "    \"\"\" fusion brain collate fn \"\"\"\n",
    "    encoded, htr_labels, htr_images, gt_texts = [], [], [], [] # handwritten[image]\n",
    "    code_input_ids, code_input_labels, code_targets = [], [], [] #code\n",
    "    vqa_images, vqa_input_ids, labels, targets = [], [], [], []  # vqa[image, text]\n",
    "    detection_names, detection_images, detection_input_ids, detection_attention_masks, boxes, size = [], [], [], [], [], [] # detection[image, text]\n",
    "    \n",
    "    for i, sample in enumerate(batch):\n",
    "        if sample['task_id'] == 'handwritten':\n",
    "            encoded.append(sample['encoded'])\n",
    "            htr_labels.append(sample['labels'])\n",
    "            htr_images.append(sample['image'])\n",
    "            gt_texts.append(sample['gt_text'])\n",
    "        elif sample['task_id'] == 'trans':\n",
    "            code_input_ids.append(sample['input_ids'])\n",
    "            code_input_labels.append(sample['input_labels'])\n",
    "            code_targets.append(sample['target'])\n",
    "        elif sample['task_id'] == 'detection':\n",
    "            detection_images.append(sample['image'])\n",
    "            detection_input_ids.append(sample['input_ids'])\n",
    "            detection_attention_masks.append(sample['attention_mask'])\n",
    "            boxes.append(sample['boxes'])\n",
    "            size.append(sample['size'])\n",
    "            detection_names.append(sample['image_name'])\n",
    "        elif sample['task_id'] == 'vqa':\n",
    "            vqa_images.append(sample['image'])\n",
    "            vqa_input_ids.append(sample['input_ids'])\n",
    "            labels.append(sample['labels'])\n",
    "            targets.append(sample['target'])\n",
    "        \n",
    "    if htr_images:\n",
    "        htr_images = pad_sequence(htr_images, batch_first=True)\n",
    "        encoded = torch.stack(encoded)\n",
    "        htr_labels = torch.stack(htr_labels)\n",
    "    if detection_images:\n",
    "        detection_images = torch.stack(detection_images)   \n",
    "    if vqa_images:\n",
    "        vqa_images = torch.stack(vqa_images) \n",
    "    if detection_attention_masks and torch.is_tensor(detection_attention_masks[0]):\n",
    "        detection_input_ids = pad_sequence(detection_input_ids, batch_first=True)\n",
    "        detection_attention_masks = torch.stack(detection_attention_masks)\n",
    "    elif detection_attention_masks:\n",
    "        detection_input_ids = [input_id.unsqueeze(0) for input_id in detection_input_ids[0]]\n",
    "        detection_attention_masks = [attention_mask.unsqueeze(0) for attention_mask in detection_attention_masks[0]]\n",
    "    if labels:\n",
    "        vqa_input_ids = pad_sequence(vqa_input_ids, batch_first=True)\n",
    "        labels = pad_sequence(labels, batch_first=True)    \n",
    "    if code_input_ids:\n",
    "        code_input_ids = pad_sequence(code_input_ids, batch_first=True)\n",
    "        code_input_labels = pad_sequence(code_input_labels, batch_first=True)\n",
    "    return (htr_images, encoded, htr_labels, gt_texts), (code_input_ids, code_input_labels, code_targets), (vqa_images, vqa_input_ids, labels, targets), (detection_names, detection_images, detection_input_ids, detection_attention_masks, boxes, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75f37ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2FusionBrain(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 gpt_model, \n",
    "                 attention_config, \n",
    "                 handwritten_config, \n",
    "                 vqa_config, \n",
    "                 detection_config, \n",
    "                 **freeze_gpt_kwargs):\n",
    "        super().__init__()\n",
    "        self.gpt_model = gpt_model\n",
    "        self.embedding_size = self.gpt_model.config.n_embd\n",
    "        self.freeze_gpt(**freeze_gpt_kwargs)\n",
    "\n",
    "        # handwritten[image] input/output layers:\n",
    "        self.handwritten_config = handwritten_config\n",
    "        self.handwritten_input_layer = self._build_input_net(\n",
    "            input_dim=handwritten_config['patch_w']*handwritten_config['patch_h']*3,\n",
    "            in_layer_sizes=handwritten_config['in_layer_sizes'],\n",
    "            orth_gain=handwritten_config['orth_gain'],\n",
    "            dropout=handwritten_config['dropout'],\n",
    "        )\n",
    "        self.htr_tokens_embed = nn.Linear(self.embedding_size, self.gpt_model.config.vocab_size, bias=False)\n",
    "        print('=== HANDWRITTEN TASK ===')\n",
    "        self._calculate_trainable_params([\n",
    "            self.handwritten_input_layer,\n",
    "            self.gpt_model, \n",
    "        ], without_emb=True)\n",
    "        print('=== === === === ===')\n",
    "        #####\n",
    "\n",
    "        # code2code\n",
    "        self.beam_size=3\n",
    "        self.sos_id=self.gpt_model.config.bos_token_id\n",
    "        self.eos_id=self.gpt_model.config.eos_token_id\n",
    "        self.lm_head = nn.Linear(self.gpt_model.config.n_embd, self.gpt_model.config.vocab_size, bias=False)\n",
    "\n",
    "        print('=== C2C TASK ===')\n",
    "        self._calculate_trainable_params([self.gpt_model, self.lm_head])\n",
    "        print('=== === === === ===')\n",
    "        \n",
    "        ## zhOD[image, text] and VQA[image, text] layers:\n",
    "        self.attention_config = attention_config\n",
    "        self.cross_attention = nn.ModuleList([\n",
    "            CrossAttentionLayer(self.embedding_size, attention_config['num_heads'], attention_config['pf_dim'])\n",
    "            for _ in range(attention_config['num_attention_layers'])\n",
    "        ])\n",
    "        #####\n",
    "        \n",
    "        # detection[image, text] input/output layers:\n",
    "        self.detection_config = detection_config\n",
    "        self.detection_input_image_layer = self._build_input_net(\n",
    "            input_dim=detection_config['patch_w']*detection_config['patch_h']*3,\n",
    "            in_layer_sizes=detection_config['in_layer_sizes'],\n",
    "            orth_gain=detection_config['orth_gain'],\n",
    "            dropout=detection_config['dropout'],\n",
    "        )\n",
    "        self.detection_pool = nn.AdaptiveMaxPool2d((detection_config[\"num_queries\"], None))\n",
    "        self.bbox_embed = MLP(self.embedding_size, self.embedding_size, 5, detection_config['num_mlp_layers'])\n",
    "        print('=== DETECTION TASK ===')\n",
    "        self._calculate_trainable_params([\n",
    "            self.detection_input_image_layer,\n",
    "            self.gpt_model,\n",
    "            self.cross_attention,\n",
    "            self.bbox_embed\n",
    "        ], without_emb=True)\n",
    "        print('=== === === === ===')\n",
    "        #####\n",
    "        \n",
    "         # vqa[image, text] input/output layers:\n",
    "        self.vqa_config = vqa_config\n",
    "        self.vqa_input_image_layer = self._build_input_net(\n",
    "            input_dim=vqa_config['patch_w']*vqa_config['patch_h']*3,\n",
    "            in_layer_sizes=vqa_config['in_layer_sizes'],\n",
    "            orth_gain=vqa_config['orth_gain'],\n",
    "            dropout=vqa_config['dropout'],\n",
    "        )\n",
    "        self.tokens_embed = nn.Linear(self.embedding_size, self.gpt_model.config.vocab_size, bias=False)\n",
    "        print('=== VQA TASK ===')\n",
    "        self._calculate_trainable_params([\n",
    "            self.vqa_input_image_layer,\n",
    "            self.gpt_model, \n",
    "            self.cross_attention,\n",
    "            self.tokens_embed\n",
    "        ], without_emb=True)\n",
    "        print('=== === === === ===')\n",
    "        #####\n",
    "        \n",
    "        print('=== COMMON PARAMS ===')\n",
    "        self._calculate_common_params()\n",
    "        print('=== === === === ===')\n",
    "\n",
    "\n",
    "    def forward(self, task_id, **kwargs):\n",
    "        if task_id == 'handwritten':\n",
    "            return self.forward_handwritten(**kwargs)\n",
    "        elif task_id == 'trans':\n",
    "            return self.forward_trans(**kwargs)\n",
    "        elif task_id == 'vqa':\n",
    "            return self.forward_vqa(**kwargs)\n",
    "        elif task_id == 'detection':\n",
    "            return self.forward_detection(**kwargs)\n",
    "\n",
    "    def forward_trans(self, input_ids, input_labels=None, eval_bleu=False, past=None):\n",
    "        if not eval_bleu:\n",
    "            attn_mask = torch.tensor(input_labels.clone().detach() != 0, dtype=torch.uint8)\n",
    "            attn_mask = attn_mask.to(input_labels.device)\n",
    "            outputs = self.gpt_model(input_ids, attention_mask=attn_mask)\n",
    "            x = self.lm_head(outputs[0])\n",
    "            return x\n",
    "        else:\n",
    "            if past != None:\n",
    "                outputs = self.gpt_model(input_ids, past_key_values=past)\n",
    "                logits = self.lm_head(outputs[0])\n",
    "                return logits, outputs[1]\n",
    "            else:\n",
    "                outputs = self.gpt_model(input_ids)[1]\n",
    "                return outputs\n",
    "\n",
    "    def forward_handwritten(self, images, target, labels):\n",
    "        patchs = rearrange(images, 'b c (h p1) (w p2) -> b (w) (h) (p1 p2 c)',\n",
    "                      p1=self.handwritten_config['patch_h'], p2=self.handwritten_config['patch_w'])\n",
    "        patchs = patchs.squeeze(2)\n",
    "        patchs = self.handwritten_input_layer(patchs)\n",
    "        \n",
    "        tokens_emb = self.gpt_model.wte(target)\n",
    "        tokens_emb = self.gpt_model.drop(tokens_emb)\n",
    "        \n",
    "        input_emb = torch.cat((patchs, tokens_emb), dim=1)\n",
    "        attention_mask = torch.tensor(labels.clone().detach() != 0, dtype=torch.uint8)\n",
    "        attention_mask = attention_mask.to(labels.device)\n",
    "        # Fusion Brain\n",
    "        gpt_output = self.gpt_model(inputs_embeds=input_emb, attention_mask=attention_mask).last_hidden_state\n",
    "        #####\n",
    "        output_logits = self.htr_tokens_embed(gpt_output)\n",
    "\n",
    "        return output_logits\n",
    "\n",
    "    def forward_vqa(self, images, tokens, labels):\n",
    "        patchs = rearrange(images, 'b c (h p1) (w p2) -> b (w) (h) (p1 p2 c)',\n",
    "                      p1=self.vqa_config['patch_h'], p2=self.vqa_config['patch_w'])\n",
    "        patchs = patchs.squeeze(2)\n",
    "        patchs = self.vqa_input_image_layer(patchs)\n",
    "        attention_mask = torch.tensor(labels.clone().detach() != 0, dtype=torch.uint8)\n",
    "        attention_mask = attention_mask.to(labels.device)\n",
    "        # Fusion Brain\n",
    "        img_emb = self.gpt_model(inputs_embeds=patchs).last_hidden_state\n",
    "        tokens_emb = self.gpt_model(input_ids=tokens, attention_mask=attention_mask).last_hidden_state\n",
    "        #####\n",
    "        for layer in self.cross_attention:\n",
    "            tokens_emb, _ = layer(tokens_emb, img_emb)\n",
    "            \n",
    "        output_logits = self.tokens_embed(tokens_emb)\n",
    "        \n",
    "        return output_logits\n",
    "    \n",
    "    def forward_detection(self, images, tokens, attention_masks):\n",
    "        patchs = rearrange(images, 'b c (h p1) (w p2) -> b (w) (h) (p1 p2 c)',\n",
    "                      p1=self.detection_config['patch_h'], p2=self.detection_config['patch_w'])\n",
    "        patchs = patchs.squeeze(2)\n",
    "        patchs = self.detection_input_image_layer(patchs)\n",
    "        # Fusion Brain\n",
    "        img_embs = self.gpt_model(inputs_embeds=patchs).last_hidden_state\n",
    "        tokens_embs = self.gpt_model(input_ids=tokens, attention_mask=attention_masks).last_hidden_state\n",
    "        #####\n",
    "        norm_img_embs = F.normalize(img_embs, p=2, dim=-1)\n",
    "        norm_tokens_embs = F.normalize(tokens_embs, p=2, dim=-1)\n",
    "        \n",
    "        text_masks = attention_masks.type(torch.bool)\n",
    "        for layer in self.cross_attention:\n",
    "            img_embs, _ = layer(img_embs, tokens_embs, ~text_masks)\n",
    "        img_embs = self.detection_pool(img_embs)\n",
    "        \n",
    "        output_logits = self.bbox_embed(img_embs).sigmoid()\n",
    "        out = {\n",
    "            'pred_logits': output_logits,\n",
    "            'proj_queries': norm_img_embs,\n",
    "            'proj_tokens':norm_tokens_embs,\n",
    "        }\n",
    "\n",
    "        return out\n",
    "\n",
    "    def freeze_gpt(self, freeze_pos=True, freeze_ln=True, freeze_attn=True, freeze_ff=True, freeze_other=True):\n",
    "        for name, p in self.gpt_model.named_parameters():\n",
    "            name = name.lower()\n",
    "            if 'ln' in name or 'norm' in name:\n",
    "                p.requires_grad = not freeze_ln\n",
    "            elif 'wpe' in name or 'position_embeddings' in name or 'pos_drop' in name:\n",
    "                p.requires_grad = not freeze_pos\n",
    "            elif 'mlp' in name:\n",
    "                p.requires_grad = not freeze_ff\n",
    "            elif 'attn' in name:\n",
    "                p.requires_grad = not freeze_attn\n",
    "            else:\n",
    "                p.requires_grad = not freeze_other\n",
    "\n",
    "    def _build_input_net(self, input_dim, in_layer_sizes=None, orth_gain=1.41, dropout=0.1):\n",
    "        \"\"\" вспомогательный метод для сборки input слоя, который приводит размер входящих данный к эмбеддингу gpt \"\"\"\n",
    "        in_layer_sizes = [] if not in_layer_sizes else in_layer_sizes\n",
    "        in_layers = []\n",
    "        last_output_size = input_dim\n",
    "        for size in in_layer_sizes:\n",
    "            layer = nn.Linear(last_output_size, size)\n",
    "            if orth_gain is not None:\n",
    "                torch.nn.init.orthogonal_(layer.weight, gain=orth_gain)\n",
    "            layer.bias.data.zero_()\n",
    "\n",
    "            in_layers.append(layer)\n",
    "            in_layers.append(nn.ReLU())\n",
    "            in_layers.append(nn.Dropout(dropout))\n",
    "            last_output_size = size\n",
    "\n",
    "        final_linear = nn.Linear(last_output_size, self.embedding_size)\n",
    "        if orth_gain is not None:\n",
    "            torch.nn.init.orthogonal_(final_linear.weight, gain=orth_gain)\n",
    "        final_linear.bias.data.zero_()\n",
    "\n",
    "        in_layers.append(final_linear)\n",
    "        in_layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        return nn.Sequential(*in_layers)\n",
    "\n",
    "    def _calculate_trainable_params(self, layers, without_emb=False):\n",
    "        trainable_params, all_used_params = 0, 0\n",
    "        for layer in layers:\n",
    "            trainable_params += sum(p.numel() for p in layer.parameters() if p.requires_grad)\n",
    "            all_used_params += sum(p.numel() for p in layer.parameters())        \n",
    "        print('trainable_params:', trainable_params)\n",
    "        print(' all_used_params:', all_used_params)\n",
    "        print('               %:', round(trainable_params/all_used_params*100, 2))\n",
    "\n",
    "    def _calculate_common_params(self):\n",
    "        all_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        common_params = sum(p.numel() for p in list(self.gpt_model.parameters()))\n",
    "        print('common_params:', common_params)\n",
    "        print('   all_params:', all_params)\n",
    "        print('            %:', round(common_params/all_params*100, 2))     \n",
    "        print('trainable_params:', trainable_params)\n",
    "        print('               %:', round(trainable_params/all_params*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca78cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "handwritten_config = {\n",
    "    'patch_w': 8,\n",
    "    'patch_h': 128,\n",
    "    'in_layer_sizes': [8*128*3],\n",
    "    'orth_gain': 1.41,\n",
    "    'dropout': 0.1,\n",
    "}\n",
    "\n",
    "attention_config = {\n",
    "    'num_attention_layers': 3,\n",
    "    'num_heads': 8,\n",
    "    'pf_dim': 2048,\n",
    "}\n",
    "\n",
    "vqa_config = {\n",
    "    'patch_w': 4,\n",
    "    'patch_h': 224,\n",
    "    'in_layer_sizes': [4*224*3],\n",
    "    'out_layer_sizes': [64],\n",
    "    'orth_gain': 1.41,\n",
    "    'dropout': 0.1,\n",
    "    'tokens_num': len(gpt_tokenizer),\n",
    "}\n",
    "\n",
    "detection_config = {\n",
    "    'patch_w': 4,\n",
    "    'patch_h': 224,\n",
    "    'in_layer_sizes': [4*224*3],\n",
    "    'out_layer_sizes': [64],\n",
    "    'orth_gain': 1.41,\n",
    "    'dropout': 0.1,\n",
    "    'num_mlp_layers': 3,\n",
    "    'num_queries': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a10ef28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HANDWRITTEN TASK ===\n",
      "trainable_params: 11800320\n",
      " all_used_params: 137027328\n",
      "               %: 8.61\n",
      "=== === === === ===\n",
      "=== C2C TASK ===\n",
      "trainable_params: 38598144\n",
      " all_used_params: 163825152\n",
      "               %: 23.56\n",
      "=== === === === ===\n",
      "=== DETECTION TASK ===\n",
      "trainable_params: 27020165\n",
      " all_used_params: 152247173\n",
      "               %: 17.75\n",
      "=== === === === ===\n",
      "=== VQA TASK ===\n",
      "trainable_params: 64433280\n",
      " all_used_params: 189660288\n",
      "               %: 33.97\n",
      "=== === === === ===\n",
      "=== COMMON PARAMS ===\n",
      "common_params: 125227008\n",
      "   all_params: 289135109\n",
      "            %: 43.31\n",
      "trainable_params: 163908101\n",
      "               %: 56.69\n",
      "=== === === === ===\n"
     ]
    }
   ],
   "source": [
    "model = GPT2FusionBrain(\n",
    "    gpt_model,\n",
    "    attention_config=attention_config,\n",
    "    handwritten_config=handwritten_config,\n",
    "    vqa_config=vqa_config,\n",
    "    detection_config=detection_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9525d274",
   "metadata": {},
   "source": [
    "# Lehas Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36a84f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Скрипты для обучения и оценки качества модели\n",
    "\n",
    "\n",
    "class FusionBrainExperiment(TorchGPUExperiment):\n",
    "\n",
    "    handwritten_criterion = nn.CrossEntropyLoss()\n",
    "    detection_criterion = DetectionCriterion(['boxes', 'classification', 'contrastive'], 0.07)\n",
    "    detection_loss_weights = [1.0, 1.0, 1.0, 1.0]\n",
    "    vqa_criterion = nn.CrossEntropyLoss()\n",
    "    c2c_criterion = nn.CrossEntropyLoss()\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<s>',\n",
    "            eos_token='</s>', pad_token='<pad>', unk_token='<|UNKNOWN|>', sep_token='<|SEP|>')\n",
    "    \n",
    "#     def custom_action_before_train_one_epoch(self, test_loader):\n",
    "#         run_evaluation(test_loader, self.model, tokenizer=self.tokenizer)\n",
    "        \n",
    "#     def _custom_action_before_train_one_epoch(self):\n",
    "#         self._wipe_memory()\n",
    "#         self.custom_action_before_train_one_epoch(test_loader)\n",
    "\n",
    "    def calculate_handwritten_metrics(self, gt_texts, outputs):\n",
    "        pred_texts = []\n",
    "        for encoded in outputs.argmax(2).data.cpu().numpy():\n",
    "            pred_texts.append(self.ctc_labeling.decode(encoded))\n",
    "        texts = [self.ctc_labeling.preprocess(text) for text in gt_texts]\n",
    "        return {\n",
    "            'h_cer': cer(pred_texts, texts),\n",
    "            'h_wer': wer(pred_texts, texts),\n",
    "            'h_acc': string_accuracy(pred_texts, texts),\n",
    "        }\n",
    "\n",
    "    def handle_one_batch(self, batch):\n",
    "        (htr_images, encoded, htr_labels, gt_texts), (code_input_ids, code_input_labels, code_targets), (vqa_images, vqa_input_ids, labels, targets), (detection_names, detection_images, detection_input_ids, detection_attention_masks, boxes, size) = batch\n",
    "        losses = []\n",
    "        metrics = {}\n",
    "\n",
    "        if len(htr_images) > 0:\n",
    "            bs = htr_images.shape[0]\n",
    "            images = htr_images.to(self.device, dtype=torch.float32)\n",
    "            encoded = encoded.to(self.device, dtype=torch.long)\n",
    "            htr_labels = htr_labels.to(self.device, dtype=torch.long)\n",
    "            image_labels = torch.ones(bs, 64).to(self.device, dtype=torch.long)\n",
    "            all_labels = torch.cat((image_labels, htr_labels), dim=-1)\n",
    "            loss_mask = torch.tensor(htr_labels == 2, dtype=torch.uint8)\n",
    "            loss_mask = loss_mask.to(self.device)\n",
    "            logits = self.model('handwritten', images=images, target=encoded, labels=all_labels)\n",
    "            shift_logits = logits[..., 64:-1, :].contiguous()\n",
    "            shift_labels = encoded[..., 1:].contiguous()\n",
    "            \n",
    "            flatten_shift_loss_mask = loss_mask[..., :-1].contiguous().view(-1)\n",
    "            ids = torch.nonzero(flatten_shift_loss_mask).view(-1)\n",
    "            handwritten_loss = self.handwritten_criterion(shift_logits.view(-1, shift_logits.size(-1))[ids], shift_labels.view(-1)[ids])\n",
    "            metrics['h_loss'] = handwritten_loss.detach().cpu().item()\n",
    "            losses.append(handwritten_loss)\n",
    "\n",
    "        if len(code_input_ids) > 0:\n",
    "            bs = code_input_ids.shape[0]\n",
    "            code_input_ids = code_input_ids.to(self.device, dtype=torch.long) \n",
    "            code_input_labels = code_input_labels.to(self.device, dtype=torch.long) \n",
    "            loss_mask = torch.tensor(code_input_labels.clone().detach() == 2, dtype=torch.uint8)\n",
    "            loss_mask = loss_mask.to(self.device)\n",
    "            lm_logits = self.model('trans', input_ids=code_input_ids, input_labels=code_input_labels)\n",
    "            c_labels = code_input_ids\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = c_labels[..., 1:].contiguous()\n",
    "            \n",
    "            flatten_shift_loss_mask = loss_mask[..., :-1].contiguous().view(-1)\n",
    "            ids = torch.nonzero(flatten_shift_loss_mask).view(-1)\n",
    "            c2c_loss = self.c2c_criterion(shift_logits.view(-1, shift_logits.size(-1))[ids], shift_labels.view(-1)[ids])\n",
    "            metrics['c2c_loss'] = c2c_loss.detach().cpu().item()\n",
    "            losses.append(c2c_loss)\n",
    "            \n",
    "        if len(labels) > 0:\n",
    "            images = vqa_images.to(self.device, dtype=torch.float32)\n",
    "            input_ids = vqa_input_ids.to(self.device, dtype=torch.long)\n",
    "            labels = labels.to(self.device, dtype=torch.float32)\n",
    "            loss_mask = torch.tensor(labels.clone().detach() == 2, dtype=torch.uint8)\n",
    "            loss_mask = loss_mask.to(self.device)\n",
    "            lm_logits = self.model('vqa', images=images, tokens=input_ids, labels=labels)\n",
    "            labels = input_ids\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "            flatten_shift_loss_mask = loss_mask[..., :-1].contiguous().view(-1)\n",
    "            ids = torch.nonzero(flatten_shift_loss_mask).view(-1)\n",
    "            vqa_loss = self.vqa_criterion(shift_logits.view(-1, shift_logits.size(-1))[ids], shift_labels.view(-1)[ids])\n",
    "            metrics['vqa_loss'] = vqa_loss.detach().cpu().item()\n",
    "            losses.append(vqa_loss)\n",
    "            \n",
    "        if len(boxes) > 0:\n",
    "            images = detection_images.to(self.device, dtype=torch.float32)\n",
    "            input_ids = detection_input_ids.to(self.device, dtype=torch.long) \n",
    "            attention_masks = detection_attention_masks.to(self.device, dtype=torch.long) \n",
    "            boxes = [boxes_per_label.to(self.device, dtype=torch.float) for boxes_per_label in boxes]\n",
    "            detection_outputs = self.model('detection', images=images, tokens=input_ids, attention_masks=attention_masks)\n",
    "            detection_loss = self.detection_criterion(detection_outputs, boxes)\n",
    "            #detection_loss = sum([\n",
    "            #    loss * weight for loss, weight in zip(detection_loss.values(), self.detection_loss_weights)\n",
    "            #])\n",
    "            #metrics['detection_acc'] = acc(targets.argmax(axis=1), sentiment_outputs)\n",
    "            metrics['loss_giou'] = detection_loss['loss_giou'].detach().cpu().item()\n",
    "            metrics['loss_bbox'] = detection_loss['loss_bbox'].detach().cpu().item()\n",
    "            metrics['loss_contrastive'] = detection_loss['loss_contrastive'].detach().cpu().item()\n",
    "            metrics['loss_classification'] = detection_loss['loss_classification'].detach().cpu().item()\n",
    "            losses.append(detection_loss['loss_giou'])\n",
    "            losses.append(detection_loss['loss_bbox'])\n",
    "            losses.append(detection_loss['loss_contrastive'])\n",
    "            losses.append(detection_loss['loss_classification'])\n",
    "\n",
    "        #loss = sum(losses)\n",
    "        loss = sum([loss * weight for loss, weight in zip(losses, self.detection_loss_weights)])\n",
    "        \n",
    "        #print(loss)\n",
    "        self.metrics.update(loss=loss.detach().cpu().item(), **metrics)\n",
    "        \n",
    "        #self.metrics.update(loss=loss.item(), **metrics)\n",
    "\n",
    "        if self.is_train:\n",
    "            loss.backward()\n",
    "            self.optimizer_step()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scheduler.step()\n",
    "\n",
    "\n",
    "def run_evaluation(loader, model, tokenizer=None, device=torch.device('cuda:0')):\n",
    "    result = []\n",
    "    true_json_detection = {}\n",
    "    pred_json_detection = {}\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            (htr_images, encoded, htr_labels, gt_texts), (code_input_ids, code_input_labels, code_targets), (vqa_images, vqa_input_ids, labels, targets), (detection_names, detection_images, detection_input_ids, detection_attention_masks, boxes, size) = batch\n",
    "            if len(htr_images) > 0:\n",
    "                bs = htr_images.shape[0]\n",
    "                images = htr_images.to(device, dtype=torch.float32)\n",
    "                attention_mask = torch.ones(bs, 65).to(device, dtype=torch.long)\n",
    "                tokens = torch.tensor([[gpt_tokenizer.bos_token_id]]).repeat(bs, 1)\n",
    "                tokens = tokens.to(device, dtype=torch.long)\n",
    "                handwritten_outputs = htr_evaluation(model, images, attention_mask, tokens, 6)\n",
    "                for encoded, gt_text in zip(handwritten_outputs.argmax(2).data.cpu().numpy(), gt_texts):\n",
    "                    pred_text = gpt_tokenizer.decode(encoded, skip_special_tokens=True)\n",
    "                    result.append({\n",
    "                        'task_id': 'handwritten',\n",
    "                        'gt_output': gt_text,\n",
    "                        'pred_output': pred_text,\n",
    "                    })\n",
    "\n",
    "            if len(code_input_ids) > 0:\n",
    "                code_input_ids = code_input_ids.to(device, dtype=torch.long)\n",
    "                code_input_labels = code_input_labels.to(device, dtype=torch.long)\n",
    "                loss_mask = torch.tensor(code_input_labels.clone().detach() == 2, dtype=torch.uint8)\n",
    "                loss_mask = loss_mask.to(device)\n",
    "                hidden_states = model('trans', input_ids=code_input_ids, input_labels=code_input_labels, eval_bleu=True)\n",
    "                bleu_score, _ = eval_bleu(model, hidden_states, input_ids=code_input_ids, beam_size=5, tokenizer=tokenizer, targets=code_targets)\n",
    "                result.append({\n",
    "                        'task_id': 'trans',\n",
    "                        'true_text': code_targets,\n",
    "                        'bleu_score': bleu_score,\n",
    "                })\n",
    "                \n",
    "            if len(labels) > 0:\n",
    "                images = vqa_images.to(device, dtype=torch.float32)\n",
    "                input_ids = vqa_input_ids.to(device, dtype=torch.long)\n",
    "                labels = labels.to(device, dtype=torch.float) \n",
    "                attention_mask = torch.tensor(labels.clone().detach() != 0, dtype=torch.uint8)\n",
    "                attention_mask = attention_mask.to(labels.device)\n",
    "                vqa_outputs = vqa_evaluation(model, images, input_ids, attention_mask, 10)\n",
    "                for target, pred_labels in zip(targets, vqa_outputs.argmax(-1).cpu().numpy()):\n",
    "                    result.append({\n",
    "                        'task_id': 'vqa',\n",
    "                        'gt_output': target,\n",
    "                        'pred_output': gpt_tokenizer.decode(pred_labels, skip_special_tokens=True).split(gpt_tokenizer.eos_token)[0],\n",
    "                    })\n",
    "                    \n",
    "            if len(boxes) > 0:\n",
    "                images = detection_images.to(device, dtype=torch.float32)\n",
    "                input_ids = [input_id.to(device, dtype=torch.long) for input_id in detection_input_ids]\n",
    "                attention_masks = [attention_mask.to(device, dtype=torch.long) for attention_mask in detection_attention_masks]\n",
    "                detection_outputs = detection_evaluation(model, images, input_ids, attention_masks, 0.12, 0.5)\n",
    "                img_h, img_w = size[0]\n",
    "                for i in range(len(detection_outputs)):\n",
    "                    if detection_outputs[i].numel() != 0:\n",
    "                        detection_outputs[i][:, 0] = detection_outputs[i][:, 0] * img_w\n",
    "                        detection_outputs[i][:, 2] = detection_outputs[i][:, 2] * img_w\n",
    "                        detection_outputs[i][:, 1] = detection_outputs[i][:, 1] * img_h\n",
    "                        detection_outputs[i][:, 3] = detection_outputs[i][:, 3] * img_h\n",
    "                image_name = detection_names[0]\n",
    "                for boxes_for_img in boxes:\n",
    "                    true_json_detection[image_name] = boxes_for_img\n",
    "                    pred_json_detection[image_name] = {\n",
    "                        input_text: output.type(torch.int32).cpu().tolist()\n",
    "                        for input_text, output in zip(boxes_for_img.keys(), detection_outputs)\n",
    "                    }\n",
    "                result.append({\n",
    "                        'task_id': 'detection',\n",
    "                    })\n",
    "                \n",
    "    result = pd.DataFrame(result)\n",
    "\n",
    "    handwritten_result = result[result['task_id'] == 'handwritten']\n",
    "    if handwritten_result.shape[0]:\n",
    "        print('= Handwritten =')\n",
    "        print('CER:', round(cer(handwritten_result['pred_output'], handwritten_result['gt_output']), 3))\n",
    "        print('WER:', round(wer(handwritten_result['pred_output'], handwritten_result['gt_output']), 3))\n",
    "        print('ACC:', round(string_accuracy(handwritten_result['pred_output'], handwritten_result['gt_output']), 3))\n",
    "        print('=== === === ===')\n",
    "        \n",
    "    trans_result = result[result['task_id'] == 'trans']   \n",
    "    if trans_result.shape[0]:\n",
    "        print('== C2C ==')\n",
    "        print('meanBLEU:', np.mean(trans_result['bleu_score']))\n",
    "        print('=== === === ===')\n",
    "        \n",
    "    vqa_result = result[result['task_id'] == 'vqa']\n",
    "    if vqa_result.shape[0]:\n",
    "        print('== VQA ==')\n",
    "        print('ACC:', round(vqa_evaluate(vqa_result), 3))\n",
    "        print('=== === === ===')\n",
    "        \n",
    "    \n",
    "    if len(true_json_detection):\n",
    "        print('== Detection ==')\n",
    "        #print(true_json_detection)\n",
    "        #print(\"PRED\")\n",
    "        #print(pred_json_detection)\n",
    "        print('ACC:', round(detection_evaluate(true_json_detection, pred_json_detection), 3))\n",
    "        print('=== === === ===')\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_model(model, experiment_name):\n",
    "    paths = sorted(glob(f'./saved_models/{experiment_name}*'))\n",
    "    if len(paths) == 0:\n",
    "        print('Warning! Model not found')\n",
    "        return model\n",
    "    checkpoint_path = paths[-1] + '/last.pt'\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "#     metrics = defaultdict(list)\n",
    "#     for epoch in range(len(checkpoint['metrics_state_dict']['train_metrics'])):\n",
    "#         train_metrics = checkpoint['metrics_state_dict']['train_metrics'][epoch]['avg']\n",
    "#         valid_metrics = checkpoint['metrics_state_dict']['valid_metrics'][epoch]['avg']\n",
    "#         for key in train_metrics.keys():\n",
    "#             metrics[f'train_{key}'].append(train_metrics[key])\n",
    "#             metrics[f'valid_{key}'].append(valid_metrics[key])\n",
    "#         metrics['epoch'].append(epoch)        \n",
    "#     metrics = pd.DataFrame(metrics)\n",
    "    return model\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e778d646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/home/user/conda/lib/python3.7/site-packages/ipykernel_launcher.py:166: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "100%|██████████| 200/200 [00:27<00:00,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== VQA ==\n",
      "ACC: 0.0\n",
      "=== === === ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#@title c2c evaluation\n",
    "vqa_valid = df[(df['task_id'] == 'vqa') & (df['stage'] == 'valid')][:200]\n",
    "\n",
    "vqa_eval_dataset = DatasetRetriever(\n",
    "    task_ids=vqa_valid['task_id'].values,\n",
    "    input_images=vqa_valid['input_image'].values,\n",
    "    input_texts=vqa_valid['input_text'].values,\n",
    "    output_texts=vqa_valid['output_text'].values,\n",
    "    output_boxes=vqa_valid['output_boxes'].values,\n",
    "    stage='test',\n",
    "    handwritten_max_tokens_length=8,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "#model = load_model(model, 'fusion-brain-vqa')\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('saved_models/fusion-brain-handwritten-1635813348/last.pt')['model_state_dict'])\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    vqa_eval_dataset,\n",
    "    batch_size=1,\n",
    "    sampler=SequentialSampler(vqa_eval_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "evaluation_result = run_evaluation(valid_loader, model, tokenizer=gpt_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a7e2e2",
   "metadata": {},
   "source": [
    "## Code To Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197f9349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск обучения c2c[code]\n",
    "\n",
    "DEMO_LOGS = False \n",
    "\n",
    "trans_train = df[(df['task_id'] == 'trans') & (df['stage'] == 'train')]\n",
    "trans_valid = df[(df['task_id'] == 'trans') & (df['stage'] == 'valid')]\n",
    "trans_test = df[(df['task_id'] == 'trans') & (df['stage'] == 'test')]\n",
    "\n",
    "\n",
    "trans_train_dataset = DatasetRetriever(\n",
    "    task_ids=trans_train['task_id'].values,\n",
    "    input_images=trans_train['input_image'].values,\n",
    "    input_texts=trans_train['input_text'].values,\n",
    "    output_texts=trans_train['output_text'].values,\n",
    "    output_boxes=trans_train['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='train',\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "trans_valid_dataset = DatasetRetriever(\n",
    "    task_ids=trans_valid['task_id'].values,\n",
    "    input_images=trans_valid['input_image'].values,\n",
    "    input_texts=trans_valid['input_text'].values,\n",
    "    output_texts=trans_valid['output_text'].values,\n",
    "    output_boxes=trans_valid['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='valid',\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "trans_test_dataset = DatasetRetriever(\n",
    "    task_ids=trans_test['task_id'].values,\n",
    "    input_images=trans_test['input_image'].values,\n",
    "    input_texts=trans_test['input_text'].values,\n",
    "    output_texts=trans_test['output_text'].values,\n",
    "    output_boxes=trans_test['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='test',\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "\n",
    "#model.freeze_gpt()\n",
    "model.freeze_gpt(freeze_pos=False, freeze_ln=False, freeze_attn=True, freeze_ff=True, freeze_other=False)\n",
    "\n",
    "CONFIG = {\n",
    "    'description': 'Обучение c2c task',\n",
    "    'experiment_name': f'fusion-brain-c2c-{round(datetime.utcnow().timestamp())}',\n",
    "    'lr': 0.000005,\n",
    "    'bs': 4,\n",
    "    'num_epochs': 25,\n",
    "    'max_lr': 0.001,\n",
    "    'pct_start': 0.1,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trans_train_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=RandomSampler(trans_train_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    trans_valid_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=SequentialSampler(trans_valid_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    trans_test_dataset,\n",
    "    batch_size=1,\n",
    "    sampler=SequentialSampler(trans_test_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['max_lr'],\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    pct_start=CONFIG['pct_start'],\n",
    "    epochs=CONFIG['num_epochs'],\n",
    ")\n",
    "\n",
    "if DEMO_LOGS:\n",
    "    print(open('./saved_models/fusion-brain-detection-1631794561/log.txt').read())\n",
    "else:\n",
    "    experiment = FusionBrainExperiment(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=None,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        base_dir=f'./saved_models',\n",
    "        experiment_name=CONFIG['experiment_name'],\n",
    "        verbose_step=10**5,\n",
    "        seed=42,\n",
    "        best_saving=False,\n",
    "        last_saving=True,\n",
    "        #test_loader=test_loader #before each epoch BLEU is calculated (custom_action_before_train_one_epoch in Experiment)\n",
    "    )\n",
    "    experiment.fit(train_loader, valid_loader, CONFIG['num_epochs'])\n",
    "    experiment.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b23c58",
   "metadata": {},
   "source": [
    "## Handwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3325845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Запуск обучения handwritten[image]\n",
    "\n",
    "DEMO_LOGS = False #@param {type:\"boolean\"}\n",
    "\n",
    "handwritten_train = df[(df['task_id'] == 'handwritten') & (df['stage'] == 'train')]\n",
    "handwritten_valid = df[(df['task_id'] == 'handwritten') & (df['stage'] == 'valid')]\n",
    "\n",
    "handwritten_train_dataset = DatasetRetriever(\n",
    "    task_ids=handwritten_train['task_id'].values,\n",
    "    input_images=handwritten_train['input_image'].values,\n",
    "    input_texts=handwritten_train['input_text'].values,\n",
    "    output_texts=handwritten_train['output_text'].values,\n",
    "    output_boxes=handwritten_train['output_boxes'].values,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='train',\n",
    "    handwritten_max_tokens_length=8,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "handwritten_valid_dataset = DatasetRetriever(\n",
    "    task_ids=handwritten_valid['task_id'].values,\n",
    "    input_images=handwritten_valid['input_image'].values,\n",
    "    input_texts=handwritten_valid['input_text'].values,\n",
    "    output_texts=handwritten_valid['output_text'].values,\n",
    "    output_boxes=handwritten_valid['output_boxes'].values,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='valid',\n",
    "    handwritten_max_tokens_length=8,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "CONFIG = {\n",
    "    'description': 'Обучение handwritten task',\n",
    "    'experiment_name': f'fusion-brain-handwritten-{round(datetime.utcnow().timestamp())}',\n",
    "    'lr': 0.00008,\n",
    "    'bs': 64,\n",
    "    'num_epochs': 50,\n",
    "    'max_lr': 0.001,\n",
    "    'pct_start': 0.1,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    handwritten_train_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=RandomSampler(handwritten_train_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    handwritten_valid_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=SequentialSampler(handwritten_valid_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "model.freeze_gpt()\n",
    "model = model.to(device)\n",
    "#model.load_state_dict(torch.load('MainWeights/c2c_handwritten.pt')['model_state_dict'])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['max_lr'],\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    pct_start=CONFIG['pct_start'],\n",
    "    epochs=CONFIG['num_epochs'],\n",
    ")\n",
    "\n",
    "if DEMO_LOGS:\n",
    "    print(open('./saved_models/fusion-brain-handwritten-1631797524/log.txt').read())\n",
    "else:\n",
    "    experiment = FusionBrainExperiment(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=None,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        base_dir=f'./saved_models',\n",
    "        experiment_name=CONFIG['experiment_name'],\n",
    "        verbose_step=10**5,\n",
    "        seed=42,\n",
    "        best_saving=True,\n",
    "        last_saving=True,\n",
    "    )\n",
    "    experiment.fit(train_loader, valid_loader, CONFIG['num_epochs'])\n",
    "    experiment.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d72f7",
   "metadata": {},
   "source": [
    "## VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291e45cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Запуск обучения vqa[image+text]\n",
    "\n",
    "DEMO_LOGS = False\n",
    "\n",
    "vqa_train = df[(df['task_id'] == 'vqa') & (df['stage'] == 'train')]\n",
    "vqa_valid = df[(df['task_id'] == 'vqa') & (df['stage'] == 'valid')]\n",
    "\n",
    "vqa_train_dataset = DatasetRetriever(\n",
    "    task_ids=vqa_train['task_id'].values,\n",
    "    input_images=vqa_train['input_image'].values,\n",
    "    input_texts=vqa_train['input_text'].values,\n",
    "    output_texts=vqa_train['output_text'].values,\n",
    "    output_boxes=vqa_train['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='train',\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "vqa_valid_dataset = DatasetRetriever(\n",
    "    task_ids=vqa_valid['task_id'].values,\n",
    "    input_images=vqa_valid['input_image'].values,\n",
    "    input_texts=vqa_valid['input_text'].values,\n",
    "    output_texts=vqa_valid['output_text'].values,\n",
    "    output_boxes=vqa_valid['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='valid',\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "model.freeze_gpt(freeze_pos=False, freeze_ln=False, freeze_attn=True, freeze_ff=True, freeze_other=False)\n",
    "\n",
    "CONFIG = {\n",
    "    'description': 'Обучение vqa task',\n",
    "    'experiment_name': f'fusion-brain-vqa-{round(datetime.utcnow().timestamp())}',\n",
    "    'lr': 0.000004,\n",
    "    'bs': 64,\n",
    "    'num_epochs': 20,\n",
    "    'max_lr': 0.001,\n",
    "    'pct_start': 0.1,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    vqa_train_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=RandomSampler(vqa_train_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    vqa_valid_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=SequentialSampler(vqa_valid_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "#model.load_state_dict(torch.load('MainWeights/c2c_handwritten.pt'), strict=False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['max_lr'],\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    pct_start=CONFIG['pct_start'],\n",
    "    epochs=CONFIG['num_epochs'],\n",
    ")\n",
    "\n",
    "if DEMO_LOGS:\n",
    "    print(open('./saved_models/fusion-brain-vqa-1631794561/log.txt').read())\n",
    "else:\n",
    "    experiment = FusionBrainExperiment(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=None,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        base_dir=f'./saved_models',\n",
    "        experiment_name=CONFIG['experiment_name'],\n",
    "        verbose_step=10**5,\n",
    "        seed=42,\n",
    "        best_saving=True,\n",
    "        last_saving=True,\n",
    "    )\n",
    "    experiment.fit(train_loader, valid_loader, CONFIG['num_epochs'])\n",
    "    experiment.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1266f8",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbb2ef79",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1755/1166003257.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mDEMO_LOGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdetection_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'task_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'detection'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stage'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdetection_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'task_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'detection'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stage'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Запуск обучения detection[image+text]\n",
    "\n",
    "DEMO_LOGS = False\n",
    "\n",
    "detection_train = df[(df['task_id'] == 'detection') & (df['stage'] == 'train')]\n",
    "detection_valid = df[(df['task_id'] == 'detection') & (df['stage'] == 'valid')]\n",
    "\n",
    "detection_train_dataset = DatasetRetriever(\n",
    "    task_ids=detection_train['task_id'].values,\n",
    "    input_images=detection_train['input_image'].values,\n",
    "    input_texts=detection_train['input_text'].values,\n",
    "    output_texts=detection_train['output_text'].values,\n",
    "    output_boxes=detection_train['output_boxes'].values,\n",
    "    stage='train',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "detection_valid_dataset = DatasetRetriever(\n",
    "    task_ids=detection_valid['task_id'].values,\n",
    "    input_images=detection_valid['input_image'].values,\n",
    "    input_texts=detection_valid['input_text'].values,\n",
    "    output_texts=detection_valid['output_text'].values,\n",
    "    output_boxes=detection_valid['output_boxes'].values,\n",
    "    stage='valid',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "model.freeze_gpt(freeze_pos=True, freeze_ln=False, freeze_attn=True, freeze_ff=True, freeze_other=False)\n",
    "\n",
    "CONFIG = {\n",
    "    'description': 'Обучение detection task',\n",
    "    'experiment_name': f'fusion-brain-detection-{round(datetime.utcnow().timestamp())}',\n",
    "    'lr': 0.000004,\n",
    "    'bs': 32,\n",
    "    'num_epochs': 60,\n",
    "    'max_lr': 0.0001,\n",
    "    'pct_start': 0.1,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    detection_train_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=RandomSampler(detection_train_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    detection_valid_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=SequentialSampler(detection_valid_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "#model.load_state_dict(torch.load('MainWeights/c2c_handwritten_vqa.pt'), strict=False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['max_lr'],\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    pct_start=CONFIG['pct_start'],\n",
    "    epochs=CONFIG['num_epochs'],\n",
    ")\n",
    "\n",
    "if DEMO_LOGS:\n",
    "    print(open('./saved_models/fusion-brain-detection-1631794561/log.txt').read())\n",
    "else:\n",
    "    experiment = FusionBrainExperiment(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=None,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        base_dir=f'./saved_models',\n",
    "        experiment_name=CONFIG['experiment_name'],\n",
    "        verbose_step=10**5,\n",
    "        seed=42,\n",
    "        best_saving=True,\n",
    "        last_saving=True,\n",
    "    )\n",
    "    experiment.fit(train_loader, valid_loader, CONFIG['num_epochs'])\n",
    "    experiment.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7475e446",
   "metadata": {},
   "source": [
    "## Fusion Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ab2b9bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1755/3866884740.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m CONFIG = {\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m'description'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Fusion Brain'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m'experiment_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf'fusion-brain-main-{round(datetime.utcnow().timestamp())}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.00008\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m'bs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "# @title Запуск обучения fusion brain [text, image]\n",
    "\n",
    "DEMO_LOGS = False #@param {type:\"boolean\"}\n",
    "\n",
    "CONFIG = {\n",
    "    'description': 'Fusion Brain',\n",
    "    'experiment_name': f'fusion-brain-main-{round(datetime.utcnow().timestamp())}',\n",
    "    'lr': 0.00008,\n",
    "    'bs': 64,\n",
    "    'num_epochs': 20,\n",
    "    'max_lr': 0.00004,\n",
    "    'pct_start': 0.1,\n",
    "    'final_div_factor': 100\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=BalanceClassSampler(labels=train_dataset.get_task_labels()),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=BalanceClassSampler(labels=valid_dataset.get_task_labels()),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "model.freeze_gpt(False, False, False, False, False)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('MainWeights/c2c_handwritten.pt'), strict=False)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['max_lr'],\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    pct_start=CONFIG['pct_start'],\n",
    "    epochs=CONFIG['num_epochs'],\n",
    "    final_div_factor=CONFIG['final_div_factor'],\n",
    ")\n",
    "\n",
    "if DEMO_LOGS:\n",
    "    print(open('./saved_models/fusion-brain-main-1631924391/log.txt').read())\n",
    "else:\n",
    "    experiment = FusionBrainExperiment(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=None,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        base_dir=f'./saved_models',\n",
    "        experiment_name=CONFIG['experiment_name'],\n",
    "        verbose_step=10**5,\n",
    "        seed=42,\n",
    "        best_saving=True,\n",
    "        last_saving=True,\n",
    "    )\n",
    "    experiment.fit(train_loader, valid_loader, CONFIG['num_epochs'])\n",
    "    experiment.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28893bec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee54de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
